[2022-11-17 20:33:36,374][__main__][INFO] - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

[2022-11-17 20:33:36,376][__main__][INFO] - output_dir: tuned-model
bittensor:
  network: nobunaga
dataset:
  name: bittensor
  config_name: null
  num_batches: 1000
  block_size: 256
  overwrite_cache: false
  keep_linebreaks: true
  concatenate_raw: false
model:
  name: EleutherAI/gpt-neo-2.7B
  config_name: null
tokenizer:
  name: null
  use_fast: true
  preprocessing_num_workers: null
  pad_token: '[PAD]'
training:
  seed: null
  val_split_percent: 30
  train_batch_size: 8
  eval_batch_size: 8
  learning_rate: 1.0e-05
  weight_decay: 0.0
  num_epochs: 10
  max_train_steps: null
  gradient_accumulation_steps: 1
  lr_scheduler: constant
  lr_warmup_steps: 0
  eval_every: 50
  max_eval_steps: 1000
  checkpoint:
    resume_from_checkpoint: 0
    every_n_steps: null
tracking:
  enabled: true
  report_to: all

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/config.json
Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-2.7B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      16
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 20,
  "num_layers": 32,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/config.json
Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-2.7B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      16
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 20,
  "num_layers": 32,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

loading file vocab.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/vocab.json
loading file merges.txt from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/merges.txt
loading file tokenizer.json from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/tokenizer_config.json
loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/config.json
Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-2.7B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      16
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 20,
  "num_layers": 32,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/config.json
Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-2.7B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      16
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 20,
  "num_layers": 32,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

Using pad_token, but it is not set yet.
loading weights file flax_model.msgpack from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/flax_model.msgpack
Loading Flax weights from /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/flax_model.msgpack
/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/modeling_flax_pytorch_utils.py:315: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)
  pt_model_dict[flax_key] = torch.from_numpy(flax_tensor)
All Flax model weights were used when initializing GPTNeoForCausalLM.

Some weights of GPTNeoForCausalLM were not initialized from the Flax model and are newly initialized: ['transformer.h.18.attn.attention.bias', 'transformer.h.13.attn.attention.masked_bias', 'transformer.h.0.attn.attention.masked_bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.27.attn.attention.masked_bias', 'transformer.h.30.attn.attention.masked_bias', 'transformer.h.2.attn.attention.bias', 'transformer.h.18.attn.attention.masked_bias', 'transformer.h.9.attn.attention.masked_bias', 'transformer.h.20.attn.attention.masked_bias', 'transformer.h.16.attn.attention.masked_bias', 'transformer.h.30.attn.attention.bias', 'transformer.h.15.attn.attention.masked_bias', 'transformer.h.3.attn.attention.masked_bias', 'transformer.h.16.attn.attention.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.29.attn.attention.bias', 'transformer.h.14.attn.attention.masked_bias', 'transformer.h.28.attn.attention.bias', 'transformer.h.24.attn.attention.masked_bias', 'transformer.h.25.attn.attention.masked_bias', 'transformer.h.8.attn.attention.masked_bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.12.attn.attention.masked_bias', 'transformer.h.25.attn.attention.bias', 'transformer.h.4.attn.attention.masked_bias', 'transformer.h.4.attn.attention.bias', 'transformer.h.2.attn.attention.masked_bias', 'transformer.h.8.attn.attention.bias', 'transformer.h.10.attn.attention.bias', 'transformer.h.14.attn.attention.bias', 'transformer.h.21.attn.attention.masked_bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.26.attn.attention.masked_bias', 'lm_head.weight', 'transformer.h.12.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.6.attn.attention.bias', 'transformer.h.19.attn.attention.masked_bias', 'transformer.h.20.attn.attention.bias', 'transformer.h.0.attn.attention.bias', 'transformer.h.27.attn.attention.bias', 'transformer.h.6.attn.attention.masked_bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.17.attn.attention.masked_bias', 'transformer.h.11.attn.attention.masked_bias', 'transformer.h.23.attn.attention.masked_bias', 'transformer.h.31.attn.attention.bias', 'transformer.h.10.attn.attention.masked_bias', 'transformer.h.24.attn.attention.bias', 'transformer.h.7.attn.attention.masked_bias', 'transformer.h.28.attn.attention.masked_bias', 'transformer.h.5.attn.attention.masked_bias', 'transformer.h.26.attn.attention.bias', 'transformer.h.31.attn.attention.masked_bias', 'transformer.h.22.attn.attention.masked_bias', 'transformer.h.1.attn.attention.masked_bias', 'transformer.h.17.attn.attention.bias', 'transformer.h.29.attn.attention.masked_bias', 'transformer.h.19.attn.attention.bias', 'transformer.h.22.attn.attention.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257
}

loading file vocab.json from cache at /home/paperspace/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/vocab.json
loading file merges.txt from cache at /home/paperspace/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/merges.txt
loading file tokenizer.json from cache at /home/paperspace/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--gpt2/snapshots/75e09b43581151bd1d9ef6700faa605df408979f/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257
}

Using pad_token, but it is not set yet.
starting here!
Loading data from bittensor IPFS:   0%|          | 0/500 [00:00<?, ?it/s]Loading data from bittensor IPFS: 100%|██████████| 500/500 [00:00<00:00, 10692.12it/s]
raw_datasets types: <class 'datasets.arrow_dataset.Dataset'>
raw_datasets length: 4000
Running tokenizer on dataset:   0%|          | 0/4 [00:00<?, ?ba/s]Running tokenizer on dataset:  25%|██▌       | 1/4 [00:00<00:01,  2.65ba/s]Running tokenizer on dataset:  50%|█████     | 2/4 [00:00<00:00,  2.81ba/s]Running tokenizer on dataset:  75%|███████▌  | 3/4 [00:01<00:00,  2.22ba/s]Running tokenizer on dataset: 100%|██████████| 4/4 [00:01<00:00,  2.43ba/s]Running tokenizer on dataset: 100%|██████████| 4/4 [00:01<00:00,  2.45ba/s]train_dataset types: <class 'datasets.arrow_dataset.Dataset'>
train_dataset length: 2800
eval_dataset length: 600
[2022-11-17 20:34:57,662][__main__][INFO] - Sample 645 of the training set: {'text': "f(x)=\\\\frac{\\\\ln x}{x} \\\\text{ is decreasing for } x\\\\geq \\\\e\\\\right)\\\\\\\\\\n= \\u0026\\\\Delta\\\\sqrt{\\\\frac{1}{\\\\ln\\\\frac{448K}{\\\\Delta^4\\\\delta}}\\\\ln\\\\left(\\\\frac{8K}{\\\\Delta^4\\\\delta}\\\\left(\\\\ln\\\\frac{448K}{\\\\Delta^4\\\\delta}\\\\right)^2\\\\right)}\\\\\\\\\\n\\u003e \\u0026\\\\Delta\\\\sqrt{\\\\frac{1}{\\\\ln\\\\frac{448K}{\\\\Delta^4\\\\delta}}\\\\ln\\\\left(\\\\frac{8K}{\\\\Delta^4\\\\delta}\\\\cdot 56\\\\right)}\\\\ \\\\ \\\\ \\\\left(\\\\text{by $\\\\left(\\\\ln\\\\frac{448K}{\\\\Delta^4\\\\delta}\\\\right)^2\\\\!\\u003e\\\\! \\\\left(\\\\ln\\\\frac{448\\\\cdot 2}{1^4(\\\\frac{1}{2})}\\\\right)^2\\\\!=\\\\!56.11\\\\cdots\\u003e56$}\\\\right)\\\\\\\\\\n= \\u0026\\\\Delta\\\\sqrt{\\\\frac{1}{\\\\ln\\\\frac{448K}{\\\\Delta^4\\\\delta}}\\\\ln\\\\frac{448K}{\\\\Delta^4\\\\delta}}=\\\\Delta.\\\\end{aligned}$$\\n\\nThe difference between the worst case stopping times $\\\\tau'_i-\\\\tau_i$ is lower-bounded as $$\\\\begin{aligned}\\n \\\\tau'_i-\\\\tau_i\\u003e \\u0026\\\\Tdd{\\\\Delta}-\\\\Td{\\\\Delta}=\\\\left\\\\lfloor\\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{448K}{\\\\Delta^4\\\\delta}\\\\right\\\\rfloor-\\\\left\\\\lceil\\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{\\\\sqrt{K}\\\\Ndelta}{\\\\delta}\\\\right\\\\rceil\\\\\\\\\\n \\u003e\\u0026\\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{448K}{\\\\Delta^4\\\\delta}-\\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{\\\\sqrt{K}\\\\Ndelta}{\\\\delta}-2\\\\\\\\\\n =\\u0026\\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{448\\\\sqrt{K}}{\\\\Delta^4\\\\Ndelta\\\\e^{\\\\Delta^2}}\\\\\\\\\\n \\u003e \\u0026 \\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{448\\\\sqrt{K}e^{-\\\\Delta^2}}{\\\\Delta^4\\\\left(\\\\frac{2\\\\e}{(\\\\e-1)\\\\Delta^2}\\\\ln\\\\frac{2\\\\sqrt{K}}{\\\\Delta^2\\\\delta}+1\\\\right)}\\\\\\\\\\n = \\u0026 \\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{448\\\\sqrt{K}e^{-\\\\Delta^2}\\\\cdot\\\\frac{(\\\\e-1)}{2\\\\e\\\\Delta^2}}{\\\\ln\\\\frac{2\\\\sqrt{K}}{\\\\Delta^2\\\\delta}+\\\\frac{(\\\\e-1)\\\\Delta^2}{2\\\\e}}\\\\\\\\\\n = \\u0026 \\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{\\\\frac{224\\\\sqrt{K}e^{-\\\\Delta^2-1}(\\\\e-1)}{\\\\Delta^2}}{\\\\ln\\\\frac{2\\\\sqrt{K}}{\\\\Delta^2\\\\delta}\\\\e^{\\\\frac{(\\\\e-1)\\\\Delta^2}{2\\\\e}}}\\\\\\\\\\n \\u003e\\u0026 \\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{224\\\\sqrt{K}e^{-2}(\\\\e-1)/\\\\Delta^2}{\\\\ln\\\\frac{2\\\\sqrt{K}}{\\\\Delta^2\\\\delta}\\\\e^{\\\\frac{\\\\e-1}{2\\\\e}}}\\\\ \\\\ \\\\left(\\\\text{by $\\\\Delta\\u003c 1$}\\\\right)\\\\\\\\\\n \\u003e \\u0026 \\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{52\\\\sqrt{K}/\\\\Delta^2}{\\\\ln\\\\frac{3\\\\sqrt{K}}{\\\\Delta^2\\\\delta}}\\\\ \\\\ \\\\left(\\\\text{by $224\\\\e^{-2}(\\\\e-1)\\u003e52$ and $2\\\\e^{\\\\frac{\\\\e-1}{2\\\\e}}\\u003c3$}\\\\right)\\\\\\\\\\n = \\u0026 \\\\frac{2}{\\\\Delta^2}\\\\left(\\\\ln\\\\frac{52\\\\sqrt{K}}{\\\\Delta^2}-\\\\ln\\\\ln\\\\frac{3\\\\sqrt{K}}{\\\\Delta^2\\\\delta}\\\\right).\\\\end{aligned}$$ $\\\\Box$\\n\\nProof of Theorem \\\\[thwelldef\\\\] {#app:thwelldef}\\n==============================\\n\\nProposition \\\\[prop:2\\\\] is used in the proof of Lemma \\\\[lem:E\\\\*\\\\]. The following proposition is needed to prove Proposition \\\\[prop:2\\\\].\\n\\n\\\\[ineqZNBcoVb1\\\\] For $0\\u003ca\\u003c1$, any $t \\\\geq \\\\frac{\\\\e}{(\\\\e-1)a} \\\\ln\\\\frac{1}{a}$ satisfies the following inequality. $$a t \\\\ge \\\\ln t.$$\\n\\nFor $0\\u003ca\\u003c1$, let $f(t) = a t - \\\\ln{t}$. When $a \\u003e \\\\frac{1}{\\\\e}$, $f(t)$ is always positive for any $t \\u003e 0$ since $f(t)$ takes minimum value $1 - \\\\ln{\\\\frac{1}{a}}$ at $t = \\\\frac{1}{a}$.\\n\\nWhen $a \\\\le \\\\frac{1}{\\\\e}$, if $t = \\\\frac{\\\\e}{(\\\\e-1)a} \\\\ln\\\\frac{1}{a}$, $$\\\\begin{aligned}\\n a t -\\\\ln t = \\\\left(\\\\frac{1}{\\\\e - 1} \\\\ln{\\\\frac{1}{a}} - \\\\ln{\\\\frac{\\\\e}{\\\\e-1}}\\\\right) - \\\\ln{\\\\ln{\\\\frac{1}{a}}} \\\\ge 0\\n \\\\end{aligned}$$ holds because $y=\\\\frac{1}{\\\\e - 1}x - \\\\ln\\\\frac{\\\\e}{\\\\e-1}$ is a tangential line of $y=\\\\ln x$ at $x=\\\\e-1$. If $t \\u003e \\\\frac{\\\\e}{(\\\\e-1)a} \\\\ln\\\\frac{1}{a} \\\\left(\\\\ge \\\\frac{\\\\e}{(\\\\e-1)a} \\u003e \\\\frac{1}{a}\\\\right)$, $\\\\frac{d f(t)}{d t} = a - \\\\frac{1}{t}$ is positive. Therefore, for $t \\\\ge \\\\frac{\\\\e}{(\\\\e-1)a} \\\\ln\\\\frac{1}{a}$, $a t -\\\\ln t \\\\ge 0$. $\\\\Box$\\n\\n\\\\[prop:2\\\\] $\\\\Td{\\\\Delta}\\\\leq \\\\Ndelta$.\\n\\nThe following inequality is derived from Proposition \\\\[ineqZNBcoVb1\\\\] by setting $a$ to $\\\\frac{\\\\Delta^2\\\\delta}{2\\\\sqrt{K}}$ that means $t = \\\\frac{\\\\sqrt{K}\\\\Ndelta}{\\\\delta} \\\\ge \\\\frac{2\\\\e\\\\sqrt{K}}{(\\\\e-1)\\\\Delta^2\\\\delta} \\\\ln\\\\frac{2\\\\sqrt{K}}{\\\\Delta^2\\\\delta}$,\\n\\n$$\\\\begin{aligned}\\n \\\\ln\\\\frac{\\\\sqrt{K}\\\\Ndelta}{\\\\delta} \\\\le \\\\frac{\\\\Delta^2 \\\\delta}{2\\\\sqrt{K}} \\\\cdot\\\\frac{\\\\sqrt{K}\\\\Ndelta}{\\\\delta}=\\\\frac{\\\\Delta^2\\\\Ndelta}{2}.\\\\end{aligned}$$\\n\\nThus, $$\\\\begin{aligned}\\n\\\\Ndelta\\\\geq \\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{\\\\sqrt{K}\\\\Ndelta}{\\\\delta}\\n \\\\end{aligned}$$ holds, and so $$\\\\begin{aligned}\\n\\\\Ndelta\\\\geq \\\\left\\\\lceil\\\\frac{2}{\\\\Delta^2}\\\\ln\\\\frac{\\\\sqrt{K}\\\\Ndelta}{\\\\delta}\\\\right\\\\rceil=\\\\Td{\\\\Delta}\\n \\\\end{aligned}$$ holds.\\n\\n$\\\\Box$\\n\\n\\\\[lem:E\\\\*\\\\] For the complementary events $\\\\overline{\\\\Ev^+}$, $\\\\overline{\\\\Ev^-}$ of events $\\\\Ev^+$, $\\\\Ev^-$, inequality $\\\\P\\\\{\\\\overline{\\\\Ev^+}\\\\}\\\\leq \\\\delta$ holds", 'input_ids': [69, 7, 87, 47505, 6852, 31944, 90, 6852, 18755, 2124, 18477, 87, 92, 26867, 5239, 90, 318, 24030, 329, 1782, 2124, 6852, 469, 80, 26867, 68, 6852, 3506, 8, 13426, 59, 77, 28, 3467, 84, 405, 2075, 6852, 42430, 6852, 31166, 17034, 90, 6852, 31944, 90, 16, 18477, 6852, 18755, 6852, 31944, 90, 31115, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 11709, 6852, 18755, 6852, 9464, 7, 6852, 31944, 90, 23, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 92, 6852, 9464, 7, 6852, 18755, 6852, 31944, 90, 31115, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 92, 6852, 3506, 8, 61, 17, 6852, 3506, 38165, 13426, 59, 77, 59, 84, 11245, 68, 3467, 84, 405, 2075, 6852, 42430, 6852, 31166, 17034, 90, 6852, 31944, 90, 16, 18477, 6852, 18755, 6852, 31944, 90, 31115, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 11709, 6852, 18755, 6852, 9464, 7, 6852, 31944, 90, 23, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 92, 6852, 10210, 313, 7265, 6852, 3506, 38165, 6852, 26867, 26867, 26867, 9464, 7, 6852, 5239, 90, 1525, 720, 6852, 9464, 7, 6852, 18755, 6852, 31944, 90, 31115, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 92, 6852, 3506, 8, 61, 17, 6852, 0, 59, 84, 11245, 68, 6852, 0, 26867, 9464, 7, 6852, 18755, 6852, 31944, 90, 31115, 6852, 10210, 313, 362, 18477, 16, 61, 19, 7, 6852, 31944, 90, 16, 18477, 17, 30072, 92, 6852, 3506, 8, 61, 17, 6852, 0, 28, 6852, 0, 3980, 13, 1157], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [69, 7, 87, 47505, 6852, 31944, 90, 6852, 18755, 2124, 18477, 87, 92, 26867, 5239, 90, 318, 24030, 329, 1782, 2124, 6852, 469, 80, 26867, 68, 6852, 3506, 8, 13426, 59, 77, 28, 3467, 84, 405, 2075, 6852, 42430, 6852, 31166, 17034, 90, 6852, 31944, 90, 16, 18477, 6852, 18755, 6852, 31944, 90, 31115, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 11709, 6852, 18755, 6852, 9464, 7, 6852, 31944, 90, 23, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 92, 6852, 9464, 7, 6852, 18755, 6852, 31944, 90, 31115, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 92, 6852, 3506, 8, 61, 17, 6852, 3506, 38165, 13426, 59, 77, 59, 84, 11245, 68, 3467, 84, 405, 2075, 6852, 42430, 6852, 31166, 17034, 90, 6852, 31944, 90, 16, 18477, 6852, 18755, 6852, 31944, 90, 31115, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 11709, 6852, 18755, 6852, 9464, 7, 6852, 31944, 90, 23, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 92, 6852, 10210, 313, 7265, 6852, 3506, 38165, 6852, 26867, 26867, 26867, 9464, 7, 6852, 5239, 90, 1525, 720, 6852, 9464, 7, 6852, 18755, 6852, 31944, 90, 31115, 42, 18477, 6852, 42430, 61, 19, 6852, 67, 12514, 92, 6852, 3506, 8, 61, 17, 6852, 0, 59, 84, 11245, 68, 6852, 0, 26867, 9464, 7, 6852, 18755, 6852, 31944, 90, 31115, 6852, 10210, 313, 362, 18477, 16, 61, 19, 7, 6852, 31944, 90, 16, 18477, 17, 30072, 92, 6852, 3506, 8, 61, 17, 6852, 0, 28, 6852, 0, 3980, 13, 1157]}: 

[2022-11-17 20:34:57,662][__main__][INFO] - f(x)=\\frac{\\ln x}{x} \\text{ is decreasing for } x\\geq \\e\\right)\\\\\n= \u0026\\Delta\\sqrt{\\frac{1}{\\ln\\frac{448K}{\\Delta^4\\delta}}\\ln\\left(\\frac{8K}{\\Delta^4\\delta}\\left(\\ln\\frac{448K}{\\Delta^4\\delta}\\right)^2\\right)}\\\\\n\u003e \u0026\\Delta\\sqrt{\\frac{1}{\\ln\\frac{448K}{\\Delta^4\\delta}}\\ln\\left(\\frac{8K}{\\Delta^4\\delta}\\cdot 56\\right)}\\ \\ \\ \\left(\\text{by $\\left(\\ln\\frac{448K}{\\Delta^4\\delta}\\right)^2\\!\u003e\\! \\left(\\ln\\frac{448\\cdot 2}{1^4(\\frac{1}{2})}\\right)^2\\!=\\!56.11
[2022-11-17 20:34:57,663][__main__][INFO] - Sample 7 of the training set: {'text': 'bedroom. Handy as an en-\\nsuite, awkward if you had friends to stay.\\n\\n------\\ndowntide\\nThere\'s plenty of space in the UK for housing. Just go up in a plane to look.\\nThe greenbelt curtails sprawl, and inflates housing value.\\n\\nI don\'t think low-density has to even be that bad. Dumping the inner/town city\\nroad network would be a huge boon, freeing that space for green leafy\\nwalk/cycle/scooter/ways and parks.\\n\\n~~~\\nzelos\\nCombined with the enormous levels of NIMBYism in the UK, it\'s amazing anything\\ngets built at all.\\n\\n------\\ndowntide\\nThe floor plan shows a knock through, that\'s quite non-typical, non-original.\\n\\n\\u0018\\ufffdf"} {"Links":[],"Data":"\\u0008\\u0002\\u0012\\ufffd\\ufffd\\u0003$\\\\;$\\n\\n[**Effects of heavy modes on vacuum\\\\\\nstability in supersymmetric theories**]{}\\n\\n[ and [**Claudio A. Scrucca**]{}]{}\\\\\\n\\n[*Institut de Théorie des Phénomènes Physiques\\\\\\nEcole Polytechnique Fédérale de Lausanne\\\\\\nCH-1015 Lausanne, Switzerland\\\\\\n*]{}\\n\\n**Abstract**\\n\\n\\u003e We study the effects induced by heavy fields on the masses of light fields in supersymmetric theories, under the assumption that the heavy mass scale is much higher than the supersymmetry breaking scale. We show that the square-masses of light scalar fields can get two different types of significant corrections when a heavy multiplet is integrated out. The first is an indirect level-repulsion effect, which may arise from heavy chiral multiplets and is always negative. The second is a direct coupling contribution, which may arise from heavy vector multiplets and can have any sign. We then apply these results to the sGoldstino mass and study the implications for the vacuum metastability condition. We find that the correction from heavy chiral multiplets is always negative and tends to compromise vacuum metastability, whereas the contribution from heavy vector multiplets is always positive and tends on the contrary', 'input_ids': [36269, 13, 7157, 88, 355, 281, 551, 12, 59, 77, 2385, 578, 11, 13006, 611, 345, 550, 2460, 284, 2652, 13, 59, 77, 59, 77, 23031, 59, 358, 6887, 485, 59, 77, 1858, 338, 6088, 286, 2272, 287, 262, 3482, 329, 5627, 13, 2329, 467, 510, 287, 257, 6614, 284, 804, 13, 59, 77, 464, 4077, 37976, 1090, 26404, 599, 13132, 11, 290, 28472, 689, 5627, 1988, 13, 59, 77, 59, 77, 40, 836, 470, 892, 1877, 12, 43337, 468, 284, 772, 307, 326, 2089, 13, 360, 25218, 262, 8434, 14, 12735, 1748, 59, 77, 6344, 3127, 561, 307, 257, 3236, 38181, 11, 35800, 326, 2272, 329, 4077, 12835, 88, 59, 77, 11152, 14, 13696, 14, 1416, 25141, 14, 1322, 290, 14860, 13, 59, 77, 59, 77, 4907, 93, 59, 77, 17396, 418, 59, 77, 20575, 1389, 351, 262, 9812, 2974, 286, 399, 3955, 17513, 1042, 287, 262, 3482, 11, 340, 338, 4998, 1997, 59, 782, 1039, 3170, 379, 477, 13, 59, 77, 59, 77, 23031, 59, 358, 6887, 485, 59, 77, 464, 4314, 1410, 2523, 257, 10643, 832, 11, 326, 338, 2407, 1729, 12, 28004, 605, 11, 1729, 12, 14986, 13, 59, 77, 59, 77, 59, 84, 405, 1507, 59, 1648, 69, 7568, 20662, 19779, 31815, 20598, 17241, 6601, 2404, 59, 84, 830, 23, 59, 84, 34215, 59, 84, 405, 1065, 59, 1648, 16344, 59, 1648, 16344, 59, 84, 830, 18, 3, 6852, 26, 3, 59, 77, 59, 77, 58, 1174, 47738, 286, 4334, 12881, 319, 17076, 6852, 59, 77, 301, 1799, 287, 22754, 26621], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [36269, 13, 7157, 88, 355, 281, 551, 12, 59, 77, 2385, 578, 11, 13006, 611, 345, 550, 2460, 284, 2652, 13, 59, 77, 59, 77, 23031, 59, 358, 6887, 485, 59, 77, 1858, 338, 6088, 286, 2272, 287, 262, 3482, 329, 5627, 13, 2329, 467, 510, 287, 257, 6614, 284, 804, 13, 59, 77, 464, 4077, 37976, 1090, 26404, 599, 13132, 11, 290, 28472, 689, 5627, 1988, 13, 59, 77, 59, 77, 40, 836, 470, 892, 1877, 12, 43337, 468, 284, 772, 307, 326, 2089, 13, 360, 25218, 262, 8434, 14, 12735, 1748, 59, 77, 6344, 3127, 561, 307, 257, 3236, 38181, 11, 35800, 326, 2272, 329, 4077, 12835, 88, 59, 77, 11152, 14, 13696, 14, 1416, 25141, 14, 1322, 290, 14860, 13, 59, 77, 59, 77, 4907, 93, 59, 77, 17396, 418, 59, 77, 20575, 1389, 351, 262, 9812, 2974, 286, 399, 3955, 17513, 1042, 287, 262, 3482, 11, 340, 338, 4998, 1997, 59, 782, 1039, 3170, 379, 477, 13, 59, 77, 59, 77, 23031, 59, 358, 6887, 485, 59, 77, 464, 4314, 1410, 2523, 257, 10643, 832, 11, 326, 338, 2407, 1729, 12, 28004, 605, 11, 1729, 12, 14986, 13, 59, 77, 59, 77, 59, 84, 405, 1507, 59, 1648, 69, 7568, 20662, 19779, 31815, 20598, 17241, 6601, 2404, 59, 84, 830, 23, 59, 84, 34215, 59, 84, 405, 1065, 59, 1648, 16344, 59, 1648, 16344, 59, 84, 830, 18, 3, 6852, 26, 3, 59, 77, 59, 77, 58, 1174, 47738, 286, 4334, 12881, 319, 17076, 6852, 59, 77, 301, 1799, 287, 22754, 26621]}: 

[2022-11-17 20:34:57,664][__main__][INFO] - bedroom. Handy as an en-\nsuite, awkward if you had friends to stay.\n\n------\ndowntide\nThere's plenty of space in the UK for housing. Just go up in a plane to look.\nThe greenbelt curtails sprawl, and inflates housing value.\n\nI don't think low-density has to even be that bad. Dumping the inner/town city\nroad network would be a huge boon, freeing that space for green leafy\nwalk/cycle/scooter/ways and parks.\n\n~~~\nzelos\nCombined with the enormous levels of NIMBYism in the UK, it's amazing anything\ngets built at all.\n\n------\ndowntide\nThe floor plan shows a knock through, that's quite non-typical, non-original.\n\n\u0018\ufffdf"} {"Links":[],"Data":"\u0008\u0002\u0012\ufffd\ufffd\u0003$\\;$\n\n[**Effects of heavy modes on vacuum\\\nstability in supersymm
[2022-11-17 20:34:57,664][__main__][INFO] - Sample 2508 of the training set: {'text': 'sketch out in section 3.2), we then show that the bounded distance isometries can glue together to give a global isometry which is still at a bounded distance from the original quasi-isometry. From such a statement, standard methods yield a quasi-isometry classification.\\n\\nThe main theorem.\\n=================\\n\\nIn this section, we provide a proof of Theorem 1.1. We start by noting that one direction of the conjecture stated in the introduction is easy to prove:\\n\\nLet $X^n$ a simple, thick $n$-dimensional P-manifold, and let $p$ be a point in the boundary at infinity of $\\\\tilde X^n$. If $\\\\gamma$ is a geodesic ray contained entirely in a connected lift $\\\\tilde B$ of $X_{n-1}$, then $\\\\gamma(\\\\infty)$ is $(n-1)$-branching.\\n\\nThis is easy to show: by the thickness hypothesis, there are at least three distinct chambers $\\\\tilde W_i$ containing $\\\\tilde B$ in their closure. For each of these chambers, we can consider the various boundary components of $\\\\tilde W_i$. To each boundary component distinct from $\\\\tilde B$, we can again use the thickness hypothesis to find chambers incident to each of the boundary components. Extending this procedure, we see that we can find three totally geodesic subset in $\\\\tilde X^n$ glued together along the codimension one strata $\\\\tilde B$. Furthermore, the simplicity assumption implies that $\\\\tilde B$ is isometric to ${\\\\mathbb H}^{n-1}$, while each of the three totally geodesic subsets is isometric to a “half” ${\\\\mathbb H}^n$. This implies that, in the boundary at infinity, there are three embedded disks ${\\\\mathbb D}^{n-1}$ glued along their boundary to $S^{n-1}\\\\cong \\\\partial ^\\\\infty \\\\tilde B$. It is now immediate', 'input_ids': [82, 7126, 354, 503, 287, 2665, 513, 13, 17, 828, 356, 788, 905, 326, 262, 49948, 5253, 318, 908, 1678, 460, 22749, 1978, 284, 1577, 257, 3298, 318, 15748, 543, 318, 991, 379, 257, 49948, 5253, 422, 262, 2656, 32551, 12, 271, 15748, 13, 3574, 884, 257, 2643, 11, 3210, 5050, 7800, 257, 32551, 12, 271, 15748, 17923, 13, 59, 77, 59, 77, 464, 1388, 44728, 13, 59, 77, 4770, 28, 59, 77, 59, 77, 818, 428, 2665, 11, 356, 2148, 257, 6617, 286, 383, 29625, 352, 13, 16, 13, 775, 923, 416, 10820, 326, 530, 4571, 286, 262, 46768, 5081, 287, 262, 9793, 318, 2562, 284, 5879, 7479, 77, 59, 77, 5756, 720, 55, 61, 77, 3, 257, 2829, 11, 6546, 720, 77, 3, 12, 19577, 350, 12, 805, 361, 727, 11, 290, 1309, 720, 79, 3, 307, 257, 966, 287, 262, 18645, 379, 37174, 286, 720, 6852, 83, 44725, 1395, 61, 77, 35307, 1002, 720, 6852, 28483, 2611, 3, 318, 257, 4903, 4147, 291, 26842, 7763, 5000, 287, 257, 5884, 10303, 720, 6852, 83, 44725, 347, 3, 286, 720, 55, 23330, 77, 12, 16, 92, 47113, 788, 720, 6852, 28483, 2611, 7, 6852, 259, 19628, 8, 3, 318, 29568, 77, 12, 16, 8, 3, 12, 1671, 3702, 278, 13, 59, 77, 59, 77, 1212, 318, 2562, 284, 905, 25, 416, 262, 20735, 14078, 11, 612, 389, 379, 1551, 1115, 7310, 23204, 720, 6852, 83, 44725, 370, 62, 72, 3, 7268, 720, 6852, 83, 44725, 347, 3, 287, 511, 16512, 13, 1114, 1123, 286, 777], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [82, 7126, 354, 503, 287, 2665, 513, 13, 17, 828, 356, 788, 905, 326, 262, 49948, 5253, 318, 908, 1678, 460, 22749, 1978, 284, 1577, 257, 3298, 318, 15748, 543, 318, 991, 379, 257, 49948, 5253, 422, 262, 2656, 32551, 12, 271, 15748, 13, 3574, 884, 257, 2643, 11, 3210, 5050, 7800, 257, 32551, 12, 271, 15748, 17923, 13, 59, 77, 59, 77, 464, 1388, 44728, 13, 59, 77, 4770, 28, 59, 77, 59, 77, 818, 428, 2665, 11, 356, 2148, 257, 6617, 286, 383, 29625, 352, 13, 16, 13, 775, 923, 416, 10820, 326, 530, 4571, 286, 262, 46768, 5081, 287, 262, 9793, 318, 2562, 284, 5879, 7479, 77, 59, 77, 5756, 720, 55, 61, 77, 3, 257, 2829, 11, 6546, 720, 77, 3, 12, 19577, 350, 12, 805, 361, 727, 11, 290, 1309, 720, 79, 3, 307, 257, 966, 287, 262, 18645, 379, 37174, 286, 720, 6852, 83, 44725, 1395, 61, 77, 35307, 1002, 720, 6852, 28483, 2611, 3, 318, 257, 4903, 4147, 291, 26842, 7763, 5000, 287, 257, 5884, 10303, 720, 6852, 83, 44725, 347, 3, 286, 720, 55, 23330, 77, 12, 16, 92, 47113, 788, 720, 6852, 28483, 2611, 7, 6852, 259, 19628, 8, 3, 318, 29568, 77, 12, 16, 8, 3, 12, 1671, 3702, 278, 13, 59, 77, 59, 77, 1212, 318, 2562, 284, 905, 25, 416, 262, 20735, 14078, 11, 612, 389, 379, 1551, 1115, 7310, 23204, 720, 6852, 83, 44725, 370, 62, 72, 3, 7268, 720, 6852, 83, 44725, 347, 3, 287, 511, 16512, 13, 1114, 1123, 286, 777]}: 

[2022-11-17 20:34:57,665][__main__][INFO] - sketch out in section 3.2), we then show that the bounded distance isometries can glue together to give a global isometry which is still at a bounded distance from the original quasi-isometry. From such a statement, standard methods yield a quasi-isometry classification.\n\nThe main theorem.\n=================\n\nIn this section, we provide a proof of Theorem 1.1. We start by noting that one direction of the conjecture stated in the introduction is easy to prove:\n\nLet $X^n$ a simple, thick $n$-dimensional P-manifold, and let $p$ be a point in the boundary at infinity of $\\tilde X^n$. If $\\gamma$ is a geodesic ray contained entirely in a connected lift $\\tilde B$ of $X_{n-1}$, then $\\gamma(\\infty)$ is $(n-1)$-branching.\n\nThis is easy to show: by the thickness hypothesis, there are at least three distinct chambers $\\tilde W_i$ containing $\\tilde B$ in their closure. For each of these
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)

wandb: Currently logged in as: joshuapaperspace. Use `wandb login --relogin` to force relogin
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
wandb: wandb version 0.13.5 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /home/paperspace/Documents/Repos/clm_model_tuning/wandb/run-20221117_203501-3irnetjb
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run distinctive-snow-44
wandb: ⭐️ View project at https://wandb.ai/joshuapaperspace/finetune_using_clm
wandb: 🚀 View run at https://wandb.ai/joshuapaperspace/finetune_using_clm/runs/3irnetjb
[2022-11-17 20:35:01,825][__main__][INFO] - ***** Running training *****
[2022-11-17 20:35:01,825][__main__][INFO] -   Num examples = 2800
[2022-11-17 20:35:01,825][__main__][INFO] -   Num Epochs = 10
[2022-11-17 20:35:01,825][__main__][INFO] -   Gradient Accumulation steps = 1
[2022-11-17 20:35:01,825][__main__][INFO] -   Total optimization steps = 3500
  0%|          | 0/3500 [00:00<?, ?it/s]  0%|          | 1/3500 [00:11<11:30:23, 11.84s/it]a
b
c
[2022-11-17 20:36:01,965][__main__][INFO] - epoch 0: perplexity: 9.057472925911544 train_loss: 2.6342060565948486 eval_loss: 2.203590154647827
Configuration saved in tuned-model/epoch_0_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_0_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_0_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_0_most_recent/special_tokens_map.json
  0%|          | 2/3500 [01:32<50:50:32, 52.32s/it]  0%|          | 3/3500 [01:34<28:32:01, 29.37s/it]  0%|          | 4/3500 [01:36<18:03:17, 18.59s/it]  0%|          | 5/3500 [01:38<12:15:44, 12.63s/it]  0%|          | 6/3500 [01:40<8:46:14,  9.04s/it]   0%|          | 7/3500 [01:42<6:33:20,  6.76s/it]  0%|          | 8/3500 [01:44<5:06:13,  5.26s/it]  0%|          | 9/3500 [01:46<4:07:58,  4.26s/it]  0%|          | 10/3500 [01:49<3:28:26,  3.58s/it]  0%|          | 11/3500 [01:51<3:01:17,  3.12s/it]  0%|          | 12/3500 [01:53<2:42:40,  2.80s/it]  0%|          | 13/3500 [01:55<2:29:39,  2.58s/it]  0%|          | 14/3500 [01:57<2:20:36,  2.42s/it]  0%|          | 15/3500 [01:59<2:14:18,  2.31s/it]  0%|          | 16/3500 [02:01<2:09:54,  2.24s/it]  0%|          | 17/3500 [02:03<2:06:59,  2.19s/it]  1%|          | 18/3500 [02:05<2:04:52,  2.15s/it]  1%|          | 19/3500 [02:07<2:03:15,  2.12s/it]  1%|          | 20/3500 [02:09<2:02:09,  2.11s/it]  1%|          | 21/3500 [02:11<2:01:28,  2.09s/it]  1%|          | 22/3500 [02:13<2:00:50,  2.08s/it]  1%|          | 23/3500 [02:15<2:00:24,  2.08s/it]  1%|          | 24/3500 [02:17<2:00:04,  2.07s/it]  1%|          | 25/3500 [02:19<1:59:51,  2.07s/it]  1%|          | 26/3500 [02:22<1:59:40,  2.07s/it]  1%|          | 27/3500 [02:24<1:59:32,  2.07s/it]  1%|          | 28/3500 [02:26<1:59:27,  2.06s/it]  1%|          | 29/3500 [02:28<1:59:23,  2.06s/it]  1%|          | 30/3500 [02:30<1:59:20,  2.06s/it]  1%|          | 31/3500 [02:32<1:59:16,  2.06s/it]  1%|          | 32/3500 [02:34<1:59:13,  2.06s/it]  1%|          | 33/3500 [02:36<1:59:12,  2.06s/it]  1%|          | 34/3500 [02:38<1:59:09,  2.06s/it]  1%|          | 35/3500 [02:40<1:59:08,  2.06s/it]  1%|          | 36/3500 [02:42<1:59:05,  2.06s/it]  1%|          | 37/3500 [02:44<1:59:01,  2.06s/it]  1%|          | 38/3500 [02:46<1:58:58,  2.06s/it]  1%|          | 39/3500 [02:48<1:58:56,  2.06s/it]  1%|          | 40/3500 [02:50<1:58:54,  2.06s/it]  1%|          | 41/3500 [02:52<1:58:51,  2.06s/it]  1%|          | 42/3500 [02:55<1:58:47,  2.06s/it]  1%|          | 43/3500 [02:57<1:58:45,  2.06s/it]  1%|▏         | 44/3500 [02:59<1:58:43,  2.06s/it]  1%|▏         | 45/3500 [03:01<1:58:42,  2.06s/it]  1%|▏         | 46/3500 [03:03<1:58:40,  2.06s/it]  1%|▏         | 47/3500 [03:05<1:58:40,  2.06s/it]  1%|▏         | 48/3500 [03:07<1:58:37,  2.06s/it]  1%|▏         | 49/3500 [03:09<1:58:38,  2.06s/it]  1%|▏         | 50/3500 [03:11<1:58:33,  2.06s/it]  1%|▏         | 51/3500 [03:13<1:58:31,  2.06s/it][2022-11-17 20:39:03,695][__main__][INFO] - epoch 0: perplexity: 7.563593622207677 train_loss: 2.0869321823120117 eval_loss: 2.023346424102783
Configuration saved in tuned-model/epoch_0_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_0_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_0_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_0_most_recent/special_tokens_map.json
  1%|▏         | 52/3500 [04:36<25:11:02, 26.29s/it]  2%|▏         | 53/3500 [04:38<18:12:55, 19.02s/it]  2%|▏         | 54/3500 [04:40<13:20:19, 13.93s/it]  2%|▏         | 55/3500 [04:42<9:55:34, 10.37s/it]   2%|▏         | 56/3500 [04:44<7:32:16,  7.88s/it]  2%|▏         | 57/3500 [04:46<5:51:59,  6.13s/it]  2%|▏         | 58/3500 [04:48<4:41:48,  4.91s/it]  2%|▏         | 59/3500 [04:50<3:52:41,  4.06s/it]  2%|▏         | 60/3500 [04:52<3:18:17,  3.46s/it]  2%|▏         | 61/3500 [04:54<2:54:14,  3.04s/it]  2%|▏         | 62/3500 [04:57<2:37:30,  2.75s/it]  2%|▏         | 63/3500 [04:59<2:25:39,  2.54s/it]  2%|▏         | 64/3500 [05:01<2:17:21,  2.40s/it]  2%|▏         | 65/3500 [05:03<2:11:30,  2.30s/it]  2%|▏         | 66/3500 [05:05<2:07:26,  2.23s/it]  2%|▏         | 67/3500 [05:07<2:04:34,  2.18s/it]  2%|▏         | 68/3500 [05:09<2:02:33,  2.14s/it]  2%|▏         | 69/3500 [05:11<2:01:07,  2.12s/it]  2%|▏         | 70/3500 [05:13<2:00:07,  2.10s/it]  2%|▏         | 71/3500 [05:15<1:59:25,  2.09s/it]  2%|▏         | 72/3500 [05:17<1:58:55,  2.08s/it]  2%|▏         | 73/3500 [05:19<1:58:33,  2.08s/it]  2%|▏         | 74/3500 [05:21<1:58:18,  2.07s/it]  2%|▏         | 75/3500 [05:23<1:58:09,  2.07s/it]  2%|▏         | 76/3500 [05:25<1:57:59,  2.07s/it]  2%|▏         | 77/3500 [05:27<1:57:51,  2.07s/it]  2%|▏         | 78/3500 [05:30<1:57:47,  2.07s/it]  2%|▏         | 79/3500 [05:32<1:57:41,  2.06s/it]  2%|▏         | 80/3500 [05:34<1:57:37,  2.06s/it]  2%|▏         | 81/3500 [05:36<1:57:32,  2.06s/it]  2%|▏         | 82/3500 [05:38<1:57:30,  2.06s/it]  2%|▏         | 83/3500 [05:40<1:57:28,  2.06s/it]  2%|▏         | 84/3500 [05:42<1:57:25,  2.06s/it]  2%|▏         | 85/3500 [05:44<1:57:22,  2.06s/it]  2%|▏         | 86/3500 [05:46<1:57:21,  2.06s/it]  2%|▏         | 87/3500 [05:48<1:57:17,  2.06s/it]  3%|▎         | 88/3500 [05:50<1:57:15,  2.06s/it]  3%|▎         | 89/3500 [05:52<1:57:13,  2.06s/it]  3%|▎         | 90/3500 [05:54<1:57:12,  2.06s/it]  3%|▎         | 91/3500 [05:56<1:57:10,  2.06s/it]  3%|▎         | 92/3500 [05:58<1:57:08,  2.06s/it]  3%|▎         | 93/3500 [06:00<1:57:08,  2.06s/it]  3%|▎         | 94/3500 [06:03<1:57:05,  2.06s/it]  3%|▎         | 95/3500 [06:05<1:57:00,  2.06s/it]  3%|▎         | 96/3500 [06:07<1:56:57,  2.06s/it]  3%|▎         | 97/3500 [06:09<1:56:56,  2.06s/it]  3%|▎         | 98/3500 [06:11<1:56:54,  2.06s/it]  3%|▎         | 99/3500 [06:13<1:56:52,  2.06s/it]  3%|▎         | 100/3500 [06:15<1:56:49,  2.06s/it]  3%|▎         | 101/3500 [06:17<1:56:46,  2.06s/it][2022-11-17 20:42:07,568][__main__][INFO] - epoch 0: perplexity: 7.320906069810028 train_loss: 2.0407137870788574 eval_loss: 1.9907341003417969
Configuration saved in tuned-model/epoch_0_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_0_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_0_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_0_most_recent/special_tokens_map.json
  3%|▎         | 102/3500 [07:40<24:44:29, 26.21s/it]  3%|▎         | 103/3500 [07:42<17:53:53, 18.97s/it]  3%|▎         | 104/3500 [07:44<13:06:30, 13.90s/it]  3%|▎         | 105/3500 [07:46<9:45:23, 10.35s/it]   3%|▎         | 106/3500 [07:48<7:24:36,  7.86s/it]  3%|▎         | 107/3500 [07:50<5:46:05,  6.12s/it]  3%|▎         | 108/3500 [07:52<4:37:08,  4.90s/it]  3%|▎         | 109/3500 [07:54<3:48:51,  4.05s/it]  3%|▎         | 110/3500 [07:56<3:15:04,  3.45s/it]  3%|▎         | 111/3500 [07:58<2:51:31,  3.04s/it]  3%|▎         | 112/3500 [08:00<2:35:07,  2.75s/it]  3%|▎         | 113/3500 [08:02<2:23:29,  2.54s/it]  3%|▎         | 114/3500 [08:04<2:15:23,  2.40s/it]  3%|▎         | 115/3500 [08:06<2:09:41,  2.30s/it]  3%|▎         | 116/3500 [08:08<2:05:43,  2.23s/it]  3%|▎         | 117/3500 [08:10<2:02:52,  2.18s/it]  3%|▎         | 118/3500 [08:13<2:00:52,  2.14s/it]  3%|▎         | 119/3500 [08:15<1:59:30,  2.12s/it]  3%|▎         | 120/3500 [08:17<1:58:30,  2.10s/it]  3%|▎         | 121/3500 [08:19<1:57:48,  2.09s/it]  3%|▎         | 122/3500 [08:21<1:57:25,  2.09s/it]  4%|▎         | 123/3500 [08:23<1:57:01,  2.08s/it]  4%|▎         | 124/3500 [08:25<1:56:44,  2.07s/it]  4%|▎         | 125/3500 [08:27<1:56:34,  2.07s/it]  4%|▎         | 126/3500 [08:29<1:56:24,  2.07s/it]  4%|▎         | 127/3500 [08:31<1:56:18,  2.07s/it]  4%|▎         | 128/3500 [08:33<1:56:12,  2.07s/it]  4%|▎         | 129/3500 [08:35<1:56:06,  2.07s/it]  4%|▎         | 130/3500 [08:37<1:56:01,  2.07s/it]  4%|▎         | 131/3500 [08:39<1:55:54,  2.06s/it]  4%|▍         | 132/3500 [08:41<1:55:48,  2.06s/it]  4%|▍         | 133/3500 [08:43<1:55:44,  2.06s/it]  4%|▍         | 134/3500 [08:46<1:55:41,  2.06s/it]  4%|▍         | 135/3500 [08:48<1:55:39,  2.06s/it]  4%|▍         | 136/3500 [08:50<1:55:36,  2.06s/it]  4%|▍         | 137/3500 [08:52<1:55:34,  2.06s/it]  4%|▍         | 138/3500 [08:54<1:55:31,  2.06s/it]  4%|▍         | 139/3500 [08:56<1:55:29,  2.06s/it]  4%|▍         | 140/3500 [08:58<1:55:28,  2.06s/it]  4%|▍         | 141/3500 [09:00<1:55:26,  2.06s/it]  4%|▍         | 142/3500 [09:02<1:55:23,  2.06s/it]  4%|▍         | 143/3500 [09:04<1:55:20,  2.06s/it]  4%|▍         | 144/3500 [09:06<1:55:18,  2.06s/it]  4%|▍         | 145/3500 [09:08<1:55:17,  2.06s/it]  4%|▍         | 146/3500 [09:10<1:55:14,  2.06s/it]  4%|▍         | 147/3500 [09:12<1:55:12,  2.06s/it]  4%|▍         | 148/3500 [09:14<1:55:10,  2.06s/it]  4%|▍         | 149/3500 [09:16<1:55:10,  2.06s/it]  4%|▍         | 150/3500 [09:19<1:55:08,  2.06s/it]  4%|▍         | 151/3500 [09:21<1:55:06,  2.06s/it][2022-11-17 20:45:11,239][__main__][INFO] - epoch 0: perplexity: 7.170730546813816 train_loss: 2.0405173301696777 eval_loss: 1.9700075387954712
Configuration saved in tuned-model/epoch_0_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_0_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_0_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_0_most_recent/special_tokens_map.json
  4%|▍         | 152/3500 [10:44<24:29:21, 26.33s/it]  4%|▍         | 153/3500 [10:46<17:42:43, 19.05s/it]  4%|▍         | 154/3500 [10:48<12:58:10, 13.95s/it]  4%|▍         | 155/3500 [10:50<9:39:01, 10.39s/it]   4%|▍         | 156/3500 [10:52<7:19:41,  7.89s/it]  4%|▍         | 157/3500 [10:54<5:42:09,  6.14s/it]  5%|▍         | 158/3500 [10:56<4:33:52,  4.92s/it]  5%|▍         | 159/3500 [10:58<3:46:06,  4.06s/it]  5%|▍         | 160/3500 [11:00<3:12:39,  3.46s/it]  5%|▍         | 161/3500 [11:02<2:49:13,  3.04s/it]  5%|▍         | 162/3500 [11:04<2:32:50,  2.75s/it]  5%|▍         | 163/3500 [11:06<2:21:21,  2.54s/it]  5%|▍         | 164/3500 [11:08<2:13:19,  2.40s/it]  5%|▍         | 165/3500 [11:10<2:07:42,  2.30s/it]  5%|▍         | 166/3500 [11:12<2:03:45,  2.23s/it]  5%|▍         | 167/3500 [11:14<2:00:57,  2.18s/it]  5%|▍         | 168/3500 [11:17<1:59:01,  2.14s/it]  5%|▍         | 169/3500 [11:19<1:57:36,  2.12s/it]  5%|▍         | 170/3500 [11:21<1:56:36,  2.10s/it]  5%|▍         | 171/3500 [11:23<1:55:54,  2.09s/it]  5%|▍         | 172/3500 [11:25<1:55:23,  2.08s/it]  5%|▍         | 173/3500 [11:27<1:55:02,  2.07s/it]  5%|▍         | 174/3500 [11:29<1:54:48,  2.07s/it]  5%|▌         | 175/3500 [11:31<1:54:38,  2.07s/it]  5%|▌         | 176/3500 [11:33<1:54:33,  2.07s/it]  5%|▌         | 177/3500 [11:35<1:54:25,  2.07s/it]  5%|▌         | 178/3500 [11:37<1:54:19,  2.06s/it]  5%|▌         | 179/3500 [11:39<1:54:13,  2.06s/it]  5%|▌         | 180/3500 [11:41<1:54:10,  2.06s/it]  5%|▌         | 181/3500 [11:43<1:54:06,  2.06s/it]  5%|▌         | 182/3500 [11:45<1:54:11,  2.06s/it]  5%|▌         | 183/3500 [11:47<1:54:07,  2.06s/it]  5%|▌         | 184/3500 [11:50<1:54:04,  2.06s/it]  5%|▌         | 185/3500 [11:52<1:54:04,  2.06s/it]  5%|▌         | 186/3500 [11:54<1:53:59,  2.06s/it]  5%|▌         | 187/3500 [11:56<1:53:56,  2.06s/it]  5%|▌         | 188/3500 [11:58<1:53:55,  2.06s/it]  5%|▌         | 189/3500 [12:00<1:53:54,  2.06s/it]  5%|▌         | 190/3500 [12:02<1:53:50,  2.06s/it]  5%|▌         | 191/3500 [12:04<1:53:46,  2.06s/it]  5%|▌         | 192/3500 [12:06<1:53:44,  2.06s/it]  6%|▌         | 193/3500 [12:08<1:53:41,  2.06s/it]  6%|▌         | 194/3500 [12:10<1:53:40,  2.06s/it]  6%|▌         | 195/3500 [12:12<1:53:39,  2.06s/it]  6%|▌         | 196/3500 [12:14<1:53:37,  2.06s/it]  6%|▌         | 197/3500 [12:16<1:53:43,  2.07s/it]  6%|▌         | 198/3500 [12:18<1:53:42,  2.07s/it]  6%|▌         | 199/3500 [12:21<1:53:36,  2.06s/it]  6%|▌         | 200/3500 [12:23<1:53:36,  2.07s/it]  6%|▌         | 201/3500 [12:25<1:53:31,  2.06s/it][2022-11-17 20:48:15,284][__main__][INFO] - epoch 0: perplexity: 7.059684909938791 train_loss: 2.0190517902374268 eval_loss: 1.9544004201889038
Configuration saved in tuned-model/epoch_0_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_0_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_0_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_0_most_recent/special_tokens_map.json
  6%|▌         | 202/3500 [13:48<24:15:09, 26.47s/it]  6%|▌         | 203/3500 [13:50<17:32:18, 19.15s/it]  6%|▌         | 204/3500 [13:52<12:50:23, 14.02s/it]  6%|▌         | 205/3500 [13:54<9:33:03, 10.44s/it]   6%|▌         | 206/3500 [13:56<7:14:57,  7.92s/it]  6%|▌         | 207/3500 [13:58<5:38:19,  6.16s/it]  6%|▌         | 208/3500 [14:00<4:30:40,  4.93s/it]  6%|▌         | 209/3500 [14:03<3:43:23,  4.07s/it]  6%|▌         | 210/3500 [14:05<3:10:13,  3.47s/it]  6%|▌         | 211/3500 [14:07<2:47:01,  3.05s/it]  6%|▌         | 212/3500 [14:09<2:30:46,  2.75s/it]  6%|▌         | 213/3500 [14:11<2:19:23,  2.54s/it]  6%|▌         | 214/3500 [14:13<2:11:24,  2.40s/it]  6%|▌         | 215/3500 [14:15<2:05:50,  2.30s/it]  6%|▌         | 216/3500 [14:17<2:01:53,  2.23s/it]  6%|▌         | 217/3500 [14:19<1:59:06,  2.18s/it]  6%|▌         | 218/3500 [14:21<1:57:11,  2.14s/it]  6%|▋         | 219/3500 [14:23<1:55:48,  2.12s/it]  6%|▋         | 220/3500 [14:25<1:54:51,  2.10s/it]  6%|▋         | 221/3500 [14:27<1:54:10,  2.09s/it]  6%|▋         | 222/3500 [14:29<1:53:40,  2.08s/it]  6%|▋         | 223/3500 [14:31<1:53:18,  2.07s/it]  6%|▋         | 224/3500 [14:33<1:53:02,  2.07s/it]  6%|▋         | 225/3500 [14:35<1:52:52,  2.07s/it]  6%|▋         | 226/3500 [14:38<1:52:44,  2.07s/it]  6%|▋         | 227/3500 [14:40<1:52:37,  2.06s/it]  7%|▋         | 228/3500 [14:42<1:52:32,  2.06s/it]  7%|▋         | 229/3500 [14:44<1:52:39,  2.07s/it]  7%|▋         | 230/3500 [14:46<1:52:32,  2.07s/it]  7%|▋         | 231/3500 [14:48<1:52:27,  2.06s/it]  7%|▋         | 232/3500 [14:50<1:52:22,  2.06s/it]  7%|▋         | 233/3500 [14:52<1:52:19,  2.06s/it]  7%|▋         | 234/3500 [14:54<1:52:17,  2.06s/it]  7%|▋         | 235/3500 [14:56<1:52:13,  2.06s/it]  7%|▋         | 236/3500 [14:58<1:52:12,  2.06s/it]  7%|▋         | 237/3500 [15:00<1:52:10,  2.06s/it]  7%|▋         | 238/3500 [15:02<1:52:05,  2.06s/it]  7%|▋         | 239/3500 [15:04<1:52:04,  2.06s/it]  7%|▋         | 240/3500 [15:06<1:52:06,  2.06s/it]  7%|▋         | 241/3500 [15:08<1:52:03,  2.06s/it]  7%|▋         | 242/3500 [15:11<1:52:00,  2.06s/it]  7%|▋         | 243/3500 [15:13<1:51:59,  2.06s/it]  7%|▋         | 244/3500 [15:15<1:51:56,  2.06s/it]  7%|▋         | 245/3500 [15:17<1:51:54,  2.06s/it]  7%|▋         | 246/3500 [15:19<1:51:51,  2.06s/it]  7%|▋         | 247/3500 [15:21<1:51:48,  2.06s/it]  7%|▋         | 248/3500 [15:23<1:51:46,  2.06s/it]  7%|▋         | 249/3500 [15:25<1:51:44,  2.06s/it]  7%|▋         | 250/3500 [15:27<1:51:41,  2.06s/it]  7%|▋         | 251/3500 [15:29<1:51:39,  2.06s/it][2022-11-17 20:51:19,773][__main__][INFO] - epoch 0: perplexity: 6.978812981488791 train_loss: 2.0052311420440674 eval_loss: 1.9428788423538208
Configuration saved in tuned-model/epoch_0_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_0_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_0_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_0_most_recent/special_tokens_map.json
  7%|▋         | 252/3500 [16:52<23:43:13, 26.29s/it]  7%|▋         | 253/3500 [16:54<17:09:24, 19.02s/it]  7%|▋         | 254/3500 [16:56<12:33:47, 13.93s/it]  7%|▋         | 255/3500 [16:58<9:20:55, 10.37s/it]   7%|▋         | 256/3500 [17:00<7:05:57,  7.88s/it]  7%|▋         | 257/3500 [17:02<5:31:30,  6.13s/it]  7%|▋         | 258/3500 [17:04<4:25:28,  4.91s/it]  7%|▋         | 259/3500 [17:06<3:39:11,  4.06s/it]  7%|▋         | 260/3500 [17:08<3:06:47,  3.46s/it]  7%|▋         | 261/3500 [17:11<2:44:05,  3.04s/it]  7%|▋         | 262/3500 [17:13<2:28:11,  2.75s/it]  8%|▊         | 263/3500 [17:15<2:17:02,  2.54s/it]  8%|▊         | 264/3500 [17:17<2:09:18,  2.40s/it]  8%|▊         | 265/3500 [17:19<2:03:55,  2.30s/it]  8%|▊         | 266/3500 [17:21<2:00:03,  2.23s/it]  8%|▊         | 267/3500 [17:23<1:57:21,  2.18s/it]  8%|▊         | 268/3500 [17:25<1:55:26,  2.14s/it]  8%|▊         | 269/3500 [17:27<1:54:05,  2.12s/it]  8%|▊         | 270/3500 [17:29<1:53:17,  2.10s/it]  8%|▊         | 271/3500 [17:31<1:52:34,  2.09s/it]  8%|▊         | 272/3500 [17:33<1:52:06,  2.08s/it]  8%|▊         | 273/3500 [17:35<1:51:43,  2.08s/it]  8%|▊         | 274/3500 [17:37<1:51:34,  2.08s/it]  8%|▊         | 275/3500 [17:39<1:51:28,  2.07s/it]  8%|▊         | 276/3500 [17:41<1:51:14,  2.07s/it]  8%|▊         | 277/3500 [17:44<1:51:04,  2.07s/it]  8%|▊         | 278/3500 [17:46<1:50:58,  2.07s/it]  8%|▊         | 279/3500 [17:48<1:50:51,  2.07s/it]  8%|▊         | 280/3500 [17:50<1:50:46,  2.06s/it]  8%|▊         | 281/3500 [17:52<1:50:41,  2.06s/it]  8%|▊         | 282/3500 [17:54<1:50:38,  2.06s/it]  8%|▊         | 283/3500 [17:56<1:50:36,  2.06s/it]  8%|▊         | 284/3500 [17:58<1:50:35,  2.06s/it]  8%|▊         | 285/3500 [18:00<1:50:32,  2.06s/it]  8%|▊         | 286/3500 [18:02<1:50:30,  2.06s/it]  8%|▊         | 287/3500 [18:04<1:50:30,  2.06s/it]  8%|▊         | 288/3500 [18:06<1:50:27,  2.06s/it]  8%|▊         | 289/3500 [18:08<1:50:28,  2.06s/it]  8%|▊         | 290/3500 [18:10<1:50:25,  2.06s/it]  8%|▊         | 291/3500 [18:12<1:50:21,  2.06s/it]  8%|▊         | 292/3500 [18:14<1:50:18,  2.06s/it]  8%|▊         | 293/3500 [18:17<1:50:16,  2.06s/it]  8%|▊         | 294/3500 [18:19<1:50:13,  2.06s/it]  8%|▊         | 295/3500 [18:21<1:50:13,  2.06s/it]  8%|▊         | 296/3500 [18:23<1:50:10,  2.06s/it]  8%|▊         | 297/3500 [18:25<1:50:07,  2.06s/it]  9%|▊         | 298/3500 [18:27<1:50:05,  2.06s/it]  9%|▊         | 299/3500 [18:29<1:50:02,  2.06s/it]  9%|▊         | 300/3500 [18:31<1:50:00,  2.06s/it]  9%|▊         | 301/3500 [18:33<1:49:57,  2.06s/it][2022-11-17 20:54:23,704][__main__][INFO] - epoch 0: perplexity: 6.897083903346766 train_loss: 2.002673387527466 eval_loss: 1.9310986995697021
Configuration saved in tuned-model/epoch_0_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_0_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_0_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_0_most_recent/special_tokens_map.json
  9%|▊         | 302/3500 [19:53<22:30:44, 25.34s/it]  9%|▊         | 303/3500 [19:55<16:18:13, 18.36s/it]  9%|▊         | 304/3500 [19:57<11:57:27, 13.47s/it]  9%|▊         | 305/3500 [19:59<8:55:01, 10.05s/it]   9%|▊         | 306/3500 [20:01<6:47:17,  7.65s/it]  9%|▉         | 307/3500 [20:03<5:17:54,  5.97s/it]  9%|▉         | 308/3500 [20:05<4:15:21,  4.80s/it]  9%|▉         | 309/3500 [20:07<3:31:34,  3.98s/it]  9%|▉         | 310/3500 [20:09<3:00:54,  3.40s/it]  9%|▉         | 311/3500 [20:11<2:39:27,  3.00s/it]  9%|▉         | 312/3500 [20:13<2:24:26,  2.72s/it]  9%|▉         | 313/3500 [20:15<2:13:55,  2.52s/it]  9%|▉         | 314/3500 [20:17<2:06:33,  2.38s/it]  9%|▉         | 315/3500 [20:19<2:01:23,  2.29s/it]  9%|▉         | 316/3500 [20:22<1:57:47,  2.22s/it]  9%|▉         | 317/3500 [20:24<1:55:12,  2.17s/it]  9%|▉         | 318/3500 [20:26<1:53:26,  2.14s/it]  9%|▉         | 319/3500 [20:28<1:52:09,  2.12s/it]  9%|▉         | 320/3500 [20:30<1:51:15,  2.10s/it]  9%|▉         | 321/3500 [20:32<1:50:39,  2.09s/it]  9%|▉         | 322/3500 [20:34<1:50:10,  2.08s/it]  9%|▉         | 323/3500 [20:36<1:49:50,  2.07s/it]  9%|▉         | 324/3500 [20:38<1:49:35,  2.07s/it]  9%|▉         | 325/3500 [20:40<1:49:25,  2.07s/it]  9%|▉         | 326/3500 [20:42<1:49:16,  2.07s/it]  9%|▉         | 327/3500 [20:44<1:49:10,  2.06s/it]  9%|▉         | 328/3500 [20:46<1:49:06,  2.06s/it]  9%|▉         | 329/3500 [20:48<1:49:01,  2.06s/it]  9%|▉         | 330/3500 [20:50<1:48:56,  2.06s/it]  9%|▉         | 331/3500 [20:52<1:48:53,  2.06s/it]  9%|▉         | 332/3500 [20:55<1:48:50,  2.06s/it] 10%|▉         | 333/3500 [20:57<1:48:46,  2.06s/it] 10%|▉         | 334/3500 [20:59<1:48:45,  2.06s/it] 10%|▉         | 335/3500 [21:01<1:48:41,  2.06s/it] 10%|▉         | 336/3500 [21:03<1:48:43,  2.06s/it] 10%|▉         | 337/3500 [21:05<1:48:38,  2.06s/it] 10%|▉         | 338/3500 [21:07<1:48:35,  2.06s/it] 10%|▉         | 339/3500 [21:09<1:48:33,  2.06s/it] 10%|▉         | 340/3500 [21:11<1:48:30,  2.06s/it] 10%|▉         | 341/3500 [21:13<1:48:28,  2.06s/it] 10%|▉         | 342/3500 [21:15<1:48:26,  2.06s/it] 10%|▉         | 343/3500 [21:17<1:48:25,  2.06s/it] 10%|▉         | 344/3500 [21:19<1:48:23,  2.06s/it] 10%|▉         | 345/3500 [21:21<1:48:20,  2.06s/it] 10%|▉         | 346/3500 [21:23<1:48:19,  2.06s/it] 10%|▉         | 347/3500 [21:25<1:48:16,  2.06s/it] 10%|▉         | 348/3500 [21:28<1:48:13,  2.06s/it] 10%|▉         | 349/3500 [21:30<1:48:12,  2.06s/it] 10%|█         | 350/3500 [21:32<1:48:06,  2.06s/it][2022-11-17 20:56:33,956][__main__][INFO] - done epoch 0
 10%|█         | 351/3500 [21:34<1:48:55,  2.08s/it][2022-11-17 20:57:24,629][__main__][INFO] - epoch 1: perplexity: 6.830704703654449 train_loss: 1.8400152921676636 eval_loss: 1.921427845954895
Configuration saved in tuned-model/epoch_1_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_1_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_1_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_1_most_recent/special_tokens_map.json
 10%|█         | 352/3500 [22:39<18:21:47, 21.00s/it] 10%|█         | 353/3500 [22:41<13:23:26, 15.32s/it] 10%|█         | 354/3500 [22:43<9:54:39, 11.34s/it]  10%|█         | 355/3500 [22:45<7:28:33,  8.56s/it] 10%|█         | 356/3500 [22:47<5:46:19,  6.61s/it] 10%|█         | 357/3500 [22:49<4:34:46,  5.25s/it] 10%|█         | 358/3500 [22:51<3:44:41,  4.29s/it] 10%|█         | 359/3500 [22:53<3:09:38,  3.62s/it] 10%|█         | 360/3500 [22:55<2:45:05,  3.15s/it] 10%|█         | 361/3500 [22:57<2:27:54,  2.83s/it] 10%|█         | 362/3500 [23:00<2:15:51,  2.60s/it] 10%|█         | 363/3500 [23:02<2:07:24,  2.44s/it] 10%|█         | 364/3500 [23:04<2:01:28,  2.32s/it] 10%|█         | 365/3500 [23:06<1:57:18,  2.25s/it] 10%|█         | 366/3500 [23:08<1:54:24,  2.19s/it] 10%|█         | 367/3500 [23:10<1:52:24,  2.15s/it] 11%|█         | 368/3500 [23:12<1:50:56,  2.13s/it] 11%|█         | 369/3500 [23:14<1:49:55,  2.11s/it] 11%|█         | 370/3500 [23:16<1:49:11,  2.09s/it] 11%|█         | 371/3500 [23:18<1:48:38,  2.08s/it] 11%|█         | 372/3500 [23:20<1:48:15,  2.08s/it] 11%|█         | 373/3500 [23:22<1:47:58,  2.07s/it] 11%|█         | 374/3500 [23:24<1:47:46,  2.07s/it] 11%|█         | 375/3500 [23:26<1:47:36,  2.07s/it] 11%|█         | 376/3500 [23:28<1:47:29,  2.06s/it] 11%|█         | 377/3500 [23:30<1:47:25,  2.06s/it] 11%|█         | 378/3500 [23:33<1:47:20,  2.06s/it] 11%|█         | 379/3500 [23:35<1:47:18,  2.06s/it] 11%|█         | 380/3500 [23:37<1:47:13,  2.06s/it] 11%|█         | 381/3500 [23:39<1:47:10,  2.06s/it] 11%|█         | 382/3500 [23:41<1:47:07,  2.06s/it] 11%|█         | 383/3500 [23:43<1:47:04,  2.06s/it] 11%|█         | 384/3500 [23:45<1:47:02,  2.06s/it] 11%|█         | 385/3500 [23:47<1:47:01,  2.06s/it] 11%|█         | 386/3500 [23:49<1:46:58,  2.06s/it] 11%|█         | 387/3500 [23:51<1:46:56,  2.06s/it] 11%|█         | 388/3500 [23:53<1:46:55,  2.06s/it] 11%|█         | 389/3500 [23:55<1:46:52,  2.06s/it] 11%|█         | 390/3500 [23:57<1:46:50,  2.06s/it] 11%|█         | 391/3500 [23:59<1:46:50,  2.06s/it] 11%|█         | 392/3500 [24:01<1:46:47,  2.06s/it] 11%|█         | 393/3500 [24:03<1:46:44,  2.06s/it] 11%|█▏        | 394/3500 [24:05<1:46:43,  2.06s/it] 11%|█▏        | 395/3500 [24:08<1:46:40,  2.06s/it] 11%|█▏        | 396/3500 [24:10<1:46:37,  2.06s/it] 11%|█▏        | 397/3500 [24:12<1:46:35,  2.06s/it] 11%|█▏        | 398/3500 [24:14<1:46:35,  2.06s/it] 11%|█▏        | 399/3500 [24:16<1:46:31,  2.06s/it] 11%|█▏        | 400/3500 [24:18<1:46:29,  2.06s/it] 11%|█▏        | 401/3500 [24:20<1:46:26,  2.06s/it][2022-11-17 21:00:10,507][__main__][INFO] - epoch 1: perplexity: 6.9970804456754845 train_loss: 1.6196638345718384 eval_loss: 1.9454929828643799
Configuration saved in tuned-model/epoch_1_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_1_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_1_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_1_most_recent/special_tokens_map.json
 11%|█▏        | 402/3500 [25:45<23:11:00, 26.94s/it] 12%|█▏        | 403/3500 [25:47<16:45:17, 19.48s/it] 12%|█▏        | 404/3500 [25:49<12:15:21, 14.25s/it] 12%|█▏        | 405/3500 [25:51<9:06:27, 10.59s/it]  12%|█▏        | 406/3500 [25:53<6:54:15,  8.03s/it] 12%|█▏        | 407/3500 [25:55<5:21:47,  6.24s/it] 12%|█▏        | 408/3500 [25:57<4:17:03,  4.99s/it] 12%|█▏        | 409/3500 [25:59<3:31:45,  4.11s/it] 12%|█▏        | 410/3500 [26:01<3:00:03,  3.50s/it] 12%|█▏        | 411/3500 [26:03<2:37:50,  3.07s/it] 12%|█▏        | 412/3500 [26:06<2:22:16,  2.76s/it] 12%|█▏        | 413/3500 [26:08<2:11:23,  2.55s/it] 12%|█▏        | 414/3500 [26:10<2:03:48,  2.41s/it] 12%|█▏        | 415/3500 [26:12<1:58:27,  2.30s/it] 12%|█▏        | 416/3500 [26:14<1:54:43,  2.23s/it] 12%|█▏        | 417/3500 [26:16<1:52:02,  2.18s/it] 12%|█▏        | 418/3500 [26:18<1:50:10,  2.15s/it] 12%|█▏        | 419/3500 [26:20<1:48:50,  2.12s/it] 12%|█▏        | 420/3500 [26:22<1:47:54,  2.10s/it] 12%|█▏        | 421/3500 [26:24<1:47:14,  2.09s/it] 12%|█▏        | 422/3500 [26:26<1:46:45,  2.08s/it] 12%|█▏        | 423/3500 [26:28<1:46:25,  2.08s/it] 12%|█▏        | 424/3500 [26:30<1:46:15,  2.07s/it] 12%|█▏        | 425/3500 [26:32<1:46:05,  2.07s/it] 12%|█▏        | 426/3500 [26:34<1:46:00,  2.07s/it] 12%|█▏        | 427/3500 [26:36<1:45:55,  2.07s/it] 12%|█▏        | 428/3500 [26:39<1:45:46,  2.07s/it] 12%|█▏        | 429/3500 [26:41<1:45:41,  2.07s/it] 12%|█▏        | 430/3500 [26:43<1:45:37,  2.06s/it] 12%|█▏        | 431/3500 [26:45<1:45:32,  2.06s/it] 12%|█▏        | 432/3500 [26:47<1:45:28,  2.06s/it] 12%|█▏        | 433/3500 [26:49<1:45:25,  2.06s/it] 12%|█▏        | 434/3500 [26:51<1:45:22,  2.06s/it] 12%|█▏        | 435/3500 [26:53<1:45:18,  2.06s/it] 12%|█▏        | 436/3500 [26:55<1:45:16,  2.06s/it] 12%|█▏        | 437/3500 [26:57<1:45:22,  2.06s/it] 13%|█▎        | 438/3500 [26:59<1:45:18,  2.06s/it] 13%|█▎        | 439/3500 [27:01<1:45:14,  2.06s/it] 13%|█▎        | 440/3500 [27:03<1:45:12,  2.06s/it] 13%|█▎        | 441/3500 [27:05<1:45:13,  2.06s/it] 13%|█▎        | 442/3500 [27:07<1:45:08,  2.06s/it] 13%|█▎        | 443/3500 [27:09<1:45:05,  2.06s/it] 13%|█▎        | 444/3500 [27:12<1:45:02,  2.06s/it] 13%|█▎        | 445/3500 [27:14<1:44:58,  2.06s/it] 13%|█▎        | 446/3500 [27:16<1:44:56,  2.06s/it] 13%|█▎        | 447/3500 [27:18<1:44:54,  2.06s/it] 13%|█▎        | 448/3500 [27:20<1:44:51,  2.06s/it] 13%|█▎        | 449/3500 [27:22<1:44:49,  2.06s/it] 13%|█▎        | 450/3500 [27:24<1:44:46,  2.06s/it] 13%|█▎        | 451/3500 [27:26<1:44:44,  2.06s/it][2022-11-17 21:03:16,592][__main__][INFO] - epoch 1: perplexity: 6.96829604930492 train_loss: 1.6215271949768066 eval_loss: 1.9413707256317139
Configuration saved in tuned-model/epoch_1_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_1_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_1_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_1_most_recent/special_tokens_map.json
 13%|█▎        | 452/3500 [28:52<23:03:36, 27.24s/it] 13%|█▎        | 453/3500 [28:54<16:39:35, 19.68s/it] 13%|█▎        | 454/3500 [28:56<12:10:50, 14.40s/it] 13%|█▎        | 455/3500 [28:58<9:02:48, 10.70s/it]  13%|█▎        | 456/3500 [29:00<6:51:12,  8.11s/it] 13%|█▎        | 457/3500 [29:02<5:19:06,  6.29s/it] 13%|█▎        | 458/3500 [29:04<4:14:37,  5.02s/it] 13%|█▎        | 459/3500 [29:06<3:29:30,  4.13s/it] 13%|█▎        | 460/3500 [29:08<2:57:54,  3.51s/it] 13%|█▎        | 461/3500 [29:10<2:35:48,  3.08s/it] 13%|█▎        | 462/3500 [29:13<2:20:19,  2.77s/it] 13%|█▎        | 463/3500 [29:15<2:09:28,  2.56s/it] 13%|█▎        | 464/3500 [29:17<2:01:54,  2.41s/it] 13%|█▎        | 465/3500 [29:19<1:56:36,  2.31s/it] 13%|█▎        | 466/3500 [29:21<1:52:54,  2.23s/it] 13%|█▎        | 467/3500 [29:23<1:50:19,  2.18s/it] 13%|█▎        | 468/3500 [29:25<1:48:26,  2.15s/it] 13%|█▎        | 469/3500 [29:27<1:47:07,  2.12s/it] 13%|█▎        | 470/3500 [29:29<1:46:12,  2.10s/it] 13%|█▎        | 471/3500 [29:31<1:45:31,  2.09s/it] 13%|█▎        | 472/3500 [29:33<1:45:02,  2.08s/it] 14%|█▎        | 473/3500 [29:35<1:44:42,  2.08s/it] 14%|█▎        | 474/3500 [29:37<1:44:27,  2.07s/it] 14%|█▎        | 475/3500 [29:39<1:44:16,  2.07s/it] 14%|█▎        | 476/3500 [29:41<1:44:10,  2.07s/it] 14%|█▎        | 477/3500 [29:43<1:44:04,  2.07s/it] 14%|█▎        | 478/3500 [29:46<1:43:58,  2.06s/it] 14%|█▎        | 479/3500 [29:48<1:43:52,  2.06s/it] 14%|█▎        | 480/3500 [29:50<1:43:47,  2.06s/it] 14%|█▎        | 481/3500 [29:52<1:43:45,  2.06s/it] 14%|█▍        | 482/3500 [29:54<1:43:43,  2.06s/it] 14%|█▍        | 483/3500 [29:56<1:43:41,  2.06s/it] 14%|█▍        | 484/3500 [29:58<1:43:38,  2.06s/it] 14%|█▍        | 485/3500 [30:00<1:43:34,  2.06s/it] 14%|█▍        | 486/3500 [30:02<1:43:32,  2.06s/it] 14%|█▍        | 487/3500 [30:04<1:43:29,  2.06s/it] 14%|█▍        | 488/3500 [30:06<1:43:26,  2.06s/it] 14%|█▍        | 489/3500 [30:08<1:43:23,  2.06s/it] 14%|█▍        | 490/3500 [30:10<1:43:21,  2.06s/it] 14%|█▍        | 491/3500 [30:12<1:43:18,  2.06s/it] 14%|█▍        | 492/3500 [30:14<1:43:17,  2.06s/it] 14%|█▍        | 493/3500 [30:16<1:43:16,  2.06s/it] 14%|█▍        | 494/3500 [30:18<1:43:13,  2.06s/it] 14%|█▍        | 495/3500 [30:21<1:43:11,  2.06s/it] 14%|█▍        | 496/3500 [30:23<1:43:09,  2.06s/it] 14%|█▍        | 497/3500 [30:25<1:43:07,  2.06s/it] 14%|█▍        | 498/3500 [30:27<1:43:07,  2.06s/it] 14%|█▍        | 499/3500 [30:29<1:43:06,  2.06s/it] 14%|█▍        | 500/3500 [30:31<1:43:03,  2.06s/it] 14%|█▍        | 501/3500 [30:33<1:43:00,  2.06s/it][2022-11-17 21:06:23,549][__main__][INFO] - epoch 1: perplexity: 6.960686973001626 train_loss: 1.6055606603622437 eval_loss: 1.940278172492981
Configuration saved in tuned-model/epoch_1_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_1_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_1_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_1_most_recent/special_tokens_map.json
 14%|█▍        | 502/3500 [31:58<22:22:03, 26.86s/it] 14%|█▍        | 503/3500 [32:00<16:10:01, 19.42s/it] 14%|█▍        | 504/3500 [32:02<11:49:39, 14.21s/it] 14%|█▍        | 505/3500 [32:04<8:47:26, 10.57s/it]  14%|█▍        | 506/3500 [32:06<6:39:55,  8.01s/it] 14%|█▍        | 507/3500 [32:08<5:10:44,  6.23s/it] 15%|█▍        | 508/3500 [32:10<4:08:17,  4.98s/it] 15%|█▍        | 509/3500 [32:12<3:24:37,  4.10s/it] 15%|█▍        | 510/3500 [32:14<2:53:59,  3.49s/it] 15%|█▍        | 511/3500 [32:16<2:32:32,  3.06s/it] 15%|█▍        | 512/3500 [32:18<2:17:32,  2.76s/it] 15%|█▍        | 513/3500 [32:20<2:07:01,  2.55s/it] 15%|█▍        | 514/3500 [32:22<1:59:39,  2.40s/it] 15%|█▍        | 515/3500 [32:24<1:54:30,  2.30s/it] 15%|█▍        | 516/3500 [32:27<1:50:52,  2.23s/it] 15%|█▍        | 517/3500 [32:29<1:48:19,  2.18s/it] 15%|█▍        | 518/3500 [32:31<1:46:31,  2.14s/it] 15%|█▍        | 519/3500 [32:33<1:45:16,  2.12s/it] 15%|█▍        | 520/3500 [32:35<1:44:22,  2.10s/it] 15%|█▍        | 521/3500 [32:37<1:43:44,  2.09s/it] 15%|█▍        | 522/3500 [32:39<1:43:16,  2.08s/it] 15%|█▍        | 523/3500 [32:41<1:42:57,  2.08s/it] 15%|█▍        | 524/3500 [32:43<1:42:43,  2.07s/it] 15%|█▌        | 525/3500 [32:45<1:42:33,  2.07s/it] 15%|█▌        | 526/3500 [32:47<1:42:23,  2.07s/it] 15%|█▌        | 527/3500 [32:49<1:42:16,  2.06s/it] 15%|█▌        | 528/3500 [32:51<1:42:12,  2.06s/it] 15%|█▌        | 529/3500 [32:53<1:42:09,  2.06s/it] 15%|█▌        | 530/3500 [32:55<1:42:05,  2.06s/it] 15%|█▌        | 531/3500 [32:57<1:42:02,  2.06s/it] 15%|█▌        | 532/3500 [32:59<1:41:59,  2.06s/it] 15%|█▌        | 533/3500 [33:02<1:41:57,  2.06s/it] 15%|█▌        | 534/3500 [33:04<1:41:53,  2.06s/it] 15%|█▌        | 535/3500 [33:06<1:41:51,  2.06s/it] 15%|█▌        | 536/3500 [33:08<1:41:50,  2.06s/it] 15%|█▌        | 537/3500 [33:10<1:41:49,  2.06s/it] 15%|█▌        | 538/3500 [33:12<1:41:46,  2.06s/it] 15%|█▌        | 539/3500 [33:14<1:41:44,  2.06s/it] 15%|█▌        | 540/3500 [33:16<1:41:43,  2.06s/it] 15%|█▌        | 541/3500 [33:18<1:41:41,  2.06s/it] 15%|█▌        | 542/3500 [33:20<1:41:38,  2.06s/it] 16%|█▌        | 543/3500 [33:22<1:41:34,  2.06s/it] 16%|█▌        | 544/3500 [33:24<1:41:32,  2.06s/it] 16%|█▌        | 545/3500 [33:26<1:41:29,  2.06s/it] 16%|█▌        | 546/3500 [33:28<1:41:27,  2.06s/it] 16%|█▌        | 547/3500 [33:30<1:41:24,  2.06s/it] 16%|█▌        | 548/3500 [33:32<1:41:21,  2.06s/it] 16%|█▌        | 549/3500 [33:35<1:41:20,  2.06s/it] 16%|█▌        | 550/3500 [33:37<1:41:19,  2.06s/it] 16%|█▌        | 551/3500 [33:39<1:41:16,  2.06s/it][2022-11-17 21:09:29,292][__main__][INFO] - epoch 1: perplexity: 6.951356563808237 train_loss: 1.6096218824386597 eval_loss: 1.9389368295669556
Configuration saved in tuned-model/epoch_1_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_1_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_1_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_1_most_recent/special_tokens_map.json
 16%|█▌        | 552/3500 [35:03<21:59:15, 26.85s/it] 16%|█▌        | 553/3500 [35:05<15:53:31, 19.41s/it] 16%|█▌        | 554/3500 [35:07<11:37:35, 14.21s/it] 16%|█▌        | 555/3500 [35:10<8:38:30, 10.56s/it]  16%|█▌        | 556/3500 [35:12<6:33:10,  8.01s/it] 16%|█▌        | 557/3500 [35:14<5:05:26,  6.23s/it] 16%|█▌        | 558/3500 [35:16<4:04:04,  4.98s/it] 16%|█▌        | 559/3500 [35:18<3:21:05,  4.10s/it] 16%|█▌        | 560/3500 [35:20<2:51:00,  3.49s/it] 16%|█▌        | 561/3500 [35:22<2:29:56,  3.06s/it] 16%|█▌        | 562/3500 [35:24<2:15:12,  2.76s/it] 16%|█▌        | 563/3500 [35:26<2:04:52,  2.55s/it] 16%|█▌        | 564/3500 [35:28<1:57:38,  2.40s/it] 16%|█▌        | 565/3500 [35:30<1:52:37,  2.30s/it] 16%|█▌        | 566/3500 [35:32<1:49:03,  2.23s/it] 16%|█▌        | 567/3500 [35:34<1:46:32,  2.18s/it] 16%|█▌        | 568/3500 [35:36<1:44:46,  2.14s/it] 16%|█▋        | 569/3500 [35:38<1:43:34,  2.12s/it] 16%|█▋        | 570/3500 [35:40<1:42:42,  2.10s/it] 16%|█▋        | 571/3500 [35:43<1:42:03,  2.09s/it] 16%|█▋        | 572/3500 [35:45<1:41:35,  2.08s/it] 16%|█▋        | 573/3500 [35:47<1:41:16,  2.08s/it] 16%|█▋        | 574/3500 [35:49<1:41:03,  2.07s/it] 16%|█▋        | 575/3500 [35:51<1:40:51,  2.07s/it] 16%|█▋        | 576/3500 [35:53<1:40:42,  2.07s/it] 16%|█▋        | 577/3500 [35:55<1:40:36,  2.07s/it] 17%|█▋        | 578/3500 [35:57<1:40:33,  2.06s/it] 17%|█▋        | 579/3500 [35:59<1:40:28,  2.06s/it] 17%|█▋        | 580/3500 [36:01<1:40:24,  2.06s/it] 17%|█▋        | 581/3500 [36:03<1:40:21,  2.06s/it] 17%|█▋        | 582/3500 [36:05<1:40:19,  2.06s/it] 17%|█▋        | 583/3500 [36:07<1:40:15,  2.06s/it] 17%|█▋        | 584/3500 [36:09<1:40:11,  2.06s/it] 17%|█▋        | 585/3500 [36:11<1:40:09,  2.06s/it] 17%|█▋        | 586/3500 [36:13<1:40:05,  2.06s/it] 17%|█▋        | 587/3500 [36:15<1:40:04,  2.06s/it] 17%|█▋        | 588/3500 [36:18<1:40:02,  2.06s/it] 17%|█▋        | 589/3500 [36:20<1:40:00,  2.06s/it] 17%|█▋        | 590/3500 [36:22<1:39:59,  2.06s/it] 17%|█▋        | 591/3500 [36:24<1:39:58,  2.06s/it] 17%|█▋        | 592/3500 [36:26<1:39:55,  2.06s/it] 17%|█▋        | 593/3500 [36:28<1:39:53,  2.06s/it] 17%|█▋        | 594/3500 [36:30<1:39:51,  2.06s/it] 17%|█▋        | 595/3500 [36:32<1:39:47,  2.06s/it] 17%|█▋        | 596/3500 [36:34<1:39:44,  2.06s/it] 17%|█▋        | 597/3500 [36:36<1:39:45,  2.06s/it] 17%|█▋        | 598/3500 [36:38<1:39:43,  2.06s/it] 17%|█▋        | 599/3500 [36:40<1:39:39,  2.06s/it] 17%|█▋        | 600/3500 [36:42<1:39:38,  2.06s/it] 17%|█▋        | 601/3500 [36:44<1:39:36,  2.06s/it][2022-11-17 21:12:34,987][__main__][INFO] - epoch 1: perplexity: 6.921007640397578 train_loss: 1.6121994256973267 eval_loss: 1.9345613718032837
Configuration saved in tuned-model/epoch_1_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_1_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_1_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_1_most_recent/special_tokens_map.json
 17%|█▋        | 602/3500 [38:08<21:27:50, 26.66s/it] 17%|█▋        | 603/3500 [38:10<15:31:01, 19.28s/it] 17%|█▋        | 604/3500 [38:13<11:21:19, 14.12s/it] 17%|█▋        | 605/3500 [38:15<8:26:35, 10.50s/it]  17%|█▋        | 606/3500 [38:17<6:24:20,  7.97s/it] 17%|█▋        | 607/3500 [38:19<4:58:45,  6.20s/it] 17%|█▋        | 608/3500 [38:21<3:58:50,  4.96s/it] 17%|█▋        | 609/3500 [38:23<3:16:55,  4.09s/it] 17%|█▋        | 610/3500 [38:25<2:47:35,  3.48s/it] 17%|█▋        | 611/3500 [38:27<2:27:00,  3.05s/it] 17%|█▋        | 612/3500 [38:29<2:12:38,  2.76s/it] 18%|█▊        | 613/3500 [38:31<2:02:34,  2.55s/it] 18%|█▊        | 614/3500 [38:33<1:55:30,  2.40s/it] 18%|█▊        | 615/3500 [38:35<1:50:33,  2.30s/it] 18%|█▊        | 616/3500 [38:37<1:47:04,  2.23s/it] 18%|█▊        | 617/3500 [38:39<1:44:37,  2.18s/it] 18%|█▊        | 618/3500 [38:41<1:42:56,  2.14s/it] 18%|█▊        | 619/3500 [38:43<1:41:44,  2.12s/it] 18%|█▊        | 620/3500 [38:46<1:40:51,  2.10s/it] 18%|█▊        | 621/3500 [38:48<1:40:14,  2.09s/it] 18%|█▊        | 622/3500 [38:50<1:39:48,  2.08s/it] 18%|█▊        | 623/3500 [38:52<1:39:30,  2.08s/it] 18%|█▊        | 624/3500 [38:54<1:39:16,  2.07s/it] 18%|█▊        | 625/3500 [38:56<1:39:06,  2.07s/it] 18%|█▊        | 626/3500 [38:58<1:38:57,  2.07s/it] 18%|█▊        | 627/3500 [39:00<1:38:52,  2.06s/it] 18%|█▊        | 628/3500 [39:02<1:38:47,  2.06s/it] 18%|█▊        | 629/3500 [39:04<1:38:44,  2.06s/it] 18%|█▊        | 630/3500 [39:06<1:38:40,  2.06s/it] 18%|█▊        | 631/3500 [39:08<1:38:39,  2.06s/it] 18%|█▊        | 632/3500 [39:10<1:38:35,  2.06s/it] 18%|█▊        | 633/3500 [39:12<1:38:32,  2.06s/it] 18%|█▊        | 634/3500 [39:14<1:38:29,  2.06s/it] 18%|█▊        | 635/3500 [39:16<1:38:28,  2.06s/it] 18%|█▊        | 636/3500 [39:19<1:38:25,  2.06s/it] 18%|█▊        | 637/3500 [39:21<1:38:26,  2.06s/it] 18%|█▊        | 638/3500 [39:23<1:38:23,  2.06s/it] 18%|█▊        | 639/3500 [39:25<1:38:23,  2.06s/it] 18%|█▊        | 640/3500 [39:27<1:38:20,  2.06s/it] 18%|█▊        | 641/3500 [39:29<1:38:16,  2.06s/it] 18%|█▊        | 642/3500 [39:31<1:38:11,  2.06s/it] 18%|█▊        | 643/3500 [39:33<1:38:10,  2.06s/it] 18%|█▊        | 644/3500 [39:35<1:38:07,  2.06s/it] 18%|█▊        | 645/3500 [39:37<1:38:04,  2.06s/it] 18%|█▊        | 646/3500 [39:39<1:38:01,  2.06s/it] 18%|█▊        | 647/3500 [39:41<1:37:59,  2.06s/it] 19%|█▊        | 648/3500 [39:43<1:37:58,  2.06s/it] 19%|█▊        | 649/3500 [39:45<1:37:56,  2.06s/it] 19%|█▊        | 650/3500 [39:47<1:37:54,  2.06s/it] 19%|█▊        | 651/3500 [39:49<1:37:51,  2.06s/it][2022-11-17 21:15:40,089][__main__][INFO] - epoch 1: perplexity: 6.851275917681181 train_loss: 1.6192972660064697 eval_loss: 1.9244349002838135
Configuration saved in tuned-model/epoch_1_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_1_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_1_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_1_most_recent/special_tokens_map.json
 19%|█▊        | 652/3500 [41:14<21:13:58, 26.84s/it] 19%|█▊        | 653/3500 [41:16<15:20:48, 19.41s/it] 19%|█▊        | 654/3500 [41:18<11:13:38, 14.20s/it] 19%|█▊        | 655/3500 [41:20<8:20:41, 10.56s/it]  19%|█▊        | 656/3500 [41:22<6:19:40,  8.01s/it] 19%|█▉        | 657/3500 [41:24<4:55:00,  6.23s/it] 19%|█▉        | 658/3500 [41:26<3:55:42,  4.98s/it] 19%|█▉        | 659/3500 [41:29<3:14:12,  4.10s/it] 19%|█▉        | 660/3500 [41:31<2:45:09,  3.49s/it] 19%|█▉        | 661/3500 [41:33<2:24:50,  3.06s/it] 19%|█▉        | 662/3500 [41:35<2:10:36,  2.76s/it] 19%|█▉        | 663/3500 [41:37<2:00:38,  2.55s/it] 19%|█▉        | 664/3500 [41:39<1:53:40,  2.41s/it] 19%|█▉        | 665/3500 [41:41<1:48:47,  2.30s/it] 19%|█▉        | 666/3500 [41:43<1:45:20,  2.23s/it] 19%|█▉        | 667/3500 [41:45<1:42:54,  2.18s/it] 19%|█▉        | 668/3500 [41:47<1:41:11,  2.14s/it] 19%|█▉        | 669/3500 [41:49<1:39:59,  2.12s/it] 19%|█▉        | 670/3500 [41:51<1:39:07,  2.10s/it] 19%|█▉        | 671/3500 [41:53<1:38:31,  2.09s/it] 19%|█▉        | 672/3500 [41:55<1:38:06,  2.08s/it] 19%|█▉        | 673/3500 [41:57<1:37:47,  2.08s/it] 19%|█▉        | 674/3500 [41:59<1:37:37,  2.07s/it] 19%|█▉        | 675/3500 [42:02<1:37:27,  2.07s/it] 19%|█▉        | 676/3500 [42:04<1:37:18,  2.07s/it] 19%|█▉        | 677/3500 [42:06<1:37:12,  2.07s/it] 19%|█▉        | 678/3500 [42:08<1:37:06,  2.06s/it] 19%|█▉        | 679/3500 [42:10<1:37:02,  2.06s/it] 19%|█▉        | 680/3500 [42:12<1:36:59,  2.06s/it] 19%|█▉        | 681/3500 [42:14<1:36:58,  2.06s/it] 19%|█▉        | 682/3500 [42:16<1:36:54,  2.06s/it] 20%|█▉        | 683/3500 [42:18<1:36:50,  2.06s/it] 20%|█▉        | 684/3500 [42:20<1:36:46,  2.06s/it] 20%|█▉        | 685/3500 [42:22<1:36:44,  2.06s/it] 20%|█▉        | 686/3500 [42:24<1:36:41,  2.06s/it] 20%|█▉        | 687/3500 [42:26<1:36:38,  2.06s/it] 20%|█▉        | 688/3500 [42:28<1:36:36,  2.06s/it] 20%|█▉        | 689/3500 [42:30<1:36:35,  2.06s/it] 20%|█▉        | 690/3500 [42:32<1:36:34,  2.06s/it] 20%|█▉        | 691/3500 [42:35<1:36:31,  2.06s/it] 20%|█▉        | 692/3500 [42:37<1:36:31,  2.06s/it] 20%|█▉        | 693/3500 [42:39<1:36:28,  2.06s/it] 20%|█▉        | 694/3500 [42:41<1:36:26,  2.06s/it] 20%|█▉        | 695/3500 [42:43<1:36:24,  2.06s/it] 20%|█▉        | 696/3500 [42:45<1:36:21,  2.06s/it] 20%|█▉        | 697/3500 [42:47<1:36:20,  2.06s/it] 20%|█▉        | 698/3500 [42:49<1:36:18,  2.06s/it] 20%|█▉        | 699/3500 [42:51<1:36:15,  2.06s/it] 20%|██        | 700/3500 [42:53<1:36:10,  2.06s/it][2022-11-17 21:17:55,388][__main__][INFO] - done epoch 1
 20%|██        | 701/3500 [42:55<1:37:04,  2.08s/it][2022-11-17 21:18:46,181][__main__][INFO] - epoch 2: perplexity: 6.824720714341395 train_loss: 1.56263267993927 eval_loss: 1.9205514192581177
Configuration saved in tuned-model/epoch_2_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_2_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_2_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_2_most_recent/special_tokens_map.json
 20%|██        | 702/3500 [44:01<16:29:38, 21.22s/it] 20%|██        | 703/3500 [44:03<12:01:21, 15.47s/it] 20%|██        | 704/3500 [44:05<8:53:33, 11.45s/it]  20%|██        | 705/3500 [44:07<6:42:09,  8.63s/it] 20%|██        | 706/3500 [44:09<5:10:17,  6.66s/it] 20%|██        | 707/3500 [44:11<4:05:59,  5.28s/it] 20%|██        | 708/3500 [44:13<3:20:56,  4.32s/it] 20%|██        | 709/3500 [44:16<2:49:24,  3.64s/it] 20%|██        | 710/3500 [44:18<2:27:21,  3.17s/it] 20%|██        | 711/3500 [44:20<2:11:56,  2.84s/it] 20%|██        | 712/3500 [44:22<2:01:07,  2.61s/it] 20%|██        | 713/3500 [44:24<1:53:31,  2.44s/it] 20%|██        | 714/3500 [44:26<1:48:11,  2.33s/it] 20%|██        | 715/3500 [44:28<1:44:27,  2.25s/it] 20%|██        | 716/3500 [44:30<1:41:47,  2.19s/it] 20%|██        | 717/3500 [44:32<1:39:54,  2.15s/it] 21%|██        | 718/3500 [44:34<1:38:35,  2.13s/it] 21%|██        | 719/3500 [44:36<1:37:40,  2.11s/it] 21%|██        | 720/3500 [44:38<1:37:06,  2.10s/it] 21%|██        | 721/3500 [44:40<1:36:35,  2.09s/it] 21%|██        | 722/3500 [44:42<1:36:14,  2.08s/it] 21%|██        | 723/3500 [44:44<1:35:59,  2.07s/it] 21%|██        | 724/3500 [44:46<1:35:48,  2.07s/it] 21%|██        | 725/3500 [44:49<1:35:40,  2.07s/it] 21%|██        | 726/3500 [44:51<1:35:31,  2.07s/it] 21%|██        | 727/3500 [44:53<1:35:24,  2.06s/it] 21%|██        | 728/3500 [44:55<1:35:19,  2.06s/it] 21%|██        | 729/3500 [44:57<1:35:16,  2.06s/it] 21%|██        | 730/3500 [44:59<1:35:12,  2.06s/it] 21%|██        | 731/3500 [45:01<1:35:10,  2.06s/it] 21%|██        | 732/3500 [45:03<1:35:07,  2.06s/it] 21%|██        | 733/3500 [45:05<1:35:04,  2.06s/it] 21%|██        | 734/3500 [45:07<1:35:01,  2.06s/it] 21%|██        | 735/3500 [45:09<1:34:59,  2.06s/it] 21%|██        | 736/3500 [45:11<1:34:56,  2.06s/it] 21%|██        | 737/3500 [45:13<1:34:55,  2.06s/it] 21%|██        | 738/3500 [45:15<1:34:53,  2.06s/it] 21%|██        | 739/3500 [45:17<1:34:53,  2.06s/it] 21%|██        | 740/3500 [45:19<1:34:50,  2.06s/it] 21%|██        | 741/3500 [45:22<1:34:48,  2.06s/it] 21%|██        | 742/3500 [45:24<1:34:45,  2.06s/it] 21%|██        | 743/3500 [45:26<1:34:43,  2.06s/it] 21%|██▏       | 744/3500 [45:28<1:34:41,  2.06s/it] 21%|██▏       | 745/3500 [45:30<1:34:41,  2.06s/it] 21%|██▏       | 746/3500 [45:32<1:34:39,  2.06s/it] 21%|██▏       | 747/3500 [45:34<1:34:38,  2.06s/it] 21%|██▏       | 748/3500 [45:36<1:34:36,  2.06s/it] 21%|██▏       | 749/3500 [45:38<1:34:33,  2.06s/it] 21%|██▏       | 750/3500 [45:40<1:34:31,  2.06s/it] 21%|██▏       | 751/3500 [45:42<1:34:29,  2.06s/it][2022-11-17 21:21:32,764][__main__][INFO] - epoch 2: perplexity: 7.662103791726185 train_loss: 1.2335327863693237 eval_loss: 2.0362865924835205
Configuration saved in tuned-model/epoch_2_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_2_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_2_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_2_most_recent/special_tokens_map.json
 21%|██▏       | 752/3500 [47:08<20:46:30, 27.22s/it] 22%|██▏       | 753/3500 [47:10<15:00:33, 19.67s/it] 22%|██▏       | 754/3500 [47:12<10:58:27, 14.39s/it] 22%|██▏       | 755/3500 [47:14<8:09:02, 10.69s/it]  22%|██▏       | 756/3500 [47:16<6:10:28,  8.10s/it] 22%|██▏       | 757/3500 [47:18<4:47:29,  6.29s/it] 22%|██▏       | 758/3500 [47:20<3:49:25,  5.02s/it] 22%|██▏       | 759/3500 [47:22<3:08:46,  4.13s/it] 22%|██▏       | 760/3500 [47:25<2:40:19,  3.51s/it] 22%|██▏       | 761/3500 [47:27<2:20:24,  3.08s/it] 22%|██▏       | 762/3500 [47:29<2:06:26,  2.77s/it] 22%|██▏       | 763/3500 [47:31<1:56:39,  2.56s/it] 22%|██▏       | 764/3500 [47:33<1:49:49,  2.41s/it] 22%|██▏       | 765/3500 [47:35<1:45:01,  2.30s/it] 22%|██▏       | 766/3500 [47:37<1:41:40,  2.23s/it] 22%|██▏       | 767/3500 [47:39<1:39:19,  2.18s/it] 22%|██▏       | 768/3500 [47:41<1:37:39,  2.14s/it] 22%|██▏       | 769/3500 [47:43<1:36:28,  2.12s/it] 22%|██▏       | 770/3500 [47:45<1:35:39,  2.10s/it] 22%|██▏       | 771/3500 [47:47<1:35:03,  2.09s/it] 22%|██▏       | 772/3500 [47:49<1:34:39,  2.08s/it] 22%|██▏       | 773/3500 [47:51<1:34:20,  2.08s/it] 22%|██▏       | 774/3500 [47:53<1:34:07,  2.07s/it] 22%|██▏       | 775/3500 [47:55<1:33:56,  2.07s/it] 22%|██▏       | 776/3500 [47:58<1:33:48,  2.07s/it] 22%|██▏       | 777/3500 [48:00<1:33:42,  2.06s/it] 22%|██▏       | 778/3500 [48:02<1:33:37,  2.06s/it] 22%|██▏       | 779/3500 [48:04<1:33:32,  2.06s/it] 22%|██▏       | 780/3500 [48:06<1:33:29,  2.06s/it] 22%|██▏       | 781/3500 [48:08<1:33:26,  2.06s/it] 22%|██▏       | 782/3500 [48:10<1:33:23,  2.06s/it] 22%|██▏       | 783/3500 [48:12<1:33:21,  2.06s/it] 22%|██▏       | 784/3500 [48:14<1:33:20,  2.06s/it] 22%|██▏       | 785/3500 [48:16<1:33:17,  2.06s/it] 22%|██▏       | 786/3500 [48:18<1:33:16,  2.06s/it] 22%|██▏       | 787/3500 [48:20<1:33:15,  2.06s/it] 23%|██▎       | 788/3500 [48:22<1:33:16,  2.06s/it] 23%|██▎       | 789/3500 [48:24<1:33:12,  2.06s/it] 23%|██▎       | 790/3500 [48:26<1:33:08,  2.06s/it] 23%|██▎       | 791/3500 [48:28<1:33:05,  2.06s/it] 23%|██▎       | 792/3500 [48:31<1:33:04,  2.06s/it] 23%|██▎       | 793/3500 [48:33<1:33:01,  2.06s/it] 23%|██▎       | 794/3500 [48:35<1:32:59,  2.06s/it] 23%|██▎       | 795/3500 [48:37<1:32:56,  2.06s/it] 23%|██▎       | 796/3500 [48:39<1:32:55,  2.06s/it] 23%|██▎       | 797/3500 [48:41<1:32:52,  2.06s/it] 23%|██▎       | 798/3500 [48:43<1:32:49,  2.06s/it] 23%|██▎       | 799/3500 [48:45<1:32:47,  2.06s/it] 23%|██▎       | 800/3500 [48:47<1:32:45,  2.06s/it] 23%|██▎       | 801/3500 [48:49<1:32:44,  2.06s/it][2022-11-17 21:24:39,683][__main__][INFO] - epoch 2: perplexity: 7.630574544314665 train_loss: 1.2391315698623657 eval_loss: 2.032163143157959
Configuration saved in tuned-model/epoch_2_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_2_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_2_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_2_most_recent/special_tokens_map.json
 23%|██▎       | 802/3500 [50:12<19:48:58, 26.44s/it] 23%|██▎       | 803/3500 [50:14<14:19:45, 19.13s/it] 23%|██▎       | 804/3500 [50:17<10:29:23, 14.01s/it] 23%|██▎       | 805/3500 [50:19<7:48:10, 10.42s/it]  23%|██▎       | 806/3500 [50:21<5:55:21,  7.91s/it] 23%|██▎       | 807/3500 [50:23<4:36:24,  6.16s/it] 23%|██▎       | 808/3500 [50:25<3:41:09,  4.93s/it] 23%|██▎       | 809/3500 [50:27<3:02:30,  4.07s/it] 23%|██▎       | 810/3500 [50:29<2:35:24,  3.47s/it] 23%|██▎       | 811/3500 [50:31<2:16:29,  3.05s/it] 23%|██▎       | 812/3500 [50:33<2:03:13,  2.75s/it] 23%|██▎       | 813/3500 [50:35<1:53:55,  2.54s/it] 23%|██▎       | 814/3500 [50:37<1:47:23,  2.40s/it] 23%|██▎       | 815/3500 [50:39<1:42:49,  2.30s/it] 23%|██▎       | 816/3500 [50:41<1:39:36,  2.23s/it] 23%|██▎       | 817/3500 [50:43<1:37:19,  2.18s/it] 23%|██▎       | 818/3500 [50:45<1:35:44,  2.14s/it] 23%|██▎       | 819/3500 [50:47<1:34:38,  2.12s/it] 23%|██▎       | 820/3500 [50:49<1:33:50,  2.10s/it] 23%|██▎       | 821/3500 [50:52<1:33:16,  2.09s/it] 23%|██▎       | 822/3500 [50:54<1:32:51,  2.08s/it] 24%|██▎       | 823/3500 [50:56<1:32:32,  2.07s/it] 24%|██▎       | 824/3500 [50:58<1:32:22,  2.07s/it] 24%|██▎       | 825/3500 [51:00<1:32:12,  2.07s/it] 24%|██▎       | 826/3500 [51:02<1:32:03,  2.07s/it] 24%|██▎       | 827/3500 [51:04<1:31:58,  2.06s/it] 24%|██▎       | 828/3500 [51:06<1:31:53,  2.06s/it] 24%|██▎       | 829/3500 [51:08<1:31:48,  2.06s/it] 24%|██▎       | 830/3500 [51:10<1:31:45,  2.06s/it] 24%|██▎       | 831/3500 [51:12<1:31:42,  2.06s/it] 24%|██▍       | 832/3500 [51:14<1:31:41,  2.06s/it] 24%|██▍       | 833/3500 [51:16<1:31:38,  2.06s/it] 24%|██▍       | 834/3500 [51:18<1:31:35,  2.06s/it] 24%|██▍       | 835/3500 [51:20<1:31:32,  2.06s/it] 24%|██▍       | 836/3500 [51:22<1:31:30,  2.06s/it] 24%|██▍       | 837/3500 [51:25<1:31:28,  2.06s/it] 24%|██▍       | 838/3500 [51:27<1:31:26,  2.06s/it] 24%|██▍       | 839/3500 [51:29<1:31:23,  2.06s/it] 24%|██▍       | 840/3500 [51:31<1:31:23,  2.06s/it] 24%|██▍       | 841/3500 [51:33<1:31:20,  2.06s/it] 24%|██▍       | 842/3500 [51:35<1:31:18,  2.06s/it] 24%|██▍       | 843/3500 [51:37<1:31:16,  2.06s/it] 24%|██▍       | 844/3500 [51:39<1:31:13,  2.06s/it] 24%|██▍       | 845/3500 [51:41<1:31:11,  2.06s/it] 24%|██▍       | 846/3500 [51:43<1:31:09,  2.06s/it] 24%|██▍       | 847/3500 [51:45<1:31:08,  2.06s/it] 24%|██▍       | 848/3500 [51:47<1:31:09,  2.06s/it] 24%|██▍       | 849/3500 [51:49<1:31:06,  2.06s/it] 24%|██▍       | 850/3500 [51:51<1:31:04,  2.06s/it] 24%|██▍       | 851/3500 [51:53<1:31:00,  2.06s/it][2022-11-17 21:27:44,019][__main__][INFO] - epoch 2: perplexity: 7.6437975322419005 train_loss: 1.2327779531478882 eval_loss: 2.0338945388793945
Configuration saved in tuned-model/epoch_2_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_2_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_2_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_2_most_recent/special_tokens_map.json
 24%|██▍       | 852/3500 [53:17<19:28:31, 26.48s/it] 24%|██▍       | 853/3500 [53:19<14:04:55, 19.15s/it] 24%|██▍       | 854/3500 [53:21<10:18:28, 14.02s/it] 24%|██▍       | 855/3500 [53:23<7:40:00, 10.44s/it]  24%|██▍       | 856/3500 [53:25<5:49:06,  7.92s/it] 24%|██▍       | 857/3500 [53:27<4:31:31,  6.16s/it] 25%|██▍       | 858/3500 [53:29<3:37:11,  4.93s/it] 25%|██▍       | 859/3500 [53:31<2:59:10,  4.07s/it] 25%|██▍       | 860/3500 [53:33<2:32:33,  3.47s/it] 25%|██▍       | 861/3500 [53:35<2:13:57,  3.05s/it] 25%|██▍       | 862/3500 [53:37<2:00:55,  2.75s/it] 25%|██▍       | 863/3500 [53:39<1:51:47,  2.54s/it] 25%|██▍       | 864/3500 [53:42<1:45:22,  2.40s/it] 25%|██▍       | 865/3500 [53:44<1:40:52,  2.30s/it] 25%|██▍       | 866/3500 [53:46<1:37:44,  2.23s/it] 25%|██▍       | 867/3500 [53:48<1:35:32,  2.18s/it] 25%|██▍       | 868/3500 [53:50<1:33:59,  2.14s/it] 25%|██▍       | 869/3500 [53:52<1:32:52,  2.12s/it] 25%|██▍       | 870/3500 [53:54<1:32:05,  2.10s/it] 25%|██▍       | 871/3500 [53:56<1:31:31,  2.09s/it] 25%|██▍       | 872/3500 [53:58<1:31:08,  2.08s/it] 25%|██▍       | 873/3500 [54:00<1:30:50,  2.07s/it] 25%|██▍       | 874/3500 [54:02<1:30:37,  2.07s/it] 25%|██▌       | 875/3500 [54:04<1:30:27,  2.07s/it] 25%|██▌       | 876/3500 [54:06<1:30:20,  2.07s/it] 25%|██▌       | 877/3500 [54:08<1:30:14,  2.06s/it] 25%|██▌       | 878/3500 [54:10<1:30:09,  2.06s/it] 25%|██▌       | 879/3500 [54:12<1:30:05,  2.06s/it] 25%|██▌       | 880/3500 [54:15<1:30:03,  2.06s/it] 25%|██▌       | 881/3500 [54:17<1:30:00,  2.06s/it] 25%|██▌       | 882/3500 [54:19<1:29:58,  2.06s/it] 25%|██▌       | 883/3500 [54:21<1:29:55,  2.06s/it] 25%|██▌       | 884/3500 [54:23<1:29:53,  2.06s/it] 25%|██▌       | 885/3500 [54:25<1:29:52,  2.06s/it] 25%|██▌       | 886/3500 [54:27<1:29:51,  2.06s/it] 25%|██▌       | 887/3500 [54:29<1:29:48,  2.06s/it] 25%|██▌       | 888/3500 [54:31<1:29:47,  2.06s/it] 25%|██▌       | 889/3500 [54:33<1:29:45,  2.06s/it] 25%|██▌       | 890/3500 [54:35<1:29:42,  2.06s/it] 25%|██▌       | 891/3500 [54:37<1:29:39,  2.06s/it] 25%|██▌       | 892/3500 [54:39<1:29:36,  2.06s/it] 26%|██▌       | 893/3500 [54:41<1:29:34,  2.06s/it] 26%|██▌       | 894/3500 [54:43<1:29:31,  2.06s/it] 26%|██▌       | 895/3500 [54:45<1:29:30,  2.06s/it] 26%|██▌       | 896/3500 [54:48<1:29:27,  2.06s/it] 26%|██▌       | 897/3500 [54:50<1:29:24,  2.06s/it] 26%|██▌       | 898/3500 [54:52<1:29:22,  2.06s/it] 26%|██▌       | 899/3500 [54:54<1:29:21,  2.06s/it] 26%|██▌       | 900/3500 [54:56<1:29:19,  2.06s/it] 26%|██▌       | 901/3500 [54:58<1:29:18,  2.06s/it][2022-11-17 21:30:48,447][__main__][INFO] - epoch 2: perplexity: 7.671740672764553 train_loss: 1.2293665409088135 eval_loss: 2.037543535232544
Configuration saved in tuned-model/epoch_2_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_2_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_2_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_2_most_recent/special_tokens_map.json
 26%|██▌       | 902/3500 [56:21<19:06:12, 26.47s/it] 26%|██▌       | 903/3500 [56:23<13:48:47, 19.15s/it] 26%|██▌       | 904/3500 [56:25<10:06:41, 14.02s/it] 26%|██▌       | 905/3500 [56:27<7:31:14, 10.43s/it]  26%|██▌       | 906/3500 [56:29<5:42:28,  7.92s/it] 26%|██▌       | 907/3500 [56:32<4:26:22,  6.16s/it] 26%|██▌       | 908/3500 [56:34<3:33:05,  4.93s/it] 26%|██▌       | 909/3500 [56:36<2:55:47,  4.07s/it] 26%|██▌       | 910/3500 [56:38<2:29:41,  3.47s/it] 26%|██▌       | 911/3500 [56:40<2:11:25,  3.05s/it] 26%|██▌       | 912/3500 [56:42<1:58:36,  2.75s/it] 26%|██▌       | 913/3500 [56:44<1:49:39,  2.54s/it] 26%|██▌       | 914/3500 [56:46<1:43:22,  2.40s/it] 26%|██▌       | 915/3500 [56:48<1:38:58,  2.30s/it] 26%|██▌       | 916/3500 [56:50<1:35:52,  2.23s/it] 26%|██▌       | 917/3500 [56:52<1:33:42,  2.18s/it] 26%|██▌       | 918/3500 [56:54<1:32:10,  2.14s/it] 26%|██▋       | 919/3500 [56:56<1:31:05,  2.12s/it] 26%|██▋       | 920/3500 [56:58<1:30:19,  2.10s/it] 26%|██▋       | 921/3500 [57:00<1:29:47,  2.09s/it] 26%|██▋       | 922/3500 [57:02<1:29:23,  2.08s/it] 26%|██▋       | 923/3500 [57:05<1:29:06,  2.07s/it] 26%|██▋       | 924/3500 [57:07<1:28:53,  2.07s/it] 26%|██▋       | 925/3500 [57:09<1:28:44,  2.07s/it] 26%|██▋       | 926/3500 [57:11<1:28:37,  2.07s/it] 26%|██▋       | 927/3500 [57:13<1:28:31,  2.06s/it] 27%|██▋       | 928/3500 [57:15<1:28:27,  2.06s/it] 27%|██▋       | 929/3500 [57:17<1:28:23,  2.06s/it] 27%|██▋       | 930/3500 [57:19<1:28:20,  2.06s/it] 27%|██▋       | 931/3500 [57:21<1:28:17,  2.06s/it] 27%|██▋       | 932/3500 [57:23<1:28:14,  2.06s/it] 27%|██▋       | 933/3500 [57:25<1:28:12,  2.06s/it] 27%|██▋       | 934/3500 [57:27<1:28:08,  2.06s/it] 27%|██▋       | 935/3500 [57:29<1:28:06,  2.06s/it] 27%|██▋       | 936/3500 [57:31<1:28:03,  2.06s/it] 27%|██▋       | 937/3500 [57:33<1:28:02,  2.06s/it] 27%|██▋       | 938/3500 [57:35<1:28:00,  2.06s/it] 27%|██▋       | 939/3500 [57:38<1:27:58,  2.06s/it] 27%|██▋       | 940/3500 [57:40<1:27:55,  2.06s/it] 27%|██▋       | 941/3500 [57:42<1:27:54,  2.06s/it] 27%|██▋       | 942/3500 [57:44<1:27:51,  2.06s/it] 27%|██▋       | 943/3500 [57:46<1:27:49,  2.06s/it] 27%|██▋       | 944/3500 [57:48<1:27:46,  2.06s/it] 27%|██▋       | 945/3500 [57:50<1:27:45,  2.06s/it] 27%|██▋       | 946/3500 [57:52<1:27:43,  2.06s/it] 27%|██▋       | 947/3500 [57:54<1:27:41,  2.06s/it] 27%|██▋       | 948/3500 [57:56<1:27:40,  2.06s/it] 27%|██▋       | 949/3500 [57:58<1:27:40,  2.06s/it] 27%|██▋       | 950/3500 [58:00<1:27:37,  2.06s/it] 27%|██▋       | 951/3500 [58:02<1:27:36,  2.06s/it][2022-11-17 21:33:52,863][__main__][INFO] - epoch 2: perplexity: 7.612906261626483 train_loss: 1.2225091457366943 eval_loss: 2.0298449993133545
Configuration saved in tuned-model/epoch_2_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_2_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_2_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_2_most_recent/special_tokens_map.json
 27%|██▋       | 952/3500 [59:25<18:37:29, 26.31s/it] 27%|██▋       | 953/3500 [59:27<13:28:10, 19.04s/it] 27%|██▋       | 954/3500 [59:29<9:51:43, 13.94s/it]  27%|██▋       | 955/3500 [59:31<7:20:17, 10.38s/it] 27%|██▋       | 956/3500 [59:33<5:34:17,  7.88s/it] 27%|██▋       | 957/3500 [59:35<4:20:06,  6.14s/it] 27%|██▋       | 958/3500 [59:38<3:28:12,  4.91s/it] 27%|██▋       | 959/3500 [59:40<2:51:51,  4.06s/it] 27%|██▋       | 960/3500 [59:42<2:26:25,  3.46s/it] 27%|██▋       | 961/3500 [59:44<2:08:37,  3.04s/it] 27%|██▋       | 962/3500 [59:46<1:56:10,  2.75s/it] 28%|██▊       | 963/3500 [59:48<1:47:26,  2.54s/it] 28%|██▊       | 964/3500 [59:50<1:41:18,  2.40s/it] 28%|██▊       | 965/3500 [59:52<1:37:00,  2.30s/it] 28%|██▊       | 966/3500 [59:54<1:33:58,  2.23s/it] 28%|██▊       | 967/3500 [59:56<1:31:51,  2.18s/it] 28%|██▊       | 968/3500 [59:58<1:30:22,  2.14s/it] 28%|██▊       | 969/3500 [1:00:00<1:29:18,  2.12s/it] 28%|██▊       | 970/3500 [1:00:02<1:28:34,  2.10s/it] 28%|██▊       | 971/3500 [1:00:04<1:28:02,  2.09s/it] 28%|██▊       | 972/3500 [1:00:06<1:27:38,  2.08s/it] 28%|██▊       | 973/3500 [1:00:08<1:27:22,  2.07s/it] 28%|██▊       | 974/3500 [1:00:10<1:27:09,  2.07s/it] 28%|██▊       | 975/3500 [1:00:13<1:27:00,  2.07s/it] 28%|██▊       | 976/3500 [1:00:15<1:26:53,  2.07s/it] 28%|██▊       | 977/3500 [1:00:17<1:26:48,  2.06s/it] 28%|██▊       | 978/3500 [1:00:19<1:26:43,  2.06s/it] 28%|██▊       | 979/3500 [1:00:21<1:26:39,  2.06s/it] 28%|██▊       | 980/3500 [1:00:23<1:26:36,  2.06s/it] 28%|██▊       | 981/3500 [1:00:25<1:26:34,  2.06s/it] 28%|██▊       | 982/3500 [1:00:27<1:26:31,  2.06s/it] 28%|██▊       | 983/3500 [1:00:29<1:26:28,  2.06s/it] 28%|██▊       | 984/3500 [1:00:31<1:26:26,  2.06s/it] 28%|██▊       | 985/3500 [1:00:33<1:26:24,  2.06s/it] 28%|██▊       | 986/3500 [1:00:35<1:26:22,  2.06s/it] 28%|██▊       | 987/3500 [1:00:37<1:26:19,  2.06s/it] 28%|██▊       | 988/3500 [1:00:39<1:26:16,  2.06s/it] 28%|██▊       | 989/3500 [1:00:41<1:26:15,  2.06s/it] 28%|██▊       | 990/3500 [1:00:43<1:26:14,  2.06s/it] 28%|██▊       | 991/3500 [1:00:46<1:26:11,  2.06s/it] 28%|██▊       | 992/3500 [1:00:48<1:26:11,  2.06s/it] 28%|██▊       | 993/3500 [1:00:50<1:26:08,  2.06s/it] 28%|██▊       | 994/3500 [1:00:52<1:26:06,  2.06s/it] 28%|██▊       | 995/3500 [1:00:54<1:26:03,  2.06s/it] 28%|██▊       | 996/3500 [1:00:56<1:26:00,  2.06s/it] 28%|██▊       | 997/3500 [1:00:58<1:25:59,  2.06s/it] 29%|██▊       | 998/3500 [1:01:00<1:25:56,  2.06s/it] 29%|██▊       | 999/3500 [1:01:02<1:25:54,  2.06s/it] 29%|██▊       | 1000/3500 [1:01:04<1:25:52,  2.06s/it] 29%|██▊       | 1001/3500 [1:01:06<1:25:50,  2.06s/it][2022-11-17 21:36:56,746][__main__][INFO] - epoch 2: perplexity: 7.649272229179906 train_loss: 1.2282973527908325 eval_loss: 2.0346105098724365
Configuration saved in tuned-model/epoch_2_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_2_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_2_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_2_most_recent/special_tokens_map.json
 29%|██▊       | 1002/3500 [1:02:29<18:13:46, 26.27s/it] 29%|██▊       | 1003/3500 [1:02:31<13:11:04, 19.01s/it] 29%|██▊       | 1004/3500 [1:02:33<9:39:14, 13.92s/it]  29%|██▊       | 1005/3500 [1:02:35<7:10:59, 10.36s/it] 29%|██▊       | 1006/3500 [1:02:37<5:27:15,  7.87s/it] 29%|██▉       | 1007/3500 [1:02:39<4:14:40,  6.13s/it] 29%|██▉       | 1008/3500 [1:02:41<3:23:54,  4.91s/it] 29%|██▉       | 1009/3500 [1:02:43<2:48:21,  4.06s/it] 29%|██▉       | 1010/3500 [1:02:45<2:23:26,  3.46s/it] 29%|██▉       | 1011/3500 [1:02:47<2:06:00,  3.04s/it] 29%|██▉       | 1012/3500 [1:02:50<1:53:48,  2.74s/it] 29%|██▉       | 1013/3500 [1:02:52<1:45:15,  2.54s/it] 29%|██▉       | 1014/3500 [1:02:54<1:39:16,  2.40s/it] 29%|██▉       | 1015/3500 [1:02:56<1:35:04,  2.30s/it] 29%|██▉       | 1016/3500 [1:02:58<1:32:09,  2.23s/it] 29%|██▉       | 1017/3500 [1:03:00<1:30:04,  2.18s/it] 29%|██▉       | 1018/3500 [1:03:02<1:28:36,  2.14s/it] 29%|██▉       | 1019/3500 [1:03:04<1:27:34,  2.12s/it] 29%|██▉       | 1020/3500 [1:03:06<1:26:49,  2.10s/it] 29%|██▉       | 1021/3500 [1:03:08<1:26:17,  2.09s/it] 29%|██▉       | 1022/3500 [1:03:10<1:25:54,  2.08s/it] 29%|██▉       | 1023/3500 [1:03:12<1:25:37,  2.07s/it] 29%|██▉       | 1024/3500 [1:03:14<1:25:26,  2.07s/it] 29%|██▉       | 1025/3500 [1:03:16<1:25:20,  2.07s/it] 29%|██▉       | 1026/3500 [1:03:18<1:25:12,  2.07s/it] 29%|██▉       | 1027/3500 [1:03:20<1:25:05,  2.06s/it] 29%|██▉       | 1028/3500 [1:03:22<1:25:01,  2.06s/it] 29%|██▉       | 1029/3500 [1:03:25<1:24:58,  2.06s/it] 29%|██▉       | 1030/3500 [1:03:27<1:24:55,  2.06s/it] 29%|██▉       | 1031/3500 [1:03:29<1:24:51,  2.06s/it] 29%|██▉       | 1032/3500 [1:03:31<1:24:49,  2.06s/it] 30%|██▉       | 1033/3500 [1:03:33<1:24:46,  2.06s/it] 30%|██▉       | 1034/3500 [1:03:35<1:24:44,  2.06s/it] 30%|██▉       | 1035/3500 [1:03:37<1:24:43,  2.06s/it] 30%|██▉       | 1036/3500 [1:03:39<1:24:41,  2.06s/it] 30%|██▉       | 1037/3500 [1:03:41<1:24:40,  2.06s/it] 30%|██▉       | 1038/3500 [1:03:43<1:24:37,  2.06s/it] 30%|██▉       | 1039/3500 [1:03:45<1:24:35,  2.06s/it] 30%|██▉       | 1040/3500 [1:03:47<1:24:33,  2.06s/it] 30%|██▉       | 1041/3500 [1:03:49<1:24:31,  2.06s/it] 30%|██▉       | 1042/3500 [1:03:51<1:24:28,  2.06s/it] 30%|██▉       | 1043/3500 [1:03:53<1:24:26,  2.06s/it] 30%|██▉       | 1044/3500 [1:03:55<1:24:22,  2.06s/it] 30%|██▉       | 1045/3500 [1:03:58<1:24:19,  2.06s/it] 30%|██▉       | 1046/3500 [1:04:00<1:24:18,  2.06s/it] 30%|██▉       | 1047/3500 [1:04:02<1:24:18,  2.06s/it] 30%|██▉       | 1048/3500 [1:04:04<1:24:14,  2.06s/it] 30%|██▉       | 1049/3500 [1:04:06<1:24:12,  2.06s/it] 30%|███       | 1050/3500 [1:04:08<1:24:08,  2.06s/it][2022-11-17 21:39:10,180][__main__][INFO] - done epoch 2
 30%|███       | 1051/3500 [1:04:10<1:24:57,  2.08s/it][2022-11-17 21:40:01,012][__main__][INFO] - epoch 3: perplexity: 7.6875291011439915 train_loss: 0.8036314249038696 eval_loss: 2.0395994186401367
Configuration saved in tuned-model/epoch_3_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_3_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_3_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_3_most_recent/special_tokens_map.json
 30%|███       | 1052/3500 [1:05:16<14:22:19, 21.14s/it] 30%|███       | 1053/3500 [1:05:18<10:28:36, 15.41s/it] 30%|███       | 1054/3500 [1:05:20<7:45:04, 11.41s/it]  30%|███       | 1055/3500 [1:05:22<5:50:37,  8.60s/it] 30%|███       | 1056/3500 [1:05:24<4:30:31,  6.64s/it] 30%|███       | 1057/3500 [1:05:26<3:34:28,  5.27s/it] 30%|███       | 1058/3500 [1:05:28<2:55:14,  4.31s/it] 30%|███       | 1059/3500 [1:05:30<2:27:50,  3.63s/it] 30%|███       | 1060/3500 [1:05:32<2:08:36,  3.16s/it] 30%|███       | 1061/3500 [1:05:34<1:55:08,  2.83s/it] 30%|███       | 1062/3500 [1:05:36<1:45:42,  2.60s/it] 30%|███       | 1063/3500 [1:05:38<1:39:06,  2.44s/it] 30%|███       | 1064/3500 [1:05:40<1:34:27,  2.33s/it] 30%|███       | 1065/3500 [1:05:42<1:31:12,  2.25s/it] 30%|███       | 1066/3500 [1:05:44<1:28:55,  2.19s/it] 30%|███       | 1067/3500 [1:05:47<1:27:18,  2.15s/it] 31%|███       | 1068/3500 [1:05:49<1:26:11,  2.13s/it] 31%|███       | 1069/3500 [1:05:51<1:25:24,  2.11s/it] 31%|███       | 1070/3500 [1:05:53<1:24:49,  2.09s/it] 31%|███       | 1071/3500 [1:05:55<1:24:22,  2.08s/it] 31%|███       | 1072/3500 [1:05:57<1:24:03,  2.08s/it] 31%|███       | 1073/3500 [1:05:59<1:23:50,  2.07s/it] 31%|███       | 1074/3500 [1:06:01<1:23:43,  2.07s/it] 31%|███       | 1075/3500 [1:06:03<1:23:35,  2.07s/it] 31%|███       | 1076/3500 [1:06:05<1:23:28,  2.07s/it] 31%|███       | 1077/3500 [1:06:07<1:23:24,  2.07s/it] 31%|███       | 1078/3500 [1:06:09<1:23:19,  2.06s/it] 31%|███       | 1079/3500 [1:06:11<1:23:16,  2.06s/it] 31%|███       | 1080/3500 [1:06:13<1:23:13,  2.06s/it] 31%|███       | 1081/3500 [1:06:15<1:23:11,  2.06s/it] 31%|███       | 1082/3500 [1:06:17<1:23:07,  2.06s/it] 31%|███       | 1083/3500 [1:06:20<1:23:03,  2.06s/it] 31%|███       | 1084/3500 [1:06:22<1:23:01,  2.06s/it] 31%|███       | 1085/3500 [1:06:24<1:22:58,  2.06s/it] 31%|███       | 1086/3500 [1:06:26<1:22:56,  2.06s/it] 31%|███       | 1087/3500 [1:06:28<1:22:55,  2.06s/it] 31%|███       | 1088/3500 [1:06:30<1:22:53,  2.06s/it] 31%|███       | 1089/3500 [1:06:32<1:22:50,  2.06s/it] 31%|███       | 1090/3500 [1:06:34<1:22:48,  2.06s/it] 31%|███       | 1091/3500 [1:06:36<1:22:50,  2.06s/it] 31%|███       | 1092/3500 [1:06:38<1:22:49,  2.06s/it] 31%|███       | 1093/3500 [1:06:40<1:22:49,  2.06s/it] 31%|███▏      | 1094/3500 [1:06:42<1:22:47,  2.06s/it] 31%|███▏      | 1095/3500 [1:06:44<1:22:48,  2.07s/it] 31%|███▏      | 1096/3500 [1:06:46<1:22:45,  2.07s/it] 31%|███▏      | 1097/3500 [1:06:48<1:22:43,  2.07s/it] 31%|███▏      | 1098/3500 [1:06:50<1:22:42,  2.07s/it] 31%|███▏      | 1099/3500 [1:06:53<1:22:40,  2.07s/it] 31%|███▏      | 1100/3500 [1:06:55<1:22:38,  2.07s/it] 31%|███▏      | 1101/3500 [1:06:57<1:22:35,  2.07s/it][2022-11-17 21:42:47,347][__main__][INFO] - epoch 3: perplexity: 9.079959278627427 train_loss: 0.783418595790863 eval_loss: 2.2060697078704834
Configuration saved in tuned-model/epoch_3_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_3_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_3_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_3_most_recent/special_tokens_map.json
 31%|███▏      | 1102/3500 [1:08:20<17:36:47, 26.44s/it] 32%|███▏      | 1103/3500 [1:08:22<12:44:08, 19.13s/it] 32%|███▏      | 1104/3500 [1:08:24<9:19:21, 14.01s/it]  32%|███▏      | 1105/3500 [1:08:26<6:56:02, 10.42s/it] 32%|███▏      | 1106/3500 [1:08:28<5:15:46,  7.91s/it] 32%|███▏      | 1107/3500 [1:08:30<4:05:37,  6.16s/it] 32%|███▏      | 1108/3500 [1:08:32<3:16:30,  4.93s/it] 32%|███▏      | 1109/3500 [1:08:34<2:42:09,  4.07s/it] 32%|███▏      | 1110/3500 [1:08:36<2:18:05,  3.47s/it] 32%|███▏      | 1111/3500 [1:08:39<2:01:16,  3.05s/it] 32%|███▏      | 1112/3500 [1:08:41<1:49:27,  2.75s/it] 32%|███▏      | 1113/3500 [1:08:43<1:41:10,  2.54s/it] 32%|███▏      | 1114/3500 [1:08:45<1:35:23,  2.40s/it] 32%|███▏      | 1115/3500 [1:08:47<1:31:19,  2.30s/it] 32%|███▏      | 1116/3500 [1:08:49<1:28:29,  2.23s/it] 32%|███▏      | 1117/3500 [1:08:51<1:26:30,  2.18s/it] 32%|███▏      | 1118/3500 [1:08:53<1:25:04,  2.14s/it] 32%|███▏      | 1119/3500 [1:08:55<1:24:03,  2.12s/it] 32%|███▏      | 1120/3500 [1:08:57<1:23:21,  2.10s/it] 32%|███▏      | 1121/3500 [1:08:59<1:22:53,  2.09s/it] 32%|███▏      | 1122/3500 [1:09:01<1:22:30,  2.08s/it] 32%|███▏      | 1123/3500 [1:09:03<1:22:14,  2.08s/it] 32%|███▏      | 1124/3500 [1:09:05<1:22:01,  2.07s/it] 32%|███▏      | 1125/3500 [1:09:07<1:21:52,  2.07s/it] 32%|███▏      | 1126/3500 [1:09:09<1:21:45,  2.07s/it] 32%|███▏      | 1127/3500 [1:09:12<1:21:39,  2.06s/it] 32%|███▏      | 1128/3500 [1:09:14<1:21:35,  2.06s/it] 32%|███▏      | 1129/3500 [1:09:16<1:21:31,  2.06s/it] 32%|███▏      | 1130/3500 [1:09:18<1:21:27,  2.06s/it] 32%|███▏      | 1131/3500 [1:09:20<1:21:23,  2.06s/it] 32%|███▏      | 1132/3500 [1:09:22<1:21:21,  2.06s/it] 32%|███▏      | 1133/3500 [1:09:24<1:21:18,  2.06s/it] 32%|███▏      | 1134/3500 [1:09:26<1:21:17,  2.06s/it] 32%|███▏      | 1135/3500 [1:09:28<1:21:14,  2.06s/it] 32%|███▏      | 1136/3500 [1:09:30<1:21:12,  2.06s/it] 32%|███▏      | 1137/3500 [1:09:32<1:21:10,  2.06s/it] 33%|███▎      | 1138/3500 [1:09:34<1:21:07,  2.06s/it] 33%|███▎      | 1139/3500 [1:09:36<1:21:05,  2.06s/it] 33%|███▎      | 1140/3500 [1:09:38<1:21:03,  2.06s/it] 33%|███▎      | 1141/3500 [1:09:40<1:21:01,  2.06s/it] 33%|███▎      | 1142/3500 [1:09:42<1:20:58,  2.06s/it] 33%|███▎      | 1143/3500 [1:09:45<1:20:56,  2.06s/it] 33%|███▎      | 1144/3500 [1:09:47<1:20:54,  2.06s/it] 33%|███▎      | 1145/3500 [1:09:49<1:20:52,  2.06s/it] 33%|███▎      | 1146/3500 [1:09:51<1:20:49,  2.06s/it] 33%|███▎      | 1147/3500 [1:09:53<1:20:47,  2.06s/it] 33%|███▎      | 1148/3500 [1:09:55<1:20:45,  2.06s/it] 33%|███▎      | 1149/3500 [1:09:57<1:20:43,  2.06s/it] 33%|███▎      | 1150/3500 [1:09:59<1:20:41,  2.06s/it] 33%|███▎      | 1151/3500 [1:10:01<1:20:39,  2.06s/it][2022-11-17 21:45:51,578][__main__][INFO] - epoch 3: perplexity: 9.319205331047723 train_loss: 0.8019400238990784 eval_loss: 2.2320773601531982
Configuration saved in tuned-model/epoch_3_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_3_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_3_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_3_most_recent/special_tokens_map.json
 33%|███▎      | 1152/3500 [1:11:26<17:29:04, 26.81s/it] 33%|███▎      | 1153/3500 [1:11:28<12:38:14, 19.38s/it] 33%|███▎      | 1154/3500 [1:11:30<9:14:42, 14.19s/it]  33%|███▎      | 1155/3500 [1:11:32<6:52:16, 10.55s/it] 33%|███▎      | 1156/3500 [1:11:34<5:12:36,  8.00s/it] 33%|███▎      | 1157/3500 [1:11:36<4:02:52,  6.22s/it] 33%|███▎      | 1158/3500 [1:11:38<3:14:04,  4.97s/it] 33%|███▎      | 1159/3500 [1:11:40<2:39:54,  4.10s/it] 33%|███▎      | 1160/3500 [1:11:42<2:15:59,  3.49s/it] 33%|███▎      | 1161/3500 [1:11:44<1:59:15,  3.06s/it] 33%|███▎      | 1162/3500 [1:11:46<1:47:32,  2.76s/it] 33%|███▎      | 1163/3500 [1:11:48<1:39:19,  2.55s/it] 33%|███▎      | 1164/3500 [1:11:50<1:33:33,  2.40s/it] 33%|███▎      | 1165/3500 [1:11:52<1:29:30,  2.30s/it] 33%|███▎      | 1166/3500 [1:11:54<1:26:40,  2.23s/it] 33%|███▎      | 1167/3500 [1:11:56<1:24:40,  2.18s/it] 33%|███▎      | 1168/3500 [1:11:59<1:23:17,  2.14s/it] 33%|███▎      | 1169/3500 [1:12:01<1:22:17,  2.12s/it] 33%|███▎      | 1170/3500 [1:12:03<1:21:35,  2.10s/it] 33%|███▎      | 1171/3500 [1:12:05<1:21:05,  2.09s/it] 33%|███▎      | 1172/3500 [1:12:07<1:20:45,  2.08s/it] 34%|███▎      | 1173/3500 [1:12:09<1:20:30,  2.08s/it] 34%|███▎      | 1174/3500 [1:12:11<1:20:17,  2.07s/it] 34%|███▎      | 1175/3500 [1:12:13<1:20:08,  2.07s/it] 34%|███▎      | 1176/3500 [1:12:15<1:20:01,  2.07s/it] 34%|███▎      | 1177/3500 [1:12:17<1:19:55,  2.06s/it] 34%|███▎      | 1178/3500 [1:12:19<1:19:50,  2.06s/it] 34%|███▎      | 1179/3500 [1:12:21<1:19:47,  2.06s/it] 34%|███▎      | 1180/3500 [1:12:23<1:19:44,  2.06s/it] 34%|███▎      | 1181/3500 [1:12:25<1:19:41,  2.06s/it] 34%|███▍      | 1182/3500 [1:12:27<1:19:39,  2.06s/it] 34%|███▍      | 1183/3500 [1:12:29<1:19:37,  2.06s/it] 34%|███▍      | 1184/3500 [1:12:31<1:19:36,  2.06s/it] 34%|███▍      | 1185/3500 [1:12:34<1:19:33,  2.06s/it] 34%|███▍      | 1186/3500 [1:12:36<1:19:30,  2.06s/it] 34%|███▍      | 1187/3500 [1:12:38<1:19:27,  2.06s/it] 34%|███▍      | 1188/3500 [1:12:40<1:19:25,  2.06s/it] 34%|███▍      | 1189/3500 [1:12:42<1:19:22,  2.06s/it] 34%|███▍      | 1190/3500 [1:12:44<1:19:20,  2.06s/it] 34%|███▍      | 1191/3500 [1:12:46<1:19:19,  2.06s/it] 34%|███▍      | 1192/3500 [1:12:48<1:19:17,  2.06s/it] 34%|███▍      | 1193/3500 [1:12:50<1:19:14,  2.06s/it] 34%|███▍      | 1194/3500 [1:12:52<1:19:12,  2.06s/it] 34%|███▍      | 1195/3500 [1:12:54<1:19:09,  2.06s/it] 34%|███▍      | 1196/3500 [1:12:56<1:19:07,  2.06s/it] 34%|███▍      | 1197/3500 [1:12:58<1:19:06,  2.06s/it] 34%|███▍      | 1198/3500 [1:13:00<1:19:03,  2.06s/it] 34%|███▍      | 1199/3500 [1:13:02<1:19:02,  2.06s/it] 34%|███▍      | 1200/3500 [1:13:04<1:18:59,  2.06s/it] 34%|███▍      | 1201/3500 [1:13:07<1:18:57,  2.06s/it][2022-11-17 21:48:57,117][__main__][INFO] - epoch 3: perplexity: 9.351674427153334 train_loss: 0.8087153434753418 eval_loss: 2.235555410385132
Configuration saved in tuned-model/epoch_3_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_3_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_3_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_3_most_recent/special_tokens_map.json
 34%|███▍      | 1202/3500 [1:14:30<16:50:53, 26.39s/it] 34%|███▍      | 1203/3500 [1:14:32<12:10:58, 19.09s/it] 34%|███▍      | 1204/3500 [1:14:34<8:55:05, 13.98s/it]  34%|███▍      | 1205/3500 [1:14:36<6:38:02, 10.41s/it] 34%|███▍      | 1206/3500 [1:14:38<5:02:08,  7.90s/it] 34%|███▍      | 1207/3500 [1:14:40<3:55:01,  6.15s/it] 35%|███▍      | 1208/3500 [1:14:42<3:08:02,  4.92s/it] 35%|███▍      | 1209/3500 [1:14:44<2:35:10,  4.06s/it] 35%|███▍      | 1210/3500 [1:14:46<2:12:11,  3.46s/it] 35%|███▍      | 1211/3500 [1:14:48<1:56:03,  3.04s/it] 35%|███▍      | 1212/3500 [1:14:50<1:44:46,  2.75s/it] 35%|███▍      | 1213/3500 [1:14:52<1:36:52,  2.54s/it] 35%|███▍      | 1214/3500 [1:14:54<1:31:20,  2.40s/it] 35%|███▍      | 1215/3500 [1:14:56<1:27:27,  2.30s/it] 35%|███▍      | 1216/3500 [1:14:59<1:24:43,  2.23s/it] 35%|███▍      | 1217/3500 [1:15:01<1:22:48,  2.18s/it] 35%|███▍      | 1218/3500 [1:15:03<1:21:27,  2.14s/it] 35%|███▍      | 1219/3500 [1:15:05<1:20:29,  2.12s/it] 35%|███▍      | 1220/3500 [1:15:07<1:19:49,  2.10s/it] 35%|███▍      | 1221/3500 [1:15:09<1:19:20,  2.09s/it] 35%|███▍      | 1222/3500 [1:15:11<1:18:59,  2.08s/it] 35%|███▍      | 1223/3500 [1:15:13<1:18:46,  2.08s/it] 35%|███▍      | 1224/3500 [1:15:15<1:18:34,  2.07s/it] 35%|███▌      | 1225/3500 [1:15:17<1:18:24,  2.07s/it] 35%|███▌      | 1226/3500 [1:15:19<1:18:18,  2.07s/it] 35%|███▌      | 1227/3500 [1:15:21<1:18:12,  2.06s/it] 35%|███▌      | 1228/3500 [1:15:23<1:18:08,  2.06s/it] 35%|███▌      | 1229/3500 [1:15:25<1:18:04,  2.06s/it] 35%|███▌      | 1230/3500 [1:15:27<1:18:02,  2.06s/it] 35%|███▌      | 1231/3500 [1:15:29<1:18:00,  2.06s/it] 35%|███▌      | 1232/3500 [1:15:32<1:17:56,  2.06s/it] 35%|███▌      | 1233/3500 [1:15:34<1:17:53,  2.06s/it] 35%|███▌      | 1234/3500 [1:15:36<1:17:52,  2.06s/it] 35%|███▌      | 1235/3500 [1:15:38<1:17:49,  2.06s/it] 35%|███▌      | 1236/3500 [1:15:40<1:17:47,  2.06s/it] 35%|███▌      | 1237/3500 [1:15:42<1:17:44,  2.06s/it] 35%|███▌      | 1238/3500 [1:15:44<1:17:42,  2.06s/it] 35%|███▌      | 1239/3500 [1:15:46<1:17:40,  2.06s/it] 35%|███▌      | 1240/3500 [1:15:48<1:17:39,  2.06s/it] 35%|███▌      | 1241/3500 [1:15:50<1:17:37,  2.06s/it] 35%|███▌      | 1242/3500 [1:15:52<1:17:35,  2.06s/it] 36%|███▌      | 1243/3500 [1:15:54<1:17:32,  2.06s/it] 36%|███▌      | 1244/3500 [1:15:56<1:17:30,  2.06s/it] 36%|███▌      | 1245/3500 [1:15:58<1:17:28,  2.06s/it] 36%|███▌      | 1246/3500 [1:16:00<1:17:25,  2.06s/it] 36%|███▌      | 1247/3500 [1:16:02<1:17:23,  2.06s/it] 36%|███▌      | 1248/3500 [1:16:04<1:17:20,  2.06s/it] 36%|███▌      | 1249/3500 [1:16:07<1:17:18,  2.06s/it] 36%|███▌      | 1250/3500 [1:16:09<1:17:16,  2.06s/it] 36%|███▌      | 1251/3500 [1:16:11<1:17:13,  2.06s/it][2022-11-17 21:52:01,291][__main__][INFO] - epoch 3: perplexity: 9.351734626896182 train_loss: 0.8128525018692017 eval_loss: 2.2355618476867676
Configuration saved in tuned-model/epoch_3_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_3_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_3_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_3_most_recent/special_tokens_map.json
 36%|███▌      | 1252/3500 [1:17:34<16:25:12, 26.30s/it] 36%|███▌      | 1253/3500 [1:17:36<11:52:28, 19.02s/it] 36%|███▌      | 1254/3500 [1:17:38<8:41:38, 13.94s/it]  36%|███▌      | 1255/3500 [1:17:40<6:28:07, 10.37s/it] 36%|███▌      | 1256/3500 [1:17:42<4:54:40,  7.88s/it] 36%|███▌      | 1257/3500 [1:17:44<3:49:17,  6.13s/it] 36%|███▌      | 1258/3500 [1:17:46<3:03:32,  4.91s/it] 36%|███▌      | 1259/3500 [1:17:48<2:31:30,  4.06s/it] 36%|███▌      | 1260/3500 [1:17:50<2:09:04,  3.46s/it] 36%|███▌      | 1261/3500 [1:17:52<1:53:23,  3.04s/it] 36%|███▌      | 1262/3500 [1:17:54<1:42:24,  2.75s/it] 36%|███▌      | 1263/3500 [1:17:56<1:34:43,  2.54s/it] 36%|███▌      | 1264/3500 [1:17:58<1:29:20,  2.40s/it] 36%|███▌      | 1265/3500 [1:18:00<1:25:32,  2.30s/it] 36%|███▌      | 1266/3500 [1:18:02<1:22:52,  2.23s/it] 36%|███▌      | 1267/3500 [1:18:04<1:21:00,  2.18s/it] 36%|███▌      | 1268/3500 [1:18:07<1:19:42,  2.14s/it] 36%|███▋      | 1269/3500 [1:18:09<1:18:45,  2.12s/it] 36%|███▋      | 1270/3500 [1:18:11<1:18:04,  2.10s/it] 36%|███▋      | 1271/3500 [1:18:13<1:17:35,  2.09s/it] 36%|███▋      | 1272/3500 [1:18:15<1:17:14,  2.08s/it] 36%|███▋      | 1273/3500 [1:18:17<1:17:00,  2.07s/it] 36%|███▋      | 1274/3500 [1:18:19<1:16:49,  2.07s/it] 36%|███▋      | 1275/3500 [1:18:21<1:16:40,  2.07s/it] 36%|███▋      | 1276/3500 [1:18:23<1:16:34,  2.07s/it] 36%|███▋      | 1277/3500 [1:18:25<1:16:28,  2.06s/it] 37%|███▋      | 1278/3500 [1:18:27<1:16:25,  2.06s/it] 37%|███▋      | 1279/3500 [1:18:29<1:16:22,  2.06s/it] 37%|███▋      | 1280/3500 [1:18:31<1:16:19,  2.06s/it] 37%|███▋      | 1281/3500 [1:18:33<1:16:16,  2.06s/it] 37%|███▋      | 1282/3500 [1:18:35<1:16:13,  2.06s/it] 37%|███▋      | 1283/3500 [1:18:37<1:16:11,  2.06s/it] 37%|███▋      | 1284/3500 [1:18:39<1:16:09,  2.06s/it] 37%|███▋      | 1285/3500 [1:18:42<1:16:07,  2.06s/it] 37%|███▋      | 1286/3500 [1:18:44<1:16:04,  2.06s/it] 37%|███▋      | 1287/3500 [1:18:46<1:16:01,  2.06s/it] 37%|███▋      | 1288/3500 [1:18:48<1:15:59,  2.06s/it] 37%|███▋      | 1289/3500 [1:18:50<1:15:56,  2.06s/it] 37%|███▋      | 1290/3500 [1:18:52<1:15:55,  2.06s/it] 37%|███▋      | 1291/3500 [1:18:54<1:15:52,  2.06s/it] 37%|███▋      | 1292/3500 [1:18:56<1:15:50,  2.06s/it] 37%|███▋      | 1293/3500 [1:18:58<1:15:49,  2.06s/it] 37%|███▋      | 1294/3500 [1:19:00<1:15:47,  2.06s/it] 37%|███▋      | 1295/3500 [1:19:02<1:15:45,  2.06s/it] 37%|███▋      | 1296/3500 [1:19:04<1:15:42,  2.06s/it] 37%|███▋      | 1297/3500 [1:19:06<1:15:40,  2.06s/it] 37%|███▋      | 1298/3500 [1:19:08<1:15:37,  2.06s/it] 37%|███▋      | 1299/3500 [1:19:10<1:15:35,  2.06s/it] 37%|███▋      | 1300/3500 [1:19:12<1:15:33,  2.06s/it] 37%|███▋      | 1301/3500 [1:19:15<1:15:31,  2.06s/it][2022-11-17 21:55:05,121][__main__][INFO] - epoch 3: perplexity: 9.502883308472018 train_loss: 0.8130548000335693 eval_loss: 2.2515952587127686
Configuration saved in tuned-model/epoch_3_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_3_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_3_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_3_most_recent/special_tokens_map.json
 37%|███▋      | 1302/3500 [1:20:37<16:00:44, 26.23s/it] 37%|███▋      | 1303/3500 [1:20:39<11:34:51, 18.98s/it] 37%|███▋      | 1304/3500 [1:20:41<8:28:48, 13.90s/it]  37%|███▋      | 1305/3500 [1:20:43<6:18:36, 10.35s/it] 37%|███▋      | 1306/3500 [1:20:45<4:47:30,  7.86s/it] 37%|███▋      | 1307/3500 [1:20:47<3:43:45,  6.12s/it] 37%|███▋      | 1308/3500 [1:20:49<2:59:08,  4.90s/it] 37%|███▋      | 1309/3500 [1:20:52<2:27:55,  4.05s/it] 37%|███▋      | 1310/3500 [1:20:54<2:06:03,  3.45s/it] 37%|███▋      | 1311/3500 [1:20:56<1:50:45,  3.04s/it] 37%|███▋      | 1312/3500 [1:20:58<1:40:04,  2.74s/it] 38%|███▊      | 1313/3500 [1:21:00<1:32:33,  2.54s/it] 38%|███▊      | 1314/3500 [1:21:02<1:27:19,  2.40s/it] 38%|███▊      | 1315/3500 [1:21:04<1:23:37,  2.30s/it] 38%|███▊      | 1316/3500 [1:21:06<1:21:00,  2.23s/it] 38%|███▊      | 1317/3500 [1:21:08<1:19:10,  2.18s/it] 38%|███▊      | 1318/3500 [1:21:10<1:17:52,  2.14s/it] 38%|███▊      | 1319/3500 [1:21:12<1:16:58,  2.12s/it] 38%|███▊      | 1320/3500 [1:21:14<1:16:19,  2.10s/it] 38%|███▊      | 1321/3500 [1:21:16<1:15:51,  2.09s/it] 38%|███▊      | 1322/3500 [1:21:18<1:15:31,  2.08s/it] 38%|███▊      | 1323/3500 [1:21:20<1:15:16,  2.07s/it] 38%|███▊      | 1324/3500 [1:21:22<1:15:05,  2.07s/it] 38%|███▊      | 1325/3500 [1:21:25<1:14:57,  2.07s/it] 38%|███▊      | 1326/3500 [1:21:27<1:14:50,  2.07s/it] 38%|███▊      | 1327/3500 [1:21:29<1:14:46,  2.06s/it] 38%|███▊      | 1328/3500 [1:21:31<1:14:41,  2.06s/it] 38%|███▊      | 1329/3500 [1:21:33<1:14:37,  2.06s/it] 38%|███▊      | 1330/3500 [1:21:35<1:14:34,  2.06s/it] 38%|███▊      | 1331/3500 [1:21:37<1:14:30,  2.06s/it] 38%|███▊      | 1332/3500 [1:21:39<1:14:28,  2.06s/it] 38%|███▊      | 1333/3500 [1:21:41<1:14:26,  2.06s/it] 38%|███▊      | 1334/3500 [1:21:43<1:14:23,  2.06s/it] 38%|███▊      | 1335/3500 [1:21:45<1:14:21,  2.06s/it] 38%|███▊      | 1336/3500 [1:21:47<1:14:19,  2.06s/it] 38%|███▊      | 1337/3500 [1:21:49<1:14:18,  2.06s/it] 38%|███▊      | 1338/3500 [1:21:51<1:14:16,  2.06s/it] 38%|███▊      | 1339/3500 [1:21:53<1:14:13,  2.06s/it] 38%|███▊      | 1340/3500 [1:21:55<1:14:11,  2.06s/it] 38%|███▊      | 1341/3500 [1:21:58<1:14:09,  2.06s/it] 38%|███▊      | 1342/3500 [1:22:00<1:14:08,  2.06s/it] 38%|███▊      | 1343/3500 [1:22:02<1:14:07,  2.06s/it] 38%|███▊      | 1344/3500 [1:22:04<1:14:05,  2.06s/it] 38%|███▊      | 1345/3500 [1:22:06<1:14:02,  2.06s/it] 38%|███▊      | 1346/3500 [1:22:08<1:14:00,  2.06s/it] 38%|███▊      | 1347/3500 [1:22:10<1:13:58,  2.06s/it] 39%|███▊      | 1348/3500 [1:22:12<1:13:56,  2.06s/it] 39%|███▊      | 1349/3500 [1:22:14<1:13:55,  2.06s/it] 39%|███▊      | 1350/3500 [1:22:16<1:13:53,  2.06s/it] 39%|███▊      | 1351/3500 [1:22:18<1:13:50,  2.06s/it][2022-11-17 21:58:08,739][__main__][INFO] - epoch 3: perplexity: 9.356003107364218 train_loss: 0.8138538002967834 eval_loss: 2.236018180847168
Configuration saved in tuned-model/epoch_3_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_3_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_3_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_3_most_recent/special_tokens_map.json
 39%|███▊      | 1352/3500 [1:23:41<15:38:32, 26.22s/it] 39%|███▊      | 1353/3500 [1:23:43<11:18:46, 18.97s/it] 39%|███▊      | 1354/3500 [1:23:45<8:17:01, 13.90s/it]  39%|███▊      | 1355/3500 [1:23:47<6:09:50, 10.35s/it] 39%|███▊      | 1356/3500 [1:23:49<4:40:52,  7.86s/it] 39%|███▉      | 1357/3500 [1:23:51<3:38:34,  6.12s/it] 39%|███▉      | 1358/3500 [1:23:53<2:55:00,  4.90s/it] 39%|███▉      | 1359/3500 [1:23:55<2:24:29,  4.05s/it] 39%|███▉      | 1360/3500 [1:23:57<2:03:09,  3.45s/it] 39%|███▉      | 1361/3500 [1:23:59<1:48:12,  3.04s/it] 39%|███▉      | 1362/3500 [1:24:01<1:37:43,  2.74s/it] 39%|███▉      | 1363/3500 [1:24:03<1:30:23,  2.54s/it] 39%|███▉      | 1364/3500 [1:24:05<1:25:14,  2.39s/it] 39%|███▉      | 1365/3500 [1:24:07<1:21:37,  2.29s/it] 39%|███▉      | 1366/3500 [1:24:10<1:19:05,  2.22s/it] 39%|███▉      | 1367/3500 [1:24:12<1:17:18,  2.17s/it] 39%|███▉      | 1368/3500 [1:24:14<1:16:03,  2.14s/it] 39%|███▉      | 1369/3500 [1:24:16<1:15:12,  2.12s/it] 39%|███▉      | 1370/3500 [1:24:18<1:14:35,  2.10s/it] 39%|███▉      | 1371/3500 [1:24:20<1:14:07,  2.09s/it] 39%|███▉      | 1372/3500 [1:24:22<1:13:46,  2.08s/it] 39%|███▉      | 1373/3500 [1:24:24<1:13:32,  2.07s/it] 39%|███▉      | 1374/3500 [1:24:26<1:13:22,  2.07s/it] 39%|███▉      | 1375/3500 [1:24:28<1:13:13,  2.07s/it] 39%|███▉      | 1376/3500 [1:24:30<1:13:08,  2.07s/it] 39%|███▉      | 1377/3500 [1:24:32<1:13:02,  2.06s/it] 39%|███▉      | 1378/3500 [1:24:34<1:12:58,  2.06s/it] 39%|███▉      | 1379/3500 [1:24:36<1:12:55,  2.06s/it] 39%|███▉      | 1380/3500 [1:24:38<1:12:52,  2.06s/it] 39%|███▉      | 1381/3500 [1:24:40<1:12:49,  2.06s/it] 39%|███▉      | 1382/3500 [1:24:43<1:12:47,  2.06s/it] 40%|███▉      | 1383/3500 [1:24:45<1:12:44,  2.06s/it] 40%|███▉      | 1384/3500 [1:24:47<1:12:41,  2.06s/it] 40%|███▉      | 1385/3500 [1:24:49<1:12:39,  2.06s/it] 40%|███▉      | 1386/3500 [1:24:51<1:12:38,  2.06s/it] 40%|███▉      | 1387/3500 [1:24:53<1:12:36,  2.06s/it] 40%|███▉      | 1388/3500 [1:24:55<1:12:33,  2.06s/it] 40%|███▉      | 1389/3500 [1:24:57<1:12:31,  2.06s/it] 40%|███▉      | 1390/3500 [1:24:59<1:12:29,  2.06s/it] 40%|███▉      | 1391/3500 [1:25:01<1:12:27,  2.06s/it] 40%|███▉      | 1392/3500 [1:25:03<1:12:24,  2.06s/it] 40%|███▉      | 1393/3500 [1:25:05<1:12:22,  2.06s/it] 40%|███▉      | 1394/3500 [1:25:07<1:12:20,  2.06s/it] 40%|███▉      | 1395/3500 [1:25:09<1:12:18,  2.06s/it] 40%|███▉      | 1396/3500 [1:25:11<1:12:16,  2.06s/it] 40%|███▉      | 1397/3500 [1:25:13<1:12:15,  2.06s/it] 40%|███▉      | 1398/3500 [1:25:16<1:12:13,  2.06s/it] 40%|███▉      | 1399/3500 [1:25:18<1:12:11,  2.06s/it] 40%|████      | 1400/3500 [1:25:20<1:12:06,  2.06s/it][2022-11-17 22:00:21,959][__main__][INFO] - done epoch 3
 40%|████      | 1401/3500 [1:25:22<1:12:47,  2.08s/it][2022-11-17 22:01:12,753][__main__][INFO] - epoch 4: perplexity: 9.553223286194727 train_loss: 0.5813866853713989 eval_loss: 2.256878614425659
Configuration saved in tuned-model/epoch_4_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_4_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_4_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_4_most_recent/special_tokens_map.json
 40%|████      | 1402/3500 [1:26:27<12:14:48, 21.01s/it] 40%|████      | 1403/3500 [1:26:29<8:55:44, 15.33s/it]  40%|████      | 1404/3500 [1:26:31<6:36:28, 11.35s/it] 40%|████      | 1405/3500 [1:26:33<4:58:59,  8.56s/it] 40%|████      | 1406/3500 [1:26:35<3:50:47,  6.61s/it] 40%|████      | 1407/3500 [1:26:37<3:03:04,  5.25s/it] 40%|████      | 1408/3500 [1:26:39<2:29:39,  4.29s/it] 40%|████      | 1409/3500 [1:26:41<2:06:17,  3.62s/it] 40%|████      | 1410/3500 [1:26:43<1:49:56,  3.16s/it] 40%|████      | 1411/3500 [1:26:46<1:38:28,  2.83s/it] 40%|████      | 1412/3500 [1:26:48<1:30:24,  2.60s/it] 40%|████      | 1413/3500 [1:26:50<1:24:46,  2.44s/it] 40%|████      | 1414/3500 [1:26:52<1:20:48,  2.32s/it] 40%|████      | 1415/3500 [1:26:54<1:18:01,  2.25s/it] 40%|████      | 1416/3500 [1:26:56<1:16:05,  2.19s/it] 40%|████      | 1417/3500 [1:26:58<1:14:41,  2.15s/it] 41%|████      | 1418/3500 [1:27:00<1:13:43,  2.12s/it] 41%|████      | 1419/3500 [1:27:02<1:13:02,  2.11s/it] 41%|████      | 1420/3500 [1:27:04<1:12:32,  2.09s/it] 41%|████      | 1421/3500 [1:27:06<1:12:13,  2.08s/it] 41%|████      | 1422/3500 [1:27:08<1:11:57,  2.08s/it] 41%|████      | 1423/3500 [1:27:10<1:11:45,  2.07s/it] 41%|████      | 1424/3500 [1:27:12<1:11:35,  2.07s/it] 41%|████      | 1425/3500 [1:27:14<1:11:27,  2.07s/it] 41%|████      | 1426/3500 [1:27:16<1:11:21,  2.06s/it] 41%|████      | 1427/3500 [1:27:19<1:11:16,  2.06s/it] 41%|████      | 1428/3500 [1:27:21<1:11:12,  2.06s/it] 41%|████      | 1429/3500 [1:27:23<1:11:09,  2.06s/it] 41%|████      | 1430/3500 [1:27:25<1:11:07,  2.06s/it] 41%|████      | 1431/3500 [1:27:27<1:11:04,  2.06s/it] 41%|████      | 1432/3500 [1:27:29<1:11:01,  2.06s/it] 41%|████      | 1433/3500 [1:27:31<1:10:59,  2.06s/it] 41%|████      | 1434/3500 [1:27:33<1:10:57,  2.06s/it] 41%|████      | 1435/3500 [1:27:35<1:10:55,  2.06s/it] 41%|████      | 1436/3500 [1:27:37<1:10:52,  2.06s/it] 41%|████      | 1437/3500 [1:27:39<1:10:50,  2.06s/it] 41%|████      | 1438/3500 [1:27:41<1:10:48,  2.06s/it] 41%|████      | 1439/3500 [1:27:43<1:10:47,  2.06s/it] 41%|████      | 1440/3500 [1:27:45<1:10:45,  2.06s/it] 41%|████      | 1441/3500 [1:27:47<1:10:43,  2.06s/it] 41%|████      | 1442/3500 [1:27:49<1:10:42,  2.06s/it] 41%|████      | 1443/3500 [1:27:51<1:10:40,  2.06s/it] 41%|████▏     | 1444/3500 [1:27:54<1:10:38,  2.06s/it] 41%|████▏     | 1445/3500 [1:27:56<1:10:35,  2.06s/it] 41%|████▏     | 1446/3500 [1:27:58<1:10:33,  2.06s/it] 41%|████▏     | 1447/3500 [1:28:00<1:10:31,  2.06s/it] 41%|████▏     | 1448/3500 [1:28:02<1:10:29,  2.06s/it] 41%|████▏     | 1449/3500 [1:28:04<1:10:27,  2.06s/it] 41%|████▏     | 1450/3500 [1:28:06<1:10:26,  2.06s/it] 41%|████▏     | 1451/3500 [1:28:08<1:10:25,  2.06s/it][2022-11-17 22:03:58,579][__main__][INFO] - epoch 4: perplexity: 11.43548869505276 train_loss: 0.4492265582084656 eval_loss: 2.4367215633392334
Configuration saved in tuned-model/epoch_4_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_4_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_4_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_4_most_recent/special_tokens_map.json
 41%|████▏     | 1452/3500 [1:29:32<15:07:32, 26.59s/it] 42%|████▏     | 1453/3500 [1:29:34<10:56:03, 19.23s/it] 42%|████▏     | 1454/3500 [1:29:36<8:00:05, 14.08s/it]  42%|████▏     | 1455/3500 [1:29:38<5:56:57, 10.47s/it] 42%|████▏     | 1456/3500 [1:29:40<4:30:48,  7.95s/it] 42%|████▏     | 1457/3500 [1:29:42<3:30:31,  6.18s/it] 42%|████▏     | 1458/3500 [1:29:44<2:48:20,  4.95s/it] 42%|████▏     | 1459/3500 [1:29:46<2:18:49,  4.08s/it] 42%|████▏     | 1460/3500 [1:29:48<1:58:09,  3.48s/it] 42%|████▏     | 1461/3500 [1:29:50<1:43:41,  3.05s/it] 42%|████▏     | 1462/3500 [1:29:52<1:33:32,  2.75s/it] 42%|████▏     | 1463/3500 [1:29:54<1:26:25,  2.55s/it] 42%|████▏     | 1464/3500 [1:29:57<1:21:26,  2.40s/it] 42%|████▏     | 1465/3500 [1:29:59<1:17:57,  2.30s/it] 42%|████▏     | 1466/3500 [1:30:01<1:15:29,  2.23s/it] 42%|████▏     | 1467/3500 [1:30:03<1:13:45,  2.18s/it] 42%|████▏     | 1468/3500 [1:30:05<1:12:32,  2.14s/it] 42%|████▏     | 1469/3500 [1:30:07<1:11:41,  2.12s/it] 42%|████▏     | 1470/3500 [1:30:09<1:11:04,  2.10s/it] 42%|████▏     | 1471/3500 [1:30:11<1:10:38,  2.09s/it] 42%|████▏     | 1472/3500 [1:30:13<1:10:19,  2.08s/it] 42%|████▏     | 1473/3500 [1:30:15<1:10:05,  2.07s/it] 42%|████▏     | 1474/3500 [1:30:17<1:09:54,  2.07s/it] 42%|████▏     | 1475/3500 [1:30:19<1:09:45,  2.07s/it] 42%|████▏     | 1476/3500 [1:30:21<1:09:39,  2.07s/it] 42%|████▏     | 1477/3500 [1:30:23<1:09:36,  2.06s/it] 42%|████▏     | 1478/3500 [1:30:25<1:09:33,  2.06s/it] 42%|████▏     | 1479/3500 [1:30:27<1:09:30,  2.06s/it] 42%|████▏     | 1480/3500 [1:30:29<1:09:27,  2.06s/it] 42%|████▏     | 1481/3500 [1:30:32<1:09:24,  2.06s/it] 42%|████▏     | 1482/3500 [1:30:34<1:09:22,  2.06s/it] 42%|████▏     | 1483/3500 [1:30:36<1:09:21,  2.06s/it] 42%|████▏     | 1484/3500 [1:30:38<1:09:19,  2.06s/it] 42%|████▏     | 1485/3500 [1:30:40<1:09:16,  2.06s/it] 42%|████▏     | 1486/3500 [1:30:42<1:09:13,  2.06s/it] 42%|████▏     | 1487/3500 [1:30:44<1:09:12,  2.06s/it] 43%|████▎     | 1488/3500 [1:30:46<1:09:09,  2.06s/it] 43%|████▎     | 1489/3500 [1:30:48<1:09:06,  2.06s/it] 43%|████▎     | 1490/3500 [1:30:50<1:09:03,  2.06s/it] 43%|████▎     | 1491/3500 [1:30:52<1:09:01,  2.06s/it] 43%|████▎     | 1492/3500 [1:30:54<1:08:58,  2.06s/it] 43%|████▎     | 1493/3500 [1:30:56<1:08:55,  2.06s/it] 43%|████▎     | 1494/3500 [1:30:58<1:08:54,  2.06s/it] 43%|████▎     | 1495/3500 [1:31:00<1:08:51,  2.06s/it] 43%|████▎     | 1496/3500 [1:31:02<1:08:48,  2.06s/it] 43%|████▎     | 1497/3500 [1:31:05<1:08:46,  2.06s/it] 43%|████▎     | 1498/3500 [1:31:07<1:08:44,  2.06s/it] 43%|████▎     | 1499/3500 [1:31:09<1:08:42,  2.06s/it] 43%|████▎     | 1500/3500 [1:31:11<1:08:40,  2.06s/it] 43%|████▎     | 1501/3500 [1:31:13<1:08:38,  2.06s/it][2022-11-17 22:07:03,381][__main__][INFO] - epoch 4: perplexity: 11.698905682451985 train_loss: 0.4537384510040283 eval_loss: 2.4594953060150146
Configuration saved in tuned-model/epoch_4_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_4_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_4_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_4_most_recent/special_tokens_map.json
 43%|████▎     | 1502/3500 [1:32:32<14:02:13, 25.29s/it] 43%|████▎     | 1503/3500 [1:32:34<10:09:49, 18.32s/it] 43%|████▎     | 1504/3500 [1:32:36<7:27:14, 13.44s/it]  43%|████▎     | 1505/3500 [1:32:38<5:33:28, 10.03s/it] 43%|████▎     | 1506/3500 [1:32:41<4:13:50,  7.64s/it] 43%|████▎     | 1507/3500 [1:32:43<3:18:07,  5.96s/it] 43%|████▎     | 1508/3500 [1:32:45<2:39:08,  4.79s/it] 43%|████▎     | 1509/3500 [1:32:47<2:11:50,  3.97s/it] 43%|████▎     | 1510/3500 [1:32:49<1:52:44,  3.40s/it] 43%|████▎     | 1511/3500 [1:32:51<1:39:22,  3.00s/it] 43%|████▎     | 1512/3500 [1:32:53<1:30:01,  2.72s/it] 43%|████▎     | 1513/3500 [1:32:55<1:23:27,  2.52s/it] 43%|████▎     | 1514/3500 [1:32:57<1:18:51,  2.38s/it] 43%|████▎     | 1515/3500 [1:32:59<1:15:39,  2.29s/it] 43%|████▎     | 1516/3500 [1:33:01<1:13:22,  2.22s/it] 43%|████▎     | 1517/3500 [1:33:03<1:11:46,  2.17s/it] 43%|████▎     | 1518/3500 [1:33:05<1:10:38,  2.14s/it] 43%|████▎     | 1519/3500 [1:33:07<1:09:50,  2.12s/it] 43%|████▎     | 1520/3500 [1:33:09<1:09:16,  2.10s/it] 43%|████▎     | 1521/3500 [1:33:11<1:08:51,  2.09s/it] 43%|████▎     | 1522/3500 [1:33:13<1:08:33,  2.08s/it] 44%|████▎     | 1523/3500 [1:33:16<1:08:20,  2.07s/it] 44%|████▎     | 1524/3500 [1:33:18<1:08:10,  2.07s/it] 44%|████▎     | 1525/3500 [1:33:20<1:08:03,  2.07s/it] 44%|████▎     | 1526/3500 [1:33:22<1:07:57,  2.07s/it] 44%|████▎     | 1527/3500 [1:33:24<1:07:52,  2.06s/it] 44%|████▎     | 1528/3500 [1:33:26<1:07:48,  2.06s/it] 44%|████▎     | 1529/3500 [1:33:28<1:07:44,  2.06s/it] 44%|████▎     | 1530/3500 [1:33:30<1:07:42,  2.06s/it] 44%|████▎     | 1531/3500 [1:33:32<1:07:39,  2.06s/it] 44%|████▍     | 1532/3500 [1:33:34<1:07:36,  2.06s/it] 44%|████▍     | 1533/3500 [1:33:36<1:07:34,  2.06s/it] 44%|████▍     | 1534/3500 [1:33:38<1:07:31,  2.06s/it] 44%|████▍     | 1535/3500 [1:33:40<1:07:29,  2.06s/it] 44%|████▍     | 1536/3500 [1:33:42<1:07:27,  2.06s/it] 44%|████▍     | 1537/3500 [1:33:44<1:07:25,  2.06s/it] 44%|████▍     | 1538/3500 [1:33:46<1:07:23,  2.06s/it] 44%|████▍     | 1539/3500 [1:33:49<1:07:21,  2.06s/it] 44%|████▍     | 1540/3500 [1:33:51<1:07:19,  2.06s/it] 44%|████▍     | 1541/3500 [1:33:53<1:07:18,  2.06s/it] 44%|████▍     | 1542/3500 [1:33:55<1:07:15,  2.06s/it] 44%|████▍     | 1543/3500 [1:33:57<1:07:13,  2.06s/it] 44%|████▍     | 1544/3500 [1:33:59<1:07:11,  2.06s/it] 44%|████▍     | 1545/3500 [1:34:01<1:07:10,  2.06s/it] 44%|████▍     | 1546/3500 [1:34:03<1:07:07,  2.06s/it] 44%|████▍     | 1547/3500 [1:34:05<1:07:06,  2.06s/it] 44%|████▍     | 1548/3500 [1:34:07<1:07:03,  2.06s/it] 44%|████▍     | 1549/3500 [1:34:09<1:07:01,  2.06s/it] 44%|████▍     | 1550/3500 [1:34:11<1:06:59,  2.06s/it] 44%|████▍     | 1551/3500 [1:34:13<1:06:58,  2.06s/it][2022-11-17 22:10:03,867][__main__][INFO] - epoch 4: perplexity: 11.951162137342475 train_loss: 0.45698678493499756 eval_loss: 2.4808285236358643
Configuration saved in tuned-model/epoch_4_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_4_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_4_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_4_most_recent/special_tokens_map.json
 44%|████▍     | 1552/3500 [1:35:37<14:22:46, 26.57s/it] 44%|████▍     | 1553/3500 [1:35:39<10:23:40, 19.22s/it] 44%|████▍     | 1554/3500 [1:35:41<7:36:24, 14.07s/it]  44%|████▍     | 1555/3500 [1:35:43<5:39:20, 10.47s/it] 44%|████▍     | 1556/3500 [1:35:45<4:17:26,  7.95s/it] 44%|████▍     | 1557/3500 [1:35:47<3:20:07,  6.18s/it] 45%|████▍     | 1558/3500 [1:35:49<2:40:01,  4.94s/it] 45%|████▍     | 1559/3500 [1:35:51<2:11:57,  4.08s/it] 45%|████▍     | 1560/3500 [1:35:54<1:52:17,  3.47s/it] 45%|████▍     | 1561/3500 [1:35:56<1:38:31,  3.05s/it] 45%|████▍     | 1562/3500 [1:35:58<1:28:53,  2.75s/it] 45%|████▍     | 1563/3500 [1:36:00<1:22:09,  2.54s/it] 45%|████▍     | 1564/3500 [1:36:02<1:17:24,  2.40s/it] 45%|████▍     | 1565/3500 [1:36:04<1:14:05,  2.30s/it] 45%|████▍     | 1566/3500 [1:36:06<1:11:45,  2.23s/it] 45%|████▍     | 1567/3500 [1:36:08<1:10:06,  2.18s/it] 45%|████▍     | 1568/3500 [1:36:10<1:08:57,  2.14s/it] 45%|████▍     | 1569/3500 [1:36:12<1:08:08,  2.12s/it] 45%|████▍     | 1570/3500 [1:36:14<1:07:33,  2.10s/it] 45%|████▍     | 1571/3500 [1:36:16<1:07:07,  2.09s/it] 45%|████▍     | 1572/3500 [1:36:18<1:06:49,  2.08s/it] 45%|████▍     | 1573/3500 [1:36:20<1:06:35,  2.07s/it] 45%|████▍     | 1574/3500 [1:36:22<1:06:26,  2.07s/it] 45%|████▌     | 1575/3500 [1:36:24<1:06:18,  2.07s/it] 45%|████▌     | 1576/3500 [1:36:26<1:06:13,  2.07s/it] 45%|████▌     | 1577/3500 [1:36:29<1:06:08,  2.06s/it] 45%|████▌     | 1578/3500 [1:36:31<1:06:04,  2.06s/it] 45%|████▌     | 1579/3500 [1:36:33<1:06:00,  2.06s/it] 45%|████▌     | 1580/3500 [1:36:35<1:05:57,  2.06s/it] 45%|████▌     | 1581/3500 [1:36:37<1:05:54,  2.06s/it] 45%|████▌     | 1582/3500 [1:36:39<1:05:52,  2.06s/it] 45%|████▌     | 1583/3500 [1:36:41<1:05:49,  2.06s/it] 45%|████▌     | 1584/3500 [1:36:43<1:05:47,  2.06s/it] 45%|████▌     | 1585/3500 [1:36:45<1:05:45,  2.06s/it] 45%|████▌     | 1586/3500 [1:36:47<1:05:43,  2.06s/it] 45%|████▌     | 1587/3500 [1:36:49<1:05:41,  2.06s/it] 45%|████▌     | 1588/3500 [1:36:51<1:05:39,  2.06s/it] 45%|████▌     | 1589/3500 [1:36:53<1:05:37,  2.06s/it] 45%|████▌     | 1590/3500 [1:36:55<1:05:35,  2.06s/it] 45%|████▌     | 1591/3500 [1:36:57<1:05:33,  2.06s/it] 45%|████▌     | 1592/3500 [1:36:59<1:05:31,  2.06s/it] 46%|████▌     | 1593/3500 [1:37:02<1:05:30,  2.06s/it] 46%|████▌     | 1594/3500 [1:37:04<1:05:28,  2.06s/it] 46%|████▌     | 1595/3500 [1:37:06<1:05:26,  2.06s/it] 46%|████▌     | 1596/3500 [1:37:08<1:05:24,  2.06s/it] 46%|████▌     | 1597/3500 [1:37:10<1:05:22,  2.06s/it] 46%|████▌     | 1598/3500 [1:37:12<1:05:20,  2.06s/it] 46%|████▌     | 1599/3500 [1:37:14<1:05:18,  2.06s/it] 46%|████▌     | 1600/3500 [1:37:16<1:05:16,  2.06s/it] 46%|████▌     | 1601/3500 [1:37:18<1:05:14,  2.06s/it][2022-11-17 22:13:08,600][__main__][INFO] - epoch 4: perplexity: 11.917663433035122 train_loss: 0.4618719518184662 eval_loss: 2.4780216217041016
Configuration saved in tuned-model/epoch_4_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_4_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_4_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_4_most_recent/special_tokens_map.json
 46%|████▌     | 1602/3500 [1:38:39<13:38:03, 25.86s/it] 46%|████▌     | 1603/3500 [1:38:41<9:51:54, 18.72s/it]  46%|████▌     | 1604/3500 [1:38:44<7:13:39, 13.72s/it] 46%|████▌     | 1605/3500 [1:38:46<5:22:56, 10.22s/it] 46%|████▌     | 1606/3500 [1:38:48<4:05:26,  7.78s/it] 46%|████▌     | 1607/3500 [1:38:50<3:11:14,  6.06s/it] 46%|████▌     | 1608/3500 [1:38:52<2:33:17,  4.86s/it] 46%|████▌     | 1609/3500 [1:38:54<2:06:45,  4.02s/it] 46%|████▌     | 1610/3500 [1:38:56<1:48:09,  3.43s/it] 46%|████▌     | 1611/3500 [1:38:58<1:35:08,  3.02s/it] 46%|████▌     | 1612/3500 [1:39:00<1:26:01,  2.73s/it] 46%|████▌     | 1613/3500 [1:39:02<1:19:38,  2.53s/it] 46%|████▌     | 1614/3500 [1:39:04<1:15:09,  2.39s/it] 46%|████▌     | 1615/3500 [1:39:06<1:12:00,  2.29s/it] 46%|████▌     | 1616/3500 [1:39:08<1:09:47,  2.22s/it] 46%|████▌     | 1617/3500 [1:39:10<1:08:13,  2.17s/it] 46%|████▌     | 1618/3500 [1:39:12<1:07:07,  2.14s/it] 46%|████▋     | 1619/3500 [1:39:14<1:06:20,  2.12s/it] 46%|████▋     | 1620/3500 [1:39:16<1:05:47,  2.10s/it] 46%|████▋     | 1621/3500 [1:39:19<1:05:24,  2.09s/it] 46%|████▋     | 1622/3500 [1:39:21<1:05:06,  2.08s/it] 46%|████▋     | 1623/3500 [1:39:23<1:04:53,  2.07s/it] 46%|████▋     | 1624/3500 [1:39:25<1:04:44,  2.07s/it] 46%|████▋     | 1625/3500 [1:39:27<1:04:36,  2.07s/it] 46%|████▋     | 1626/3500 [1:39:29<1:04:30,  2.07s/it] 46%|████▋     | 1627/3500 [1:39:31<1:04:25,  2.06s/it] 47%|████▋     | 1628/3500 [1:39:33<1:04:22,  2.06s/it] 47%|████▋     | 1629/3500 [1:39:35<1:04:18,  2.06s/it] 47%|████▋     | 1630/3500 [1:39:37<1:04:15,  2.06s/it] 47%|████▋     | 1631/3500 [1:39:39<1:04:13,  2.06s/it] 47%|████▋     | 1632/3500 [1:39:41<1:04:10,  2.06s/it] 47%|████▋     | 1633/3500 [1:39:43<1:04:08,  2.06s/it] 47%|████▋     | 1634/3500 [1:39:45<1:04:07,  2.06s/it] 47%|████▋     | 1635/3500 [1:39:47<1:04:05,  2.06s/it] 47%|████▋     | 1636/3500 [1:39:49<1:04:03,  2.06s/it] 47%|████▋     | 1637/3500 [1:39:52<1:04:00,  2.06s/it] 47%|████▋     | 1638/3500 [1:39:54<1:03:57,  2.06s/it] 47%|████▋     | 1639/3500 [1:39:56<1:03:56,  2.06s/it] 47%|████▋     | 1640/3500 [1:39:58<1:03:53,  2.06s/it] 47%|████▋     | 1641/3500 [1:40:00<1:03:51,  2.06s/it] 47%|████▋     | 1642/3500 [1:40:02<1:03:49,  2.06s/it] 47%|████▋     | 1643/3500 [1:40:04<1:03:46,  2.06s/it] 47%|████▋     | 1644/3500 [1:40:06<1:03:44,  2.06s/it] 47%|████▋     | 1645/3500 [1:40:08<1:03:42,  2.06s/it] 47%|████▋     | 1646/3500 [1:40:10<1:03:40,  2.06s/it] 47%|████▋     | 1647/3500 [1:40:12<1:03:39,  2.06s/it] 47%|████▋     | 1648/3500 [1:40:14<1:03:37,  2.06s/it] 47%|████▋     | 1649/3500 [1:40:16<1:03:36,  2.06s/it] 47%|████▋     | 1650/3500 [1:40:18<1:03:35,  2.06s/it] 47%|████▋     | 1651/3500 [1:40:20<1:03:32,  2.06s/it][2022-11-17 22:16:11,013][__main__][INFO] - epoch 4: perplexity: 11.900207349337826 train_loss: 0.46607595682144165 eval_loss: 2.476555824279785
Configuration saved in tuned-model/epoch_4_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_4_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_4_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_4_most_recent/special_tokens_map.json
 47%|████▋     | 1652/3500 [1:41:44<13:34:16, 26.44s/it] 47%|████▋     | 1653/3500 [1:41:46<9:48:42, 19.12s/it]  47%|████▋     | 1654/3500 [1:41:48<7:10:52, 14.00s/it] 47%|████▋     | 1655/3500 [1:41:50<5:20:27, 10.42s/it] 47%|████▋     | 1656/3500 [1:41:52<4:03:12,  7.91s/it] 47%|████▋     | 1657/3500 [1:41:54<3:09:08,  6.16s/it] 47%|████▋     | 1658/3500 [1:41:56<2:31:18,  4.93s/it] 47%|████▋     | 1659/3500 [1:41:58<2:04:49,  4.07s/it] 47%|████▋     | 1660/3500 [1:42:00<1:46:18,  3.47s/it] 47%|████▋     | 1661/3500 [1:42:02<1:33:20,  3.05s/it] 47%|████▋     | 1662/3500 [1:42:04<1:24:15,  2.75s/it] 48%|████▊     | 1663/3500 [1:42:06<1:17:52,  2.54s/it] 48%|████▊     | 1664/3500 [1:42:08<1:13:24,  2.40s/it] 48%|████▊     | 1665/3500 [1:42:10<1:10:16,  2.30s/it] 48%|████▊     | 1666/3500 [1:42:13<1:08:04,  2.23s/it] 48%|████▊     | 1667/3500 [1:42:15<1:06:30,  2.18s/it] 48%|████▊     | 1668/3500 [1:42:17<1:05:25,  2.14s/it] 48%|████▊     | 1669/3500 [1:42:19<1:04:39,  2.12s/it] 48%|████▊     | 1670/3500 [1:42:21<1:04:06,  2.10s/it] 48%|████▊     | 1671/3500 [1:42:23<1:03:41,  2.09s/it] 48%|████▊     | 1672/3500 [1:42:25<1:03:23,  2.08s/it] 48%|████▊     | 1673/3500 [1:42:27<1:03:10,  2.07s/it] 48%|████▊     | 1674/3500 [1:42:29<1:03:01,  2.07s/it] 48%|████▊     | 1675/3500 [1:42:31<1:02:54,  2.07s/it] 48%|████▊     | 1676/3500 [1:42:33<1:02:48,  2.07s/it] 48%|████▊     | 1677/3500 [1:42:35<1:02:43,  2.06s/it] 48%|████▊     | 1678/3500 [1:42:37<1:02:39,  2.06s/it] 48%|████▊     | 1679/3500 [1:42:39<1:02:36,  2.06s/it] 48%|████▊     | 1680/3500 [1:42:41<1:02:34,  2.06s/it] 48%|████▊     | 1681/3500 [1:42:43<1:02:32,  2.06s/it] 48%|████▊     | 1682/3500 [1:42:46<1:02:29,  2.06s/it] 48%|████▊     | 1683/3500 [1:42:48<1:02:27,  2.06s/it] 48%|████▊     | 1684/3500 [1:42:50<1:02:23,  2.06s/it] 48%|████▊     | 1685/3500 [1:42:52<1:02:21,  2.06s/it] 48%|████▊     | 1686/3500 [1:42:54<1:02:19,  2.06s/it] 48%|████▊     | 1687/3500 [1:42:56<1:02:17,  2.06s/it] 48%|████▊     | 1688/3500 [1:42:58<1:02:16,  2.06s/it] 48%|████▊     | 1689/3500 [1:43:00<1:02:14,  2.06s/it] 48%|████▊     | 1690/3500 [1:43:02<1:02:12,  2.06s/it] 48%|████▊     | 1691/3500 [1:43:04<1:02:10,  2.06s/it] 48%|████▊     | 1692/3500 [1:43:06<1:02:08,  2.06s/it] 48%|████▊     | 1693/3500 [1:43:08<1:02:06,  2.06s/it] 48%|████▊     | 1694/3500 [1:43:10<1:02:04,  2.06s/it] 48%|████▊     | 1695/3500 [1:43:12<1:02:02,  2.06s/it] 48%|████▊     | 1696/3500 [1:43:14<1:02:01,  2.06s/it] 48%|████▊     | 1697/3500 [1:43:16<1:01:59,  2.06s/it] 49%|████▊     | 1698/3500 [1:43:19<1:01:58,  2.06s/it] 49%|████▊     | 1699/3500 [1:43:21<1:01:54,  2.06s/it] 49%|████▊     | 1700/3500 [1:43:23<1:01:51,  2.06s/it] 49%|████▊     | 1701/3500 [1:43:25<1:01:49,  2.06s/it][2022-11-17 22:19:15,368][__main__][INFO] - epoch 4: perplexity: 11.985986598931056 train_loss: 0.4666992425918579 eval_loss: 2.4837381839752197
Configuration saved in tuned-model/epoch_4_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_4_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_4_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_4_most_recent/special_tokens_map.json
 49%|████▊     | 1702/3500 [1:44:49<13:16:33, 26.58s/it] 49%|████▊     | 1703/3500 [1:44:51<9:35:47, 19.22s/it]  49%|████▊     | 1704/3500 [1:44:53<7:01:20, 14.08s/it] 49%|████▊     | 1705/3500 [1:44:55<5:13:16, 10.47s/it] 49%|████▊     | 1706/3500 [1:44:57<3:57:40,  7.95s/it] 49%|████▉     | 1707/3500 [1:44:59<3:04:47,  6.18s/it] 49%|████▉     | 1708/3500 [1:45:01<2:27:45,  4.95s/it] 49%|████▉     | 1709/3500 [1:45:03<2:01:50,  4.08s/it] 49%|████▉     | 1710/3500 [1:45:05<1:43:41,  3.48s/it] 49%|████▉     | 1711/3500 [1:45:07<1:30:58,  3.05s/it] 49%|████▉     | 1712/3500 [1:45:09<1:22:05,  2.75s/it] 49%|████▉     | 1713/3500 [1:45:11<1:15:51,  2.55s/it] 49%|████▉     | 1714/3500 [1:45:13<1:11:29,  2.40s/it] 49%|████▉     | 1715/3500 [1:45:15<1:08:24,  2.30s/it] 49%|████▉     | 1716/3500 [1:45:17<1:06:14,  2.23s/it] 49%|████▉     | 1717/3500 [1:45:19<1:04:42,  2.18s/it] 49%|████▉     | 1718/3500 [1:45:22<1:03:38,  2.14s/it] 49%|████▉     | 1719/3500 [1:45:24<1:02:53,  2.12s/it] 49%|████▉     | 1720/3500 [1:45:26<1:02:20,  2.10s/it] 49%|████▉     | 1721/3500 [1:45:28<1:01:57,  2.09s/it] 49%|████▉     | 1722/3500 [1:45:30<1:01:39,  2.08s/it] 49%|████▉     | 1723/3500 [1:45:32<1:01:28,  2.08s/it] 49%|████▉     | 1724/3500 [1:45:34<1:01:19,  2.07s/it] 49%|████▉     | 1725/3500 [1:45:36<1:01:12,  2.07s/it] 49%|████▉     | 1726/3500 [1:45:38<1:01:06,  2.07s/it] 49%|████▉     | 1727/3500 [1:45:40<1:01:01,  2.07s/it] 49%|████▉     | 1728/3500 [1:45:42<1:00:56,  2.06s/it] 49%|████▉     | 1729/3500 [1:45:44<1:00:53,  2.06s/it] 49%|████▉     | 1730/3500 [1:45:46<1:00:51,  2.06s/it] 49%|████▉     | 1731/3500 [1:45:48<1:00:48,  2.06s/it] 49%|████▉     | 1732/3500 [1:45:50<1:00:47,  2.06s/it] 50%|████▉     | 1733/3500 [1:45:52<1:00:44,  2.06s/it] 50%|████▉     | 1734/3500 [1:45:54<1:00:40,  2.06s/it] 50%|████▉     | 1735/3500 [1:45:57<1:00:39,  2.06s/it] 50%|████▉     | 1736/3500 [1:45:59<1:00:36,  2.06s/it] 50%|████▉     | 1737/3500 [1:46:01<1:00:34,  2.06s/it] 50%|████▉     | 1738/3500 [1:46:03<1:00:33,  2.06s/it] 50%|████▉     | 1739/3500 [1:46:05<1:00:31,  2.06s/it] 50%|████▉     | 1740/3500 [1:46:07<1:00:29,  2.06s/it] 50%|████▉     | 1741/3500 [1:46:09<1:00:27,  2.06s/it] 50%|████▉     | 1742/3500 [1:46:11<1:00:24,  2.06s/it] 50%|████▉     | 1743/3500 [1:46:13<1:00:21,  2.06s/it] 50%|████▉     | 1744/3500 [1:46:15<1:00:19,  2.06s/it] 50%|████▉     | 1745/3500 [1:46:17<1:00:17,  2.06s/it] 50%|████▉     | 1746/3500 [1:46:19<1:00:15,  2.06s/it] 50%|████▉     | 1747/3500 [1:46:21<1:00:13,  2.06s/it] 50%|████▉     | 1748/3500 [1:46:23<1:00:12,  2.06s/it] 50%|████▉     | 1749/3500 [1:46:25<1:00:10,  2.06s/it] 50%|█████     | 1750/3500 [1:46:27<1:00:05,  2.06s/it][2022-11-17 22:21:29,814][__main__][INFO] - done epoch 4
 50%|█████     | 1751/3500 [1:46:30<1:00:39,  2.08s/it][2022-11-17 22:22:20,633][__main__][INFO] - epoch 5: perplexity: 12.023211294326062 train_loss: 0.32754653692245483 eval_loss: 2.4868390560150146
Configuration saved in tuned-model/epoch_5_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_5_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_5_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_5_most_recent/special_tokens_map.json
 50%|█████     | 1752/3500 [1:47:35<10:16:33, 21.16s/it] 50%|█████     | 1753/3500 [1:47:37<7:29:20, 15.43s/it]  50%|█████     | 1754/3500 [1:47:39<5:32:21, 11.42s/it] 50%|█████     | 1755/3500 [1:47:41<4:10:30,  8.61s/it] 50%|█████     | 1756/3500 [1:47:44<3:13:14,  6.65s/it] 50%|█████     | 1757/3500 [1:47:46<2:33:09,  5.27s/it] 50%|█████     | 1758/3500 [1:47:48<2:05:06,  4.31s/it] 50%|█████     | 1759/3500 [1:47:50<1:45:28,  3.64s/it] 50%|█████     | 1760/3500 [1:47:52<1:31:43,  3.16s/it] 50%|█████     | 1761/3500 [1:47:54<1:22:05,  2.83s/it] 50%|█████     | 1762/3500 [1:47:56<1:15:20,  2.60s/it] 50%|█████     | 1763/3500 [1:47:58<1:10:42,  2.44s/it] 50%|█████     | 1764/3500 [1:48:00<1:07:22,  2.33s/it] 50%|█████     | 1765/3500 [1:48:02<1:05:02,  2.25s/it] 50%|█████     | 1766/3500 [1:48:04<1:03:24,  2.19s/it] 50%|█████     | 1767/3500 [1:48:06<1:02:15,  2.16s/it] 51%|█████     | 1768/3500 [1:48:08<1:01:26,  2.13s/it] 51%|█████     | 1769/3500 [1:48:10<1:00:49,  2.11s/it] 51%|█████     | 1770/3500 [1:48:12<1:00:22,  2.09s/it] 51%|█████     | 1771/3500 [1:48:14<1:00:04,  2.08s/it] 51%|█████     | 1772/3500 [1:48:17<59:50,  2.08s/it]   51%|█████     | 1773/3500 [1:48:19<59:39,  2.07s/it] 51%|█████     | 1774/3500 [1:48:21<59:30,  2.07s/it] 51%|█████     | 1775/3500 [1:48:23<59:24,  2.07s/it] 51%|█████     | 1776/3500 [1:48:25<59:20,  2.07s/it] 51%|█████     | 1777/3500 [1:48:27<59:16,  2.06s/it] 51%|█████     | 1778/3500 [1:48:29<59:12,  2.06s/it] 51%|█████     | 1779/3500 [1:48:31<59:09,  2.06s/it] 51%|█████     | 1780/3500 [1:48:33<59:07,  2.06s/it] 51%|█████     | 1781/3500 [1:48:35<59:04,  2.06s/it] 51%|█████     | 1782/3500 [1:48:37<59:01,  2.06s/it] 51%|█████     | 1783/3500 [1:48:39<58:59,  2.06s/it] 51%|█████     | 1784/3500 [1:48:41<58:57,  2.06s/it] 51%|█████     | 1785/3500 [1:48:43<58:54,  2.06s/it] 51%|█████     | 1786/3500 [1:48:45<58:52,  2.06s/it] 51%|█████     | 1787/3500 [1:48:47<58:50,  2.06s/it] 51%|█████     | 1788/3500 [1:48:50<58:49,  2.06s/it] 51%|█████     | 1789/3500 [1:48:52<58:46,  2.06s/it] 51%|█████     | 1790/3500 [1:48:54<58:44,  2.06s/it] 51%|█████     | 1791/3500 [1:48:56<58:42,  2.06s/it] 51%|█████     | 1792/3500 [1:48:58<58:40,  2.06s/it] 51%|█████     | 1793/3500 [1:49:00<58:37,  2.06s/it] 51%|█████▏    | 1794/3500 [1:49:02<58:35,  2.06s/it] 51%|█████▏    | 1795/3500 [1:49:04<58:33,  2.06s/it] 51%|█████▏    | 1796/3500 [1:49:06<58:31,  2.06s/it] 51%|█████▏    | 1797/3500 [1:49:08<58:29,  2.06s/it] 51%|█████▏    | 1798/3500 [1:49:10<58:28,  2.06s/it] 51%|█████▏    | 1799/3500 [1:49:12<58:26,  2.06s/it] 51%|█████▏    | 1800/3500 [1:49:14<58:23,  2.06s/it] 51%|█████▏    | 1801/3500 [1:49:16<58:21,  2.06s/it][2022-11-17 22:25:06,971][__main__][INFO] - epoch 5: perplexity: 14.368161385803782 train_loss: 0.23681974411010742 eval_loss: 2.6650147438049316
Configuration saved in tuned-model/epoch_5_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_5_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_5_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_5_most_recent/special_tokens_map.json
 51%|█████▏    | 1802/3500 [1:50:41<12:43:29, 26.98s/it] 52%|█████▏    | 1803/3500 [1:50:44<9:11:36, 19.50s/it]  52%|█████▏    | 1804/3500 [1:50:46<6:43:22, 14.27s/it] 52%|█████▏    | 1805/3500 [1:50:48<4:59:39, 10.61s/it] 52%|█████▏    | 1806/3500 [1:50:50<3:47:05,  8.04s/it] 52%|█████▏    | 1807/3500 [1:50:52<2:56:19,  6.25s/it] 52%|█████▏    | 1808/3500 [1:50:54<2:20:47,  4.99s/it] 52%|█████▏    | 1809/3500 [1:50:56<1:55:55,  4.11s/it] 52%|█████▏    | 1810/3500 [1:50:58<1:38:31,  3.50s/it] 52%|█████▏    | 1811/3500 [1:51:00<1:26:20,  3.07s/it] 52%|█████▏    | 1812/3500 [1:51:02<1:17:48,  2.77s/it] 52%|█████▏    | 1813/3500 [1:51:04<1:11:49,  2.55s/it] 52%|█████▏    | 1814/3500 [1:51:06<1:07:37,  2.41s/it] 52%|█████▏    | 1815/3500 [1:51:08<1:04:40,  2.30s/it] 52%|█████▏    | 1816/3500 [1:51:10<1:02:35,  2.23s/it] 52%|█████▏    | 1817/3500 [1:51:12<1:01:08,  2.18s/it] 52%|█████▏    | 1818/3500 [1:51:14<1:00:06,  2.14s/it] 52%|█████▏    | 1819/3500 [1:51:16<59:22,  2.12s/it]   52%|█████▏    | 1820/3500 [1:51:19<58:50,  2.10s/it] 52%|█████▏    | 1821/3500 [1:51:21<58:28,  2.09s/it] 52%|█████▏    | 1822/3500 [1:51:23<58:13,  2.08s/it] 52%|█████▏    | 1823/3500 [1:51:25<58:03,  2.08s/it] 52%|█████▏    | 1824/3500 [1:51:27<57:55,  2.07s/it] 52%|█████▏    | 1825/3500 [1:51:29<57:48,  2.07s/it] 52%|█████▏    | 1826/3500 [1:51:31<57:41,  2.07s/it] 52%|█████▏    | 1827/3500 [1:51:33<57:36,  2.07s/it] 52%|█████▏    | 1828/3500 [1:51:35<57:31,  2.06s/it] 52%|█████▏    | 1829/3500 [1:51:37<57:28,  2.06s/it] 52%|█████▏    | 1830/3500 [1:51:39<57:24,  2.06s/it] 52%|█████▏    | 1831/3500 [1:51:41<57:22,  2.06s/it] 52%|█████▏    | 1832/3500 [1:51:43<57:19,  2.06s/it] 52%|█████▏    | 1833/3500 [1:51:45<57:18,  2.06s/it] 52%|█████▏    | 1834/3500 [1:51:47<57:15,  2.06s/it] 52%|█████▏    | 1835/3500 [1:51:49<57:13,  2.06s/it] 52%|█████▏    | 1836/3500 [1:51:52<57:12,  2.06s/it] 52%|█████▏    | 1837/3500 [1:51:54<57:10,  2.06s/it] 53%|█████▎    | 1838/3500 [1:51:56<57:08,  2.06s/it] 53%|█████▎    | 1839/3500 [1:51:58<57:06,  2.06s/it] 53%|█████▎    | 1840/3500 [1:52:00<57:03,  2.06s/it] 53%|█████▎    | 1841/3500 [1:52:02<57:00,  2.06s/it] 53%|█████▎    | 1842/3500 [1:52:04<56:58,  2.06s/it] 53%|█████▎    | 1843/3500 [1:52:06<56:57,  2.06s/it] 53%|█████▎    | 1844/3500 [1:52:08<56:54,  2.06s/it] 53%|█████▎    | 1845/3500 [1:52:10<56:52,  2.06s/it] 53%|█████▎    | 1846/3500 [1:52:12<56:50,  2.06s/it] 53%|█████▎    | 1847/3500 [1:52:14<56:48,  2.06s/it] 53%|█████▎    | 1848/3500 [1:52:16<56:46,  2.06s/it] 53%|█████▎    | 1849/3500 [1:52:18<56:44,  2.06s/it] 53%|█████▎    | 1850/3500 [1:52:20<56:41,  2.06s/it] 53%|█████▎    | 1851/3500 [1:52:22<56:39,  2.06s/it][2022-11-17 22:28:13,121][__main__][INFO] - epoch 5: perplexity: 14.417138783823434 train_loss: 0.2367497980594635 eval_loss: 2.6684176921844482
Configuration saved in tuned-model/epoch_5_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_5_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_5_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_5_most_recent/special_tokens_map.json
 53%|█████▎    | 1852/3500 [1:53:47<12:14:26, 26.74s/it] 53%|█████▎    | 1853/3500 [1:53:49<8:50:46, 19.34s/it]  53%|█████▎    | 1854/3500 [1:53:51<6:28:15, 14.15s/it] 53%|█████▎    | 1855/3500 [1:53:53<4:48:34, 10.53s/it] 53%|█████▎    | 1856/3500 [1:53:55<3:38:50,  7.99s/it] 53%|█████▎    | 1857/3500 [1:53:57<2:50:03,  6.21s/it] 53%|█████▎    | 1858/3500 [1:53:59<2:15:55,  4.97s/it] 53%|█████▎    | 1859/3500 [1:54:01<1:52:01,  4.10s/it] 53%|█████▎    | 1860/3500 [1:54:03<1:35:15,  3.49s/it] 53%|█████▎    | 1861/3500 [1:54:05<1:23:32,  3.06s/it] 53%|█████▎    | 1862/3500 [1:54:07<1:15:20,  2.76s/it] 53%|█████▎    | 1863/3500 [1:54:09<1:09:35,  2.55s/it] 53%|█████▎    | 1864/3500 [1:54:12<1:05:32,  2.40s/it] 53%|█████▎    | 1865/3500 [1:54:14<1:02:41,  2.30s/it] 53%|█████▎    | 1866/3500 [1:54:16<1:00:41,  2.23s/it] 53%|█████▎    | 1867/3500 [1:54:18<59:18,  2.18s/it]   53%|█████▎    | 1868/3500 [1:54:20<58:18,  2.14s/it] 53%|█████▎    | 1869/3500 [1:54:22<57:36,  2.12s/it] 53%|█████▎    | 1870/3500 [1:54:24<57:05,  2.10s/it] 53%|█████▎    | 1871/3500 [1:54:26<56:43,  2.09s/it] 53%|█████▎    | 1872/3500 [1:54:28<56:27,  2.08s/it] 54%|█████▎    | 1873/3500 [1:54:30<56:16,  2.08s/it] 54%|█████▎    | 1874/3500 [1:54:32<56:07,  2.07s/it] 54%|█████▎    | 1875/3500 [1:54:34<56:00,  2.07s/it] 54%|█████▎    | 1876/3500 [1:54:36<55:54,  2.07s/it] 54%|█████▎    | 1877/3500 [1:54:38<55:50,  2.06s/it] 54%|█████▎    | 1878/3500 [1:54:40<55:46,  2.06s/it] 54%|█████▎    | 1879/3500 [1:54:42<55:43,  2.06s/it] 54%|█████▎    | 1880/3500 [1:54:45<55:40,  2.06s/it] 54%|█████▎    | 1881/3500 [1:54:47<55:37,  2.06s/it] 54%|█████▍    | 1882/3500 [1:54:49<55:35,  2.06s/it] 54%|█████▍    | 1883/3500 [1:54:51<55:33,  2.06s/it] 54%|█████▍    | 1884/3500 [1:54:53<55:32,  2.06s/it] 54%|█████▍    | 1885/3500 [1:54:55<55:29,  2.06s/it] 54%|█████▍    | 1886/3500 [1:54:57<55:27,  2.06s/it] 54%|█████▍    | 1887/3500 [1:54:59<55:25,  2.06s/it] 54%|█████▍    | 1888/3500 [1:55:01<55:23,  2.06s/it] 54%|█████▍    | 1889/3500 [1:55:03<55:21,  2.06s/it] 54%|█████▍    | 1890/3500 [1:55:05<55:19,  2.06s/it] 54%|█████▍    | 1891/3500 [1:55:07<55:18,  2.06s/it] 54%|█████▍    | 1892/3500 [1:55:09<55:16,  2.06s/it] 54%|█████▍    | 1893/3500 [1:55:11<55:14,  2.06s/it] 54%|█████▍    | 1894/3500 [1:55:13<55:11,  2.06s/it] 54%|█████▍    | 1895/3500 [1:55:15<55:09,  2.06s/it] 54%|█████▍    | 1896/3500 [1:55:18<55:06,  2.06s/it] 54%|█████▍    | 1897/3500 [1:55:20<55:04,  2.06s/it] 54%|█████▍    | 1898/3500 [1:55:22<55:02,  2.06s/it] 54%|█████▍    | 1899/3500 [1:55:24<55:01,  2.06s/it] 54%|█████▍    | 1900/3500 [1:55:26<54:58,  2.06s/it] 54%|█████▍    | 1901/3500 [1:55:28<54:57,  2.06s/it][2022-11-17 22:31:18,543][__main__][INFO] - epoch 5: perplexity: 14.624570397966682 train_loss: 0.23265834152698517 eval_loss: 2.6827030181884766
Configuration saved in tuned-model/epoch_5_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_5_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_5_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_5_most_recent/special_tokens_map.json
 54%|█████▍    | 1902/3500 [1:56:52<11:49:35, 26.64s/it] 54%|█████▍    | 1903/3500 [1:56:54<8:32:51, 19.27s/it]  54%|█████▍    | 1904/3500 [1:56:56<6:15:12, 14.11s/it] 54%|█████▍    | 1905/3500 [1:56:58<4:38:55, 10.49s/it] 54%|█████▍    | 1906/3500 [1:57:00<3:31:33,  7.96s/it] 54%|█████▍    | 1907/3500 [1:57:02<2:44:24,  6.19s/it] 55%|█████▍    | 1908/3500 [1:57:04<2:11:25,  4.95s/it] 55%|█████▍    | 1909/3500 [1:57:06<1:48:19,  4.09s/it] 55%|█████▍    | 1910/3500 [1:57:08<1:32:10,  3.48s/it] 55%|█████▍    | 1911/3500 [1:57:10<1:20:50,  3.05s/it] 55%|█████▍    | 1912/3500 [1:57:12<1:12:54,  2.75s/it] 55%|█████▍    | 1913/3500 [1:57:14<1:07:22,  2.55s/it] 55%|█████▍    | 1914/3500 [1:57:17<1:03:28,  2.40s/it] 55%|█████▍    | 1915/3500 [1:57:19<1:00:44,  2.30s/it] 55%|█████▍    | 1916/3500 [1:57:21<58:48,  2.23s/it]   55%|█████▍    | 1917/3500 [1:57:23<57:26,  2.18s/it] 55%|█████▍    | 1918/3500 [1:57:25<56:30,  2.14s/it] 55%|█████▍    | 1919/3500 [1:57:27<55:49,  2.12s/it] 55%|█████▍    | 1920/3500 [1:57:29<55:20,  2.10s/it] 55%|█████▍    | 1921/3500 [1:57:31<54:59,  2.09s/it] 55%|█████▍    | 1922/3500 [1:57:33<54:44,  2.08s/it] 55%|█████▍    | 1923/3500 [1:57:35<54:34,  2.08s/it] 55%|█████▍    | 1924/3500 [1:57:37<54:25,  2.07s/it] 55%|█████▌    | 1925/3500 [1:57:39<54:18,  2.07s/it] 55%|█████▌    | 1926/3500 [1:57:41<54:12,  2.07s/it] 55%|█████▌    | 1927/3500 [1:57:43<54:08,  2.06s/it] 55%|█████▌    | 1928/3500 [1:57:45<54:03,  2.06s/it] 55%|█████▌    | 1929/3500 [1:57:47<54:00,  2.06s/it] 55%|█████▌    | 1930/3500 [1:57:50<53:57,  2.06s/it] 55%|█████▌    | 1931/3500 [1:57:52<53:54,  2.06s/it] 55%|█████▌    | 1932/3500 [1:57:54<53:52,  2.06s/it] 55%|█████▌    | 1933/3500 [1:57:56<53:49,  2.06s/it] 55%|█████▌    | 1934/3500 [1:57:58<53:48,  2.06s/it] 55%|█████▌    | 1935/3500 [1:58:00<53:46,  2.06s/it] 55%|█████▌    | 1936/3500 [1:58:02<53:44,  2.06s/it] 55%|█████▌    | 1937/3500 [1:58:04<53:42,  2.06s/it] 55%|█████▌    | 1938/3500 [1:58:06<53:41,  2.06s/it] 55%|█████▌    | 1939/3500 [1:58:08<53:39,  2.06s/it] 55%|█████▌    | 1940/3500 [1:58:10<53:36,  2.06s/it] 55%|█████▌    | 1941/3500 [1:58:12<53:35,  2.06s/it] 55%|█████▌    | 1942/3500 [1:58:14<53:34,  2.06s/it] 56%|█████▌    | 1943/3500 [1:58:16<53:31,  2.06s/it] 56%|█████▌    | 1944/3500 [1:58:18<53:28,  2.06s/it] 56%|█████▌    | 1945/3500 [1:58:20<53:26,  2.06s/it] 56%|█████▌    | 1946/3500 [1:58:23<53:24,  2.06s/it] 56%|█████▌    | 1947/3500 [1:58:25<53:21,  2.06s/it] 56%|█████▌    | 1948/3500 [1:58:27<53:19,  2.06s/it] 56%|█████▌    | 1949/3500 [1:58:29<53:17,  2.06s/it] 56%|█████▌    | 1950/3500 [1:58:31<53:14,  2.06s/it] 56%|█████▌    | 1951/3500 [1:58:33<53:13,  2.06s/it][2022-11-17 22:34:23,479][__main__][INFO] - epoch 5: perplexity: 14.54845077973673 train_loss: 0.23580236732959747 eval_loss: 2.6774845123291016
Configuration saved in tuned-model/epoch_5_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_5_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_5_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_5_most_recent/special_tokens_map.json
 56%|█████▌    | 1952/3500 [1:59:57<11:31:09, 26.79s/it] 56%|█████▌    | 1953/3500 [1:59:59<8:19:25, 19.37s/it]  56%|█████▌    | 1954/3500 [2:00:01<6:05:17, 14.18s/it] 56%|█████▌    | 1955/3500 [2:00:04<4:31:27, 10.54s/it] 56%|█████▌    | 1956/3500 [2:00:06<3:25:47,  8.00s/it] 56%|█████▌    | 1957/3500 [2:00:08<2:39:51,  6.22s/it] 56%|█████▌    | 1958/3500 [2:00:10<2:07:43,  4.97s/it] 56%|█████▌    | 1959/3500 [2:00:12<1:45:14,  4.10s/it] 56%|█████▌    | 1960/3500 [2:00:14<1:29:30,  3.49s/it] 56%|█████▌    | 1961/3500 [2:00:16<1:18:30,  3.06s/it] 56%|█████▌    | 1962/3500 [2:00:18<1:10:47,  2.76s/it] 56%|█████▌    | 1963/3500 [2:00:20<1:05:23,  2.55s/it] 56%|█████▌    | 1964/3500 [2:00:22<1:01:35,  2.41s/it] 56%|█████▌    | 1965/3500 [2:00:24<58:54,  2.30s/it]   56%|█████▌    | 1966/3500 [2:00:26<57:01,  2.23s/it] 56%|█████▌    | 1967/3500 [2:00:28<55:42,  2.18s/it] 56%|█████▌    | 1968/3500 [2:00:30<54:46,  2.15s/it] 56%|█████▋    | 1969/3500 [2:00:32<54:06,  2.12s/it] 56%|█████▋    | 1970/3500 [2:00:34<53:37,  2.10s/it] 56%|█████▋    | 1971/3500 [2:00:37<53:17,  2.09s/it] 56%|█████▋    | 1972/3500 [2:00:39<53:01,  2.08s/it] 56%|█████▋    | 1973/3500 [2:00:41<52:49,  2.08s/it] 56%|█████▋    | 1974/3500 [2:00:43<52:41,  2.07s/it] 56%|█████▋    | 1975/3500 [2:00:45<52:34,  2.07s/it] 56%|█████▋    | 1976/3500 [2:00:47<52:28,  2.07s/it] 56%|█████▋    | 1977/3500 [2:00:49<52:24,  2.06s/it] 57%|█████▋    | 1978/3500 [2:00:51<52:20,  2.06s/it] 57%|█████▋    | 1979/3500 [2:00:53<52:18,  2.06s/it] 57%|█████▋    | 1980/3500 [2:00:55<52:14,  2.06s/it] 57%|█████▋    | 1981/3500 [2:00:57<52:12,  2.06s/it] 57%|█████▋    | 1982/3500 [2:00:59<52:10,  2.06s/it] 57%|█████▋    | 1983/3500 [2:01:01<52:08,  2.06s/it] 57%|█████▋    | 1984/3500 [2:01:03<52:05,  2.06s/it] 57%|█████▋    | 1985/3500 [2:01:05<52:03,  2.06s/it] 57%|█████▋    | 1986/3500 [2:01:07<52:01,  2.06s/it] 57%|█████▋    | 1987/3500 [2:01:09<51:58,  2.06s/it] 57%|█████▋    | 1988/3500 [2:01:12<51:56,  2.06s/it] 57%|█████▋    | 1989/3500 [2:01:14<51:55,  2.06s/it] 57%|█████▋    | 1990/3500 [2:01:16<51:52,  2.06s/it] 57%|█████▋    | 1991/3500 [2:01:18<51:50,  2.06s/it] 57%|█████▋    | 1992/3500 [2:01:20<51:49,  2.06s/it] 57%|█████▋    | 1993/3500 [2:01:22<51:47,  2.06s/it] 57%|█████▋    | 1994/3500 [2:01:24<51:44,  2.06s/it] 57%|█████▋    | 1995/3500 [2:01:26<51:42,  2.06s/it] 57%|█████▋    | 1996/3500 [2:01:28<51:40,  2.06s/it] 57%|█████▋    | 1997/3500 [2:01:30<51:38,  2.06s/it] 57%|█████▋    | 1998/3500 [2:01:32<51:36,  2.06s/it] 57%|█████▋    | 1999/3500 [2:01:34<51:33,  2.06s/it] 57%|█████▋    | 2000/3500 [2:01:36<51:32,  2.06s/it] 57%|█████▋    | 2001/3500 [2:01:38<51:29,  2.06s/it][2022-11-17 22:37:28,972][__main__][INFO] - epoch 5: perplexity: 14.927043846833481 train_loss: 0.23585568368434906 eval_loss: 2.703174591064453
Configuration saved in tuned-model/epoch_5_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_5_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_5_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_5_most_recent/special_tokens_map.json
 57%|█████▋    | 2002/3500 [2:03:02<11:04:52, 26.63s/it] 57%|█████▋    | 2003/3500 [2:03:04<8:00:32, 19.26s/it]  57%|█████▋    | 2004/3500 [2:03:06<5:51:33, 14.10s/it] 57%|█████▋    | 2005/3500 [2:03:08<4:21:20, 10.49s/it] 57%|█████▋    | 2006/3500 [2:03:11<3:18:13,  7.96s/it] 57%|█████▋    | 2007/3500 [2:03:13<2:34:02,  6.19s/it] 57%|█████▋    | 2008/3500 [2:03:15<2:03:08,  4.95s/it] 57%|█████▋    | 2009/3500 [2:03:17<1:41:32,  4.09s/it] 57%|█████▋    | 2010/3500 [2:03:19<1:26:24,  3.48s/it] 57%|█████▋    | 2011/3500 [2:03:21<1:15:47,  3.05s/it] 57%|█████▋    | 2012/3500 [2:03:23<1:08:21,  2.76s/it] 58%|█████▊    | 2013/3500 [2:03:25<1:03:09,  2.55s/it] 58%|█████▊    | 2014/3500 [2:03:27<59:29,  2.40s/it]   58%|█████▊    | 2015/3500 [2:03:29<56:56,  2.30s/it] 58%|█████▊    | 2016/3500 [2:03:31<55:07,  2.23s/it] 58%|█████▊    | 2017/3500 [2:03:33<53:51,  2.18s/it] 58%|█████▊    | 2018/3500 [2:03:35<52:56,  2.14s/it] 58%|█████▊    | 2019/3500 [2:03:37<52:17,  2.12s/it] 58%|█████▊    | 2020/3500 [2:03:39<51:50,  2.10s/it] 58%|█████▊    | 2021/3500 [2:03:41<51:30,  2.09s/it] 58%|█████▊    | 2022/3500 [2:03:44<51:15,  2.08s/it] 58%|█████▊    | 2023/3500 [2:03:46<51:05,  2.08s/it] 58%|█████▊    | 2024/3500 [2:03:48<50:57,  2.07s/it] 58%|█████▊    | 2025/3500 [2:03:50<50:50,  2.07s/it] 58%|█████▊    | 2026/3500 [2:03:52<50:44,  2.07s/it] 58%|█████▊    | 2027/3500 [2:03:54<50:40,  2.06s/it] 58%|█████▊    | 2028/3500 [2:03:56<50:37,  2.06s/it] 58%|█████▊    | 2029/3500 [2:03:58<50:35,  2.06s/it] 58%|█████▊    | 2030/3500 [2:04:00<50:32,  2.06s/it] 58%|█████▊    | 2031/3500 [2:04:02<50:29,  2.06s/it] 58%|█████▊    | 2032/3500 [2:04:04<50:26,  2.06s/it] 58%|█████▊    | 2033/3500 [2:04:06<50:24,  2.06s/it] 58%|█████▊    | 2034/3500 [2:04:08<50:22,  2.06s/it] 58%|█████▊    | 2035/3500 [2:04:10<50:20,  2.06s/it] 58%|█████▊    | 2036/3500 [2:04:12<50:17,  2.06s/it] 58%|█████▊    | 2037/3500 [2:04:14<50:15,  2.06s/it] 58%|█████▊    | 2038/3500 [2:04:17<50:14,  2.06s/it] 58%|█████▊    | 2039/3500 [2:04:19<50:12,  2.06s/it] 58%|█████▊    | 2040/3500 [2:04:21<50:10,  2.06s/it] 58%|█████▊    | 2041/3500 [2:04:23<50:09,  2.06s/it] 58%|█████▊    | 2042/3500 [2:04:25<50:07,  2.06s/it] 58%|█████▊    | 2043/3500 [2:04:27<50:05,  2.06s/it] 58%|█████▊    | 2044/3500 [2:04:29<50:03,  2.06s/it] 58%|█████▊    | 2045/3500 [2:04:31<50:01,  2.06s/it] 58%|█████▊    | 2046/3500 [2:04:33<49:59,  2.06s/it] 58%|█████▊    | 2047/3500 [2:04:35<49:57,  2.06s/it] 59%|█████▊    | 2048/3500 [2:04:37<49:54,  2.06s/it] 59%|█████▊    | 2049/3500 [2:04:39<49:51,  2.06s/it] 59%|█████▊    | 2050/3500 [2:04:41<49:49,  2.06s/it] 59%|█████▊    | 2051/3500 [2:04:43<49:47,  2.06s/it][2022-11-17 22:40:34,198][__main__][INFO] - epoch 5: perplexity: 14.65255405298561 train_loss: 0.23582296073436737 eval_loss: 2.684614658355713
Configuration saved in tuned-model/epoch_5_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_5_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_5_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_5_most_recent/special_tokens_map.json
 59%|█████▊    | 2052/3500 [2:06:18<12:01:53, 29.91s/it] 59%|█████▊    | 2053/3500 [2:06:20<8:39:52, 21.56s/it]  59%|█████▊    | 2054/3500 [2:06:22<6:18:34, 15.71s/it] 59%|█████▊    | 2055/3500 [2:06:24<4:39:43, 11.62s/it] 59%|█████▊    | 2056/3500 [2:06:26<3:30:33,  8.75s/it] 59%|█████▉    | 2057/3500 [2:06:29<2:42:08,  6.74s/it] 59%|█████▉    | 2058/3500 [2:06:31<2:08:18,  5.34s/it] 59%|█████▉    | 2059/3500 [2:06:33<1:44:36,  4.36s/it] 59%|█████▉    | 2060/3500 [2:06:35<1:28:03,  3.67s/it] 59%|█████▉    | 2061/3500 [2:06:37<1:16:26,  3.19s/it] 59%|█████▉    | 2062/3500 [2:06:39<1:08:17,  2.85s/it] 59%|█████▉    | 2063/3500 [2:06:41<1:02:34,  2.61s/it] 59%|█████▉    | 2064/3500 [2:06:43<58:34,  2.45s/it]   59%|█████▉    | 2065/3500 [2:06:45<55:45,  2.33s/it] 59%|█████▉    | 2066/3500 [2:06:47<53:46,  2.25s/it] 59%|█████▉    | 2067/3500 [2:06:49<52:23,  2.19s/it] 59%|█████▉    | 2068/3500 [2:06:51<51:24,  2.15s/it] 59%|█████▉    | 2069/3500 [2:06:53<50:42,  2.13s/it] 59%|█████▉    | 2070/3500 [2:06:55<50:12,  2.11s/it] 59%|█████▉    | 2071/3500 [2:06:57<49:50,  2.09s/it] 59%|█████▉    | 2072/3500 [2:06:59<49:35,  2.08s/it] 59%|█████▉    | 2073/3500 [2:07:02<49:23,  2.08s/it] 59%|█████▉    | 2074/3500 [2:07:04<49:14,  2.07s/it] 59%|█████▉    | 2075/3500 [2:07:06<49:07,  2.07s/it] 59%|█████▉    | 2076/3500 [2:07:08<49:02,  2.07s/it] 59%|█████▉    | 2077/3500 [2:07:10<48:58,  2.07s/it] 59%|█████▉    | 2078/3500 [2:07:12<48:54,  2.06s/it] 59%|█████▉    | 2079/3500 [2:07:14<48:51,  2.06s/it] 59%|█████▉    | 2080/3500 [2:07:16<48:48,  2.06s/it] 59%|█████▉    | 2081/3500 [2:07:18<48:46,  2.06s/it] 59%|█████▉    | 2082/3500 [2:07:20<48:44,  2.06s/it] 60%|█████▉    | 2083/3500 [2:07:22<48:42,  2.06s/it] 60%|█████▉    | 2084/3500 [2:07:24<48:40,  2.06s/it] 60%|█████▉    | 2085/3500 [2:07:26<48:38,  2.06s/it] 60%|█████▉    | 2086/3500 [2:07:28<48:36,  2.06s/it] 60%|█████▉    | 2087/3500 [2:07:30<48:34,  2.06s/it] 60%|█████▉    | 2088/3500 [2:07:32<48:32,  2.06s/it] 60%|█████▉    | 2089/3500 [2:07:35<48:30,  2.06s/it] 60%|█████▉    | 2090/3500 [2:07:37<48:28,  2.06s/it] 60%|█████▉    | 2091/3500 [2:07:39<48:25,  2.06s/it] 60%|█████▉    | 2092/3500 [2:07:41<48:23,  2.06s/it] 60%|█████▉    | 2093/3500 [2:07:43<48:21,  2.06s/it] 60%|█████▉    | 2094/3500 [2:07:45<48:19,  2.06s/it] 60%|█████▉    | 2095/3500 [2:07:47<48:16,  2.06s/it] 60%|█████▉    | 2096/3500 [2:07:49<48:14,  2.06s/it] 60%|█████▉    | 2097/3500 [2:07:51<48:12,  2.06s/it] 60%|█████▉    | 2098/3500 [2:07:53<48:11,  2.06s/it] 60%|█████▉    | 2099/3500 [2:07:55<48:08,  2.06s/it] 60%|██████    | 2100/3500 [2:07:57<48:04,  2.06s/it][2022-11-17 22:42:59,532][__main__][INFO] - done epoch 5
 60%|██████    | 2101/3500 [2:07:59<48:32,  2.08s/it][2022-11-17 22:43:50,406][__main__][INFO] - epoch 6: perplexity: 14.68082926432483 train_loss: 0.1349523812532425 eval_loss: 2.686542510986328
Configuration saved in tuned-model/epoch_6_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_6_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_6_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_6_most_recent/special_tokens_map.json
 60%|██████    | 2102/3500 [2:09:06<8:19:39, 21.44s/it] 60%|██████    | 2103/3500 [2:09:08<6:03:54, 15.63s/it] 60%|██████    | 2104/3500 [2:09:10<4:28:56, 11.56s/it] 60%|██████    | 2105/3500 [2:09:12<3:22:31,  8.71s/it] 60%|██████    | 2106/3500 [2:09:14<2:36:05,  6.72s/it] 60%|██████    | 2107/3500 [2:09:16<2:03:33,  5.32s/it] 60%|██████    | 2108/3500 [2:09:18<1:40:48,  4.34s/it] 60%|██████    | 2109/3500 [2:09:20<1:24:52,  3.66s/it] 60%|██████    | 2110/3500 [2:09:22<1:13:45,  3.18s/it] 60%|██████    | 2111/3500 [2:09:25<1:05:58,  2.85s/it] 60%|██████    | 2112/3500 [2:09:27<1:00:26,  2.61s/it] 60%|██████    | 2113/3500 [2:09:29<56:36,  2.45s/it]   60%|██████    | 2114/3500 [2:09:31<53:54,  2.33s/it] 60%|██████    | 2115/3500 [2:09:33<52:03,  2.26s/it] 60%|██████    | 2116/3500 [2:09:35<50:46,  2.20s/it] 60%|██████    | 2117/3500 [2:09:37<49:46,  2.16s/it] 61%|██████    | 2118/3500 [2:09:39<49:05,  2.13s/it] 61%|██████    | 2119/3500 [2:09:41<48:33,  2.11s/it] 61%|██████    | 2120/3500 [2:09:43<48:12,  2.10s/it] 61%|██████    | 2121/3500 [2:09:45<47:59,  2.09s/it] 61%|██████    | 2122/3500 [2:09:47<47:47,  2.08s/it] 61%|██████    | 2123/3500 [2:09:49<47:37,  2.08s/it] 61%|██████    | 2124/3500 [2:09:51<47:29,  2.07s/it] 61%|██████    | 2125/3500 [2:09:53<47:24,  2.07s/it] 61%|██████    | 2126/3500 [2:09:56<47:22,  2.07s/it] 61%|██████    | 2127/3500 [2:09:58<47:19,  2.07s/it] 61%|██████    | 2128/3500 [2:10:00<47:15,  2.07s/it] 61%|██████    | 2129/3500 [2:10:02<47:11,  2.06s/it] 61%|██████    | 2130/3500 [2:10:04<47:08,  2.06s/it] 61%|██████    | 2131/3500 [2:10:06<47:05,  2.06s/it] 61%|██████    | 2132/3500 [2:10:08<47:02,  2.06s/it] 61%|██████    | 2133/3500 [2:10:10<46:59,  2.06s/it] 61%|██████    | 2134/3500 [2:10:12<46:56,  2.06s/it] 61%|██████    | 2135/3500 [2:10:14<46:54,  2.06s/it] 61%|██████    | 2136/3500 [2:10:16<46:52,  2.06s/it] 61%|██████    | 2137/3500 [2:10:18<46:50,  2.06s/it] 61%|██████    | 2138/3500 [2:10:20<46:47,  2.06s/it] 61%|██████    | 2139/3500 [2:10:22<46:46,  2.06s/it] 61%|██████    | 2140/3500 [2:10:24<46:44,  2.06s/it] 61%|██████    | 2141/3500 [2:10:26<46:48,  2.07s/it] 61%|██████    | 2142/3500 [2:10:29<46:45,  2.07s/it] 61%|██████    | 2143/3500 [2:10:31<46:41,  2.06s/it] 61%|██████▏   | 2144/3500 [2:10:33<46:38,  2.06s/it] 61%|██████▏   | 2145/3500 [2:10:35<46:36,  2.06s/it] 61%|██████▏   | 2146/3500 [2:10:37<46:33,  2.06s/it] 61%|██████▏   | 2147/3500 [2:10:39<46:31,  2.06s/it] 61%|██████▏   | 2148/3500 [2:10:41<46:29,  2.06s/it] 61%|██████▏   | 2149/3500 [2:10:43<46:26,  2.06s/it] 61%|██████▏   | 2150/3500 [2:10:45<46:23,  2.06s/it] 61%|██████▏   | 2151/3500 [2:10:47<46:21,  2.06s/it][2022-11-17 22:46:38,195][__main__][INFO] - epoch 6: perplexity: 17.171888198909308 train_loss: 0.11046166718006134 eval_loss: 2.843273639678955
Configuration saved in tuned-model/epoch_6_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_6_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_6_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_6_most_recent/special_tokens_map.json
 61%|██████▏   | 2152/3500 [2:12:17<10:36:05, 28.31s/it] 62%|██████▏   | 2153/3500 [2:12:19<7:38:49, 20.44s/it]  62%|██████▏   | 2154/3500 [2:12:21<5:34:48, 14.92s/it] 62%|██████▏   | 2155/3500 [2:12:23<4:08:04, 11.07s/it] 62%|██████▏   | 2156/3500 [2:12:25<3:07:22,  8.36s/it] 62%|██████▏   | 2157/3500 [2:12:27<2:24:55,  6.47s/it] 62%|██████▏   | 2158/3500 [2:12:29<1:55:12,  5.15s/it] 62%|██████▏   | 2159/3500 [2:12:31<1:34:25,  4.22s/it] 62%|██████▏   | 2160/3500 [2:12:33<1:19:51,  3.58s/it] 62%|██████▏   | 2161/3500 [2:12:35<1:09:40,  3.12s/it] 62%|██████▏   | 2162/3500 [2:12:37<1:02:32,  2.80s/it] 62%|██████▏   | 2163/3500 [2:12:39<57:31,  2.58s/it]   62%|██████▏   | 2164/3500 [2:12:41<54:00,  2.43s/it] 62%|██████▏   | 2165/3500 [2:12:43<51:32,  2.32s/it] 62%|██████▏   | 2166/3500 [2:12:46<49:48,  2.24s/it] 62%|██████▏   | 2167/3500 [2:12:48<48:35,  2.19s/it] 62%|██████▏   | 2168/3500 [2:12:50<47:43,  2.15s/it] 62%|██████▏   | 2169/3500 [2:12:52<47:06,  2.12s/it] 62%|██████▏   | 2170/3500 [2:12:54<46:40,  2.11s/it] 62%|██████▏   | 2171/3500 [2:12:56<46:20,  2.09s/it] 62%|██████▏   | 2172/3500 [2:12:58<46:07,  2.08s/it] 62%|██████▏   | 2173/3500 [2:13:00<45:56,  2.08s/it] 62%|██████▏   | 2174/3500 [2:13:02<45:49,  2.07s/it] 62%|██████▏   | 2175/3500 [2:13:04<45:42,  2.07s/it] 62%|██████▏   | 2176/3500 [2:13:06<45:37,  2.07s/it] 62%|██████▏   | 2177/3500 [2:13:08<45:33,  2.07s/it] 62%|██████▏   | 2178/3500 [2:13:10<45:29,  2.06s/it] 62%|██████▏   | 2179/3500 [2:13:12<45:26,  2.06s/it] 62%|██████▏   | 2180/3500 [2:13:14<45:23,  2.06s/it] 62%|██████▏   | 2181/3500 [2:13:16<45:21,  2.06s/it] 62%|██████▏   | 2182/3500 [2:13:19<45:18,  2.06s/it] 62%|██████▏   | 2183/3500 [2:13:21<45:16,  2.06s/it] 62%|██████▏   | 2184/3500 [2:13:23<45:13,  2.06s/it] 62%|██████▏   | 2185/3500 [2:13:25<45:11,  2.06s/it] 62%|██████▏   | 2186/3500 [2:13:27<45:09,  2.06s/it] 62%|██████▏   | 2187/3500 [2:13:29<45:07,  2.06s/it] 63%|██████▎   | 2188/3500 [2:13:31<45:06,  2.06s/it] 63%|██████▎   | 2189/3500 [2:13:33<45:03,  2.06s/it] 63%|██████▎   | 2190/3500 [2:13:35<45:00,  2.06s/it] 63%|██████▎   | 2191/3500 [2:13:37<44:58,  2.06s/it] 63%|██████▎   | 2192/3500 [2:13:39<44:56,  2.06s/it] 63%|██████▎   | 2193/3500 [2:13:41<44:54,  2.06s/it] 63%|██████▎   | 2194/3500 [2:13:43<44:52,  2.06s/it] 63%|██████▎   | 2195/3500 [2:13:45<44:50,  2.06s/it] 63%|██████▎   | 2196/3500 [2:13:47<44:48,  2.06s/it] 63%|██████▎   | 2197/3500 [2:13:49<44:45,  2.06s/it] 63%|██████▎   | 2198/3500 [2:13:52<44:43,  2.06s/it] 63%|██████▎   | 2199/3500 [2:13:54<44:42,  2.06s/it] 63%|██████▎   | 2200/3500 [2:13:56<44:40,  2.06s/it] 63%|██████▎   | 2201/3500 [2:13:58<44:38,  2.06s/it][2022-11-17 22:49:48,381][__main__][INFO] - epoch 6: perplexity: 17.289898889880345 train_loss: 0.1110093891620636 eval_loss: 2.8501224517822266
Configuration saved in tuned-model/epoch_6_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_6_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_6_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_6_most_recent/special_tokens_map.json
 63%|██████▎   | 2202/3500 [2:15:24<9:49:11, 27.24s/it] 63%|██████▎   | 2203/3500 [2:15:26<7:05:28, 19.68s/it] 63%|██████▎   | 2204/3500 [2:15:28<5:10:58, 14.40s/it] 63%|██████▎   | 2205/3500 [2:15:30<3:50:52, 10.70s/it] 63%|██████▎   | 2206/3500 [2:15:32<2:54:49,  8.11s/it] 63%|██████▎   | 2207/3500 [2:15:34<2:15:36,  6.29s/it] 63%|██████▎   | 2208/3500 [2:15:36<1:48:10,  5.02s/it] 63%|██████▎   | 2209/3500 [2:15:38<1:28:59,  4.14s/it] 63%|██████▎   | 2210/3500 [2:15:40<1:15:32,  3.51s/it] 63%|██████▎   | 2211/3500 [2:15:42<1:06:07,  3.08s/it] 63%|██████▎   | 2212/3500 [2:15:44<59:31,  2.77s/it]   63%|██████▎   | 2213/3500 [2:15:46<54:54,  2.56s/it] 63%|██████▎   | 2214/3500 [2:15:48<51:39,  2.41s/it] 63%|██████▎   | 2215/3500 [2:15:50<49:22,  2.31s/it] 63%|██████▎   | 2216/3500 [2:15:53<47:46,  2.23s/it] 63%|██████▎   | 2217/3500 [2:15:55<46:38,  2.18s/it] 63%|██████▎   | 2218/3500 [2:15:57<45:50,  2.15s/it] 63%|██████▎   | 2219/3500 [2:15:59<45:16,  2.12s/it] 63%|██████▎   | 2220/3500 [2:16:01<44:52,  2.10s/it] 63%|██████▎   | 2221/3500 [2:16:03<44:35,  2.09s/it] 63%|██████▎   | 2222/3500 [2:16:05<44:22,  2.08s/it] 64%|██████▎   | 2223/3500 [2:16:07<44:12,  2.08s/it] 64%|██████▎   | 2224/3500 [2:16:09<44:04,  2.07s/it] 64%|██████▎   | 2225/3500 [2:16:11<43:58,  2.07s/it] 64%|██████▎   | 2226/3500 [2:16:13<43:53,  2.07s/it] 64%|██████▎   | 2227/3500 [2:16:15<43:49,  2.07s/it] 64%|██████▎   | 2228/3500 [2:16:17<43:46,  2.06s/it] 64%|██████▎   | 2229/3500 [2:16:19<43:43,  2.06s/it] 64%|██████▎   | 2230/3500 [2:16:21<43:40,  2.06s/it] 64%|██████▎   | 2231/3500 [2:16:23<43:37,  2.06s/it] 64%|██████▍   | 2232/3500 [2:16:26<43:35,  2.06s/it] 64%|██████▍   | 2233/3500 [2:16:28<43:34,  2.06s/it] 64%|██████▍   | 2234/3500 [2:16:30<43:32,  2.06s/it] 64%|██████▍   | 2235/3500 [2:16:32<43:29,  2.06s/it] 64%|██████▍   | 2236/3500 [2:16:34<43:27,  2.06s/it] 64%|██████▍   | 2237/3500 [2:16:36<43:24,  2.06s/it] 64%|██████▍   | 2238/3500 [2:16:38<43:22,  2.06s/it] 64%|██████▍   | 2239/3500 [2:16:40<43:19,  2.06s/it] 64%|██████▍   | 2240/3500 [2:16:42<43:18,  2.06s/it] 64%|██████▍   | 2241/3500 [2:16:44<43:15,  2.06s/it] 64%|██████▍   | 2242/3500 [2:16:46<43:13,  2.06s/it] 64%|██████▍   | 2243/3500 [2:16:48<43:11,  2.06s/it] 64%|██████▍   | 2244/3500 [2:16:50<43:09,  2.06s/it] 64%|██████▍   | 2245/3500 [2:16:52<43:07,  2.06s/it] 64%|██████▍   | 2246/3500 [2:16:54<43:05,  2.06s/it] 64%|██████▍   | 2247/3500 [2:16:56<43:03,  2.06s/it] 64%|██████▍   | 2248/3500 [2:16:59<43:01,  2.06s/it] 64%|██████▍   | 2249/3500 [2:17:01<42:59,  2.06s/it] 64%|██████▍   | 2250/3500 [2:17:03<42:57,  2.06s/it] 64%|██████▍   | 2251/3500 [2:17:05<42:55,  2.06s/it][2022-11-17 22:52:55,394][__main__][INFO] - epoch 6: perplexity: 17.46979091731038 train_loss: 0.1086849719285965 eval_loss: 2.860473155975342
Configuration saved in tuned-model/epoch_6_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_6_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_6_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_6_most_recent/special_tokens_map.json
 64%|██████▍   | 2252/3500 [2:18:29<9:16:01, 26.73s/it] 64%|██████▍   | 2253/3500 [2:18:31<6:41:46, 19.33s/it] 64%|██████▍   | 2254/3500 [2:18:33<4:53:52, 14.15s/it] 64%|██████▍   | 2255/3500 [2:18:35<3:38:24, 10.53s/it] 64%|██████▍   | 2256/3500 [2:18:37<2:45:37,  7.99s/it] 64%|██████▍   | 2257/3500 [2:18:39<2:08:39,  6.21s/it] 65%|██████▍   | 2258/3500 [2:18:41<1:42:47,  4.97s/it] 65%|██████▍   | 2259/3500 [2:18:43<1:24:41,  4.09s/it] 65%|██████▍   | 2260/3500 [2:18:46<1:12:00,  3.48s/it] 65%|██████▍   | 2261/3500 [2:18:48<1:03:08,  3.06s/it] 65%|██████▍   | 2262/3500 [2:18:50<56:55,  2.76s/it]   65%|██████▍   | 2263/3500 [2:18:52<52:35,  2.55s/it] 65%|██████▍   | 2264/3500 [2:18:54<49:31,  2.40s/it] 65%|██████▍   | 2265/3500 [2:18:56<47:22,  2.30s/it] 65%|██████▍   | 2266/3500 [2:18:58<45:52,  2.23s/it] 65%|██████▍   | 2267/3500 [2:19:00<44:49,  2.18s/it] 65%|██████▍   | 2268/3500 [2:19:02<44:02,  2.15s/it] 65%|██████▍   | 2269/3500 [2:19:04<43:30,  2.12s/it] 65%|██████▍   | 2270/3500 [2:19:06<43:06,  2.10s/it] 65%|██████▍   | 2271/3500 [2:19:08<42:49,  2.09s/it] 65%|██████▍   | 2272/3500 [2:19:10<42:36,  2.08s/it] 65%|██████▍   | 2273/3500 [2:19:12<42:26,  2.08s/it] 65%|██████▍   | 2274/3500 [2:19:14<42:19,  2.07s/it] 65%|██████▌   | 2275/3500 [2:19:16<42:13,  2.07s/it] 65%|██████▌   | 2276/3500 [2:19:19<42:09,  2.07s/it] 65%|██████▌   | 2277/3500 [2:19:21<42:06,  2.07s/it] 65%|██████▌   | 2278/3500 [2:19:23<42:03,  2.06s/it] 65%|██████▌   | 2279/3500 [2:19:25<42:01,  2.06s/it] 65%|██████▌   | 2280/3500 [2:19:27<41:58,  2.06s/it] 65%|██████▌   | 2281/3500 [2:19:29<41:55,  2.06s/it] 65%|██████▌   | 2282/3500 [2:19:31<41:53,  2.06s/it] 65%|██████▌   | 2283/3500 [2:19:33<41:50,  2.06s/it] 65%|██████▌   | 2284/3500 [2:19:35<41:48,  2.06s/it] 65%|██████▌   | 2285/3500 [2:19:37<41:46,  2.06s/it] 65%|██████▌   | 2286/3500 [2:19:39<41:45,  2.06s/it] 65%|██████▌   | 2287/3500 [2:19:41<41:43,  2.06s/it] 65%|██████▌   | 2288/3500 [2:19:43<41:40,  2.06s/it] 65%|██████▌   | 2289/3500 [2:19:45<41:38,  2.06s/it] 65%|██████▌   | 2290/3500 [2:19:47<41:35,  2.06s/it] 65%|██████▌   | 2291/3500 [2:19:49<41:33,  2.06s/it] 65%|██████▌   | 2292/3500 [2:19:52<41:31,  2.06s/it] 66%|██████▌   | 2293/3500 [2:19:54<41:29,  2.06s/it] 66%|██████▌   | 2294/3500 [2:19:56<41:26,  2.06s/it] 66%|██████▌   | 2295/3500 [2:19:58<41:25,  2.06s/it] 66%|██████▌   | 2296/3500 [2:20:00<41:23,  2.06s/it] 66%|██████▌   | 2297/3500 [2:20:02<41:20,  2.06s/it] 66%|██████▌   | 2298/3500 [2:20:04<41:18,  2.06s/it] 66%|██████▌   | 2299/3500 [2:20:06<41:15,  2.06s/it] 66%|██████▌   | 2300/3500 [2:20:08<41:14,  2.06s/it] 66%|██████▌   | 2301/3500 [2:20:10<41:11,  2.06s/it][2022-11-17 22:56:00,773][__main__][INFO] - epoch 6: perplexity: 17.621779669375556 train_loss: 0.1087859570980072 eval_loss: 2.869135618209839
Configuration saved in tuned-model/epoch_6_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_6_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_6_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_6_most_recent/special_tokens_map.json
 66%|██████▌   | 2302/3500 [2:21:33<8:47:27, 26.42s/it] 66%|██████▌   | 2303/3500 [2:21:35<6:21:14, 19.11s/it] 66%|██████▌   | 2304/3500 [2:21:37<4:38:58, 14.00s/it] 66%|██████▌   | 2305/3500 [2:21:40<3:27:26, 10.42s/it] 66%|██████▌   | 2306/3500 [2:21:42<2:37:23,  7.91s/it] 66%|██████▌   | 2307/3500 [2:21:44<2:02:23,  6.16s/it] 66%|██████▌   | 2308/3500 [2:21:46<1:37:53,  4.93s/it] 66%|██████▌   | 2309/3500 [2:21:48<1:20:44,  4.07s/it] 66%|██████▌   | 2310/3500 [2:21:50<1:08:44,  3.47s/it] 66%|██████▌   | 2311/3500 [2:21:52<1:00:20,  3.04s/it] 66%|██████▌   | 2312/3500 [2:21:54<54:27,  2.75s/it]   66%|██████▌   | 2313/3500 [2:21:56<50:19,  2.54s/it] 66%|██████▌   | 2314/3500 [2:21:58<47:25,  2.40s/it] 66%|██████▌   | 2315/3500 [2:22:00<45:22,  2.30s/it] 66%|██████▌   | 2316/3500 [2:22:02<43:57,  2.23s/it] 66%|██████▌   | 2317/3500 [2:22:04<42:57,  2.18s/it] 66%|██████▌   | 2318/3500 [2:22:06<42:14,  2.14s/it] 66%|██████▋   | 2319/3500 [2:22:08<41:43,  2.12s/it] 66%|██████▋   | 2320/3500 [2:22:10<41:20,  2.10s/it] 66%|██████▋   | 2321/3500 [2:22:13<41:04,  2.09s/it] 66%|██████▋   | 2322/3500 [2:22:15<40:52,  2.08s/it] 66%|██████▋   | 2323/3500 [2:22:17<40:42,  2.08s/it] 66%|██████▋   | 2324/3500 [2:22:19<40:36,  2.07s/it] 66%|██████▋   | 2325/3500 [2:22:21<40:30,  2.07s/it] 66%|██████▋   | 2326/3500 [2:22:23<40:25,  2.07s/it] 66%|██████▋   | 2327/3500 [2:22:25<40:21,  2.06s/it] 67%|██████▋   | 2328/3500 [2:22:27<40:18,  2.06s/it] 67%|██████▋   | 2329/3500 [2:22:29<40:15,  2.06s/it] 67%|██████▋   | 2330/3500 [2:22:31<40:13,  2.06s/it] 67%|██████▋   | 2331/3500 [2:22:33<40:11,  2.06s/it] 67%|██████▋   | 2332/3500 [2:22:35<40:09,  2.06s/it] 67%|██████▋   | 2333/3500 [2:22:37<40:08,  2.06s/it] 67%|██████▋   | 2334/3500 [2:22:39<40:06,  2.06s/it] 67%|██████▋   | 2335/3500 [2:22:41<40:04,  2.06s/it] 67%|██████▋   | 2336/3500 [2:22:43<40:01,  2.06s/it] 67%|██████▋   | 2337/3500 [2:22:46<39:58,  2.06s/it] 67%|██████▋   | 2338/3500 [2:22:48<39:56,  2.06s/it] 67%|██████▋   | 2339/3500 [2:22:50<39:54,  2.06s/it] 67%|██████▋   | 2340/3500 [2:22:52<39:53,  2.06s/it] 67%|██████▋   | 2341/3500 [2:22:54<39:52,  2.06s/it] 67%|██████▋   | 2342/3500 [2:22:56<39:51,  2.07s/it] 67%|██████▋   | 2343/3500 [2:22:58<39:48,  2.06s/it] 67%|██████▋   | 2344/3500 [2:23:00<39:45,  2.06s/it] 67%|██████▋   | 2345/3500 [2:23:02<39:43,  2.06s/it] 67%|██████▋   | 2346/3500 [2:23:04<39:41,  2.06s/it] 67%|██████▋   | 2347/3500 [2:23:06<39:38,  2.06s/it] 67%|██████▋   | 2348/3500 [2:23:08<39:35,  2.06s/it] 67%|██████▋   | 2349/3500 [2:23:10<39:34,  2.06s/it] 67%|██████▋   | 2350/3500 [2:23:12<39:32,  2.06s/it] 67%|██████▋   | 2351/3500 [2:23:14<39:29,  2.06s/it][2022-11-17 22:59:05,136][__main__][INFO] - epoch 6: perplexity: 17.35914643028271 train_loss: 0.1095963791012764 eval_loss: 2.8541195392608643
Configuration saved in tuned-model/epoch_6_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_6_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_6_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_6_most_recent/special_tokens_map.json
 67%|██████▋   | 2352/3500 [2:24:40<8:38:36, 27.11s/it] 67%|██████▋   | 2353/3500 [2:24:42<6:14:31, 19.59s/it] 67%|██████▋   | 2354/3500 [2:24:44<4:33:45, 14.33s/it] 67%|██████▋   | 2355/3500 [2:24:46<3:23:16, 10.65s/it] 67%|██████▋   | 2356/3500 [2:24:48<2:33:57,  8.07s/it] 67%|██████▋   | 2357/3500 [2:24:50<1:59:28,  6.27s/it] 67%|██████▋   | 2358/3500 [2:24:52<1:35:19,  5.01s/it] 67%|██████▋   | 2359/3500 [2:24:54<1:18:26,  4.12s/it] 67%|██████▋   | 2360/3500 [2:24:56<1:06:36,  3.51s/it] 67%|██████▋   | 2361/3500 [2:24:59<58:20,  3.07s/it]   67%|██████▋   | 2362/3500 [2:25:01<52:33,  2.77s/it] 68%|██████▊   | 2363/3500 [2:25:03<48:29,  2.56s/it] 68%|██████▊   | 2364/3500 [2:25:05<45:37,  2.41s/it] 68%|██████▊   | 2365/3500 [2:25:07<43:36,  2.31s/it] 68%|██████▊   | 2366/3500 [2:25:09<42:11,  2.23s/it] 68%|██████▊   | 2367/3500 [2:25:11<41:11,  2.18s/it] 68%|██████▊   | 2368/3500 [2:25:13<40:28,  2.15s/it] 68%|██████▊   | 2369/3500 [2:25:15<39:58,  2.12s/it] 68%|██████▊   | 2370/3500 [2:25:17<39:36,  2.10s/it] 68%|██████▊   | 2371/3500 [2:25:19<39:21,  2.09s/it] 68%|██████▊   | 2372/3500 [2:25:21<39:10,  2.08s/it] 68%|██████▊   | 2373/3500 [2:25:23<39:01,  2.08s/it] 68%|██████▊   | 2374/3500 [2:25:25<38:54,  2.07s/it] 68%|██████▊   | 2375/3500 [2:25:27<38:48,  2.07s/it] 68%|██████▊   | 2376/3500 [2:25:29<38:43,  2.07s/it] 68%|██████▊   | 2377/3500 [2:25:32<38:39,  2.07s/it] 68%|██████▊   | 2378/3500 [2:25:34<38:36,  2.06s/it] 68%|██████▊   | 2379/3500 [2:25:36<38:33,  2.06s/it] 68%|██████▊   | 2380/3500 [2:25:38<38:30,  2.06s/it] 68%|██████▊   | 2381/3500 [2:25:40<38:27,  2.06s/it] 68%|██████▊   | 2382/3500 [2:25:42<38:25,  2.06s/it] 68%|██████▊   | 2383/3500 [2:25:44<38:24,  2.06s/it] 68%|██████▊   | 2384/3500 [2:25:46<38:21,  2.06s/it] 68%|██████▊   | 2385/3500 [2:25:48<38:19,  2.06s/it] 68%|██████▊   | 2386/3500 [2:25:50<38:17,  2.06s/it] 68%|██████▊   | 2387/3500 [2:25:52<38:15,  2.06s/it] 68%|██████▊   | 2388/3500 [2:25:54<38:12,  2.06s/it] 68%|██████▊   | 2389/3500 [2:25:56<38:11,  2.06s/it] 68%|██████▊   | 2390/3500 [2:25:58<38:09,  2.06s/it] 68%|██████▊   | 2391/3500 [2:26:00<38:06,  2.06s/it] 68%|██████▊   | 2392/3500 [2:26:02<38:04,  2.06s/it] 68%|██████▊   | 2393/3500 [2:26:05<38:02,  2.06s/it] 68%|██████▊   | 2394/3500 [2:26:07<38:00,  2.06s/it] 68%|██████▊   | 2395/3500 [2:26:09<37:58,  2.06s/it] 68%|██████▊   | 2396/3500 [2:26:11<37:56,  2.06s/it] 68%|██████▊   | 2397/3500 [2:26:13<37:54,  2.06s/it] 69%|██████▊   | 2398/3500 [2:26:15<37:52,  2.06s/it] 69%|██████▊   | 2399/3500 [2:26:17<37:50,  2.06s/it] 69%|██████▊   | 2400/3500 [2:26:19<37:49,  2.06s/it] 69%|██████▊   | 2401/3500 [2:26:21<37:47,  2.06s/it][2022-11-17 23:02:11,750][__main__][INFO] - epoch 6: perplexity: 17.64422080443873 train_loss: 0.10971659421920776 eval_loss: 2.870408296585083
Configuration saved in tuned-model/epoch_6_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_6_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_6_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_6_most_recent/special_tokens_map.json
 69%|██████▊   | 2402/3500 [2:27:42<7:52:28, 25.82s/it] 69%|██████▊   | 2403/3500 [2:27:44<5:41:44, 18.69s/it] 69%|██████▊   | 2404/3500 [2:27:46<4:10:17, 13.70s/it] 69%|██████▊   | 2405/3500 [2:27:48<3:06:20, 10.21s/it] 69%|██████▊   | 2406/3500 [2:27:51<2:21:35,  7.77s/it] 69%|██████▉   | 2407/3500 [2:27:53<1:50:17,  6.05s/it] 69%|██████▉   | 2408/3500 [2:27:55<1:28:23,  4.86s/it] 69%|██████▉   | 2409/3500 [2:27:57<1:13:03,  4.02s/it] 69%|██████▉   | 2410/3500 [2:27:59<1:02:20,  3.43s/it] 69%|██████▉   | 2411/3500 [2:28:01<54:49,  3.02s/it]   69%|██████▉   | 2412/3500 [2:28:03<49:34,  2.73s/it] 69%|██████▉   | 2413/3500 [2:28:05<45:52,  2.53s/it] 69%|██████▉   | 2414/3500 [2:28:07<43:17,  2.39s/it] 69%|██████▉   | 2415/3500 [2:28:09<41:27,  2.29s/it] 69%|██████▉   | 2416/3500 [2:28:11<40:10,  2.22s/it] 69%|██████▉   | 2417/3500 [2:28:13<39:15,  2.17s/it] 69%|██████▉   | 2418/3500 [2:28:15<38:36,  2.14s/it] 69%|██████▉   | 2419/3500 [2:28:17<38:08,  2.12s/it] 69%|██████▉   | 2420/3500 [2:28:19<37:48,  2.10s/it] 69%|██████▉   | 2421/3500 [2:28:21<37:34,  2.09s/it] 69%|██████▉   | 2422/3500 [2:28:24<37:23,  2.08s/it] 69%|██████▉   | 2423/3500 [2:28:26<37:15,  2.08s/it] 69%|██████▉   | 2424/3500 [2:28:28<37:08,  2.07s/it] 69%|██████▉   | 2425/3500 [2:28:30<37:03,  2.07s/it] 69%|██████▉   | 2426/3500 [2:28:32<36:59,  2.07s/it] 69%|██████▉   | 2427/3500 [2:28:34<36:55,  2.06s/it] 69%|██████▉   | 2428/3500 [2:28:36<36:51,  2.06s/it] 69%|██████▉   | 2429/3500 [2:28:38<36:49,  2.06s/it] 69%|██████▉   | 2430/3500 [2:28:40<36:47,  2.06s/it] 69%|██████▉   | 2431/3500 [2:28:42<36:45,  2.06s/it] 69%|██████▉   | 2432/3500 [2:28:44<36:42,  2.06s/it] 70%|██████▉   | 2433/3500 [2:28:46<36:39,  2.06s/it] 70%|██████▉   | 2434/3500 [2:28:48<36:37,  2.06s/it] 70%|██████▉   | 2435/3500 [2:28:50<36:35,  2.06s/it] 70%|██████▉   | 2436/3500 [2:28:52<36:33,  2.06s/it] 70%|██████▉   | 2437/3500 [2:28:54<36:31,  2.06s/it] 70%|██████▉   | 2438/3500 [2:28:56<36:29,  2.06s/it] 70%|██████▉   | 2439/3500 [2:28:59<36:28,  2.06s/it] 70%|██████▉   | 2440/3500 [2:29:01<36:26,  2.06s/it] 70%|██████▉   | 2441/3500 [2:29:03<36:24,  2.06s/it] 70%|██████▉   | 2442/3500 [2:29:05<36:21,  2.06s/it] 70%|██████▉   | 2443/3500 [2:29:07<36:19,  2.06s/it] 70%|██████▉   | 2444/3500 [2:29:09<36:17,  2.06s/it] 70%|██████▉   | 2445/3500 [2:29:11<36:15,  2.06s/it] 70%|██████▉   | 2446/3500 [2:29:13<36:13,  2.06s/it] 70%|██████▉   | 2447/3500 [2:29:15<36:11,  2.06s/it] 70%|██████▉   | 2448/3500 [2:29:17<36:09,  2.06s/it] 70%|██████▉   | 2449/3500 [2:29:19<36:07,  2.06s/it] 70%|███████   | 2450/3500 [2:29:21<36:03,  2.06s/it][2022-11-17 23:04:23,568][__main__][INFO] - done epoch 6
 70%|███████   | 2451/3500 [2:29:23<36:24,  2.08s/it][2022-11-17 23:05:14,353][__main__][INFO] - epoch 7: perplexity: 17.61676816004325 train_loss: 0.04587189853191376 eval_loss: 2.8688511848449707
Configuration saved in tuned-model/epoch_7_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_7_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_7_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_7_most_recent/special_tokens_map.json
 70%|███████   | 2452/3500 [2:30:30<6:15:35, 21.50s/it] 70%|███████   | 2453/3500 [2:30:32<4:33:29, 15.67s/it] 70%|███████   | 2454/3500 [2:30:34<3:22:03, 11.59s/it] 70%|███████   | 2455/3500 [2:30:36<2:32:05,  8.73s/it] 70%|███████   | 2456/3500 [2:30:38<1:57:07,  6.73s/it] 70%|███████   | 2457/3500 [2:30:41<1:32:40,  5.33s/it] 70%|███████   | 2458/3500 [2:30:43<1:15:32,  4.35s/it] 70%|███████   | 2459/3500 [2:30:45<1:03:32,  3.66s/it] 70%|███████   | 2460/3500 [2:30:47<55:09,  3.18s/it]   70%|███████   | 2461/3500 [2:30:49<49:16,  2.85s/it] 70%|███████   | 2462/3500 [2:30:51<45:10,  2.61s/it] 70%|███████   | 2463/3500 [2:30:53<42:18,  2.45s/it] 70%|███████   | 2464/3500 [2:30:55<40:17,  2.33s/it] 70%|███████   | 2465/3500 [2:30:57<38:51,  2.25s/it] 70%|███████   | 2466/3500 [2:30:59<37:51,  2.20s/it] 70%|███████   | 2467/3500 [2:31:01<37:09,  2.16s/it] 71%|███████   | 2468/3500 [2:31:03<36:37,  2.13s/it] 71%|███████   | 2469/3500 [2:31:05<36:13,  2.11s/it] 71%|███████   | 2470/3500 [2:31:07<35:57,  2.09s/it] 71%|███████   | 2471/3500 [2:31:09<35:45,  2.08s/it] 71%|███████   | 2472/3500 [2:31:11<35:36,  2.08s/it] 71%|███████   | 2473/3500 [2:31:14<35:29,  2.07s/it] 71%|███████   | 2474/3500 [2:31:16<35:23,  2.07s/it] 71%|███████   | 2475/3500 [2:31:18<35:19,  2.07s/it] 71%|███████   | 2476/3500 [2:31:20<35:15,  2.07s/it] 71%|███████   | 2477/3500 [2:31:22<35:12,  2.06s/it] 71%|███████   | 2478/3500 [2:31:24<35:09,  2.06s/it] 71%|███████   | 2479/3500 [2:31:26<35:07,  2.06s/it] 71%|███████   | 2480/3500 [2:31:28<35:04,  2.06s/it] 71%|███████   | 2481/3500 [2:31:30<35:02,  2.06s/it] 71%|███████   | 2482/3500 [2:31:32<34:59,  2.06s/it] 71%|███████   | 2483/3500 [2:31:34<34:56,  2.06s/it] 71%|███████   | 2484/3500 [2:31:36<34:54,  2.06s/it] 71%|███████   | 2485/3500 [2:31:38<34:52,  2.06s/it] 71%|███████   | 2486/3500 [2:31:40<34:50,  2.06s/it] 71%|███████   | 2487/3500 [2:31:42<34:48,  2.06s/it] 71%|███████   | 2488/3500 [2:31:44<34:46,  2.06s/it] 71%|███████   | 2489/3500 [2:31:47<34:43,  2.06s/it] 71%|███████   | 2490/3500 [2:31:49<34:41,  2.06s/it] 71%|███████   | 2491/3500 [2:31:51<34:39,  2.06s/it] 71%|███████   | 2492/3500 [2:31:53<34:38,  2.06s/it] 71%|███████   | 2493/3500 [2:31:55<34:35,  2.06s/it] 71%|███████▏  | 2494/3500 [2:31:57<34:33,  2.06s/it] 71%|███████▏  | 2495/3500 [2:31:59<34:32,  2.06s/it] 71%|███████▏  | 2496/3500 [2:32:01<34:29,  2.06s/it] 71%|███████▏  | 2497/3500 [2:32:03<34:28,  2.06s/it] 71%|███████▏  | 2498/3500 [2:32:05<34:27,  2.06s/it] 71%|███████▏  | 2499/3500 [2:32:07<34:24,  2.06s/it] 71%|███████▏  | 2500/3500 [2:32:09<34:22,  2.06s/it] 71%|███████▏  | 2501/3500 [2:32:11<34:19,  2.06s/it][2022-11-17 23:08:01,928][__main__][INFO] - epoch 7: perplexity: 19.667066660457653 train_loss: 0.052205830812454224 eval_loss: 2.97894549369812
Configuration saved in tuned-model/epoch_7_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_7_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_7_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_7_most_recent/special_tokens_map.json
 71%|███████▏  | 2502/3500 [2:33:35<7:19:55, 26.45s/it] 72%|███████▏  | 2503/3500 [2:33:37<5:17:54, 19.13s/it] 72%|███████▏  | 2504/3500 [2:33:39<3:52:34, 14.01s/it] 72%|███████▏  | 2505/3500 [2:33:41<2:52:53, 10.43s/it] 72%|███████▏  | 2506/3500 [2:33:43<2:11:08,  7.92s/it] 72%|███████▏  | 2507/3500 [2:33:45<1:41:57,  6.16s/it] 72%|███████▏  | 2508/3500 [2:33:47<1:21:32,  4.93s/it] 72%|███████▏  | 2509/3500 [2:33:49<1:07:14,  4.07s/it] 72%|███████▏  | 2510/3500 [2:33:51<57:13,  3.47s/it]   72%|███████▏  | 2511/3500 [2:33:53<50:12,  3.05s/it] 72%|███████▏  | 2512/3500 [2:33:55<45:17,  2.75s/it] 72%|███████▏  | 2513/3500 [2:33:57<41:50,  2.54s/it] 72%|███████▏  | 2514/3500 [2:33:59<39:25,  2.40s/it] 72%|███████▏  | 2515/3500 [2:34:01<37:43,  2.30s/it] 72%|███████▏  | 2516/3500 [2:34:03<36:31,  2.23s/it] 72%|███████▏  | 2517/3500 [2:34:06<35:40,  2.18s/it] 72%|███████▏  | 2518/3500 [2:34:08<35:04,  2.14s/it] 72%|███████▏  | 2519/3500 [2:34:10<34:38,  2.12s/it] 72%|███████▏  | 2520/3500 [2:34:12<34:19,  2.10s/it] 72%|███████▏  | 2521/3500 [2:34:14<34:06,  2.09s/it] 72%|███████▏  | 2522/3500 [2:34:16<33:55,  2.08s/it] 72%|███████▏  | 2523/3500 [2:34:18<33:48,  2.08s/it] 72%|███████▏  | 2524/3500 [2:34:20<33:41,  2.07s/it] 72%|███████▏  | 2525/3500 [2:34:22<33:37,  2.07s/it] 72%|███████▏  | 2526/3500 [2:34:24<33:33,  2.07s/it] 72%|███████▏  | 2527/3500 [2:34:26<33:29,  2.07s/it] 72%|███████▏  | 2528/3500 [2:34:28<33:26,  2.06s/it] 72%|███████▏  | 2529/3500 [2:34:30<33:23,  2.06s/it] 72%|███████▏  | 2530/3500 [2:34:32<33:20,  2.06s/it] 72%|███████▏  | 2531/3500 [2:34:34<33:18,  2.06s/it] 72%|███████▏  | 2532/3500 [2:34:36<33:16,  2.06s/it] 72%|███████▏  | 2533/3500 [2:34:39<33:13,  2.06s/it] 72%|███████▏  | 2534/3500 [2:34:41<33:11,  2.06s/it] 72%|███████▏  | 2535/3500 [2:34:43<33:09,  2.06s/it] 72%|███████▏  | 2536/3500 [2:34:45<33:07,  2.06s/it] 72%|███████▏  | 2537/3500 [2:34:47<33:05,  2.06s/it] 73%|███████▎  | 2538/3500 [2:34:49<33:03,  2.06s/it] 73%|███████▎  | 2539/3500 [2:34:51<33:01,  2.06s/it] 73%|███████▎  | 2540/3500 [2:34:53<32:59,  2.06s/it] 73%|███████▎  | 2541/3500 [2:34:55<32:57,  2.06s/it] 73%|███████▎  | 2542/3500 [2:34:57<32:55,  2.06s/it] 73%|███████▎  | 2543/3500 [2:34:59<32:53,  2.06s/it] 73%|███████▎  | 2544/3500 [2:35:01<32:52,  2.06s/it] 73%|███████▎  | 2545/3500 [2:35:03<32:49,  2.06s/it] 73%|███████▎  | 2546/3500 [2:35:05<32:47,  2.06s/it] 73%|███████▎  | 2547/3500 [2:35:07<32:45,  2.06s/it] 73%|███████▎  | 2548/3500 [2:35:09<32:43,  2.06s/it] 73%|███████▎  | 2549/3500 [2:35:12<32:41,  2.06s/it] 73%|███████▎  | 2550/3500 [2:35:14<32:39,  2.06s/it] 73%|███████▎  | 2551/3500 [2:35:16<32:37,  2.06s/it][2022-11-17 23:11:06,323][__main__][INFO] - epoch 7: perplexity: 19.82616188254797 train_loss: 0.05324971303343773 eval_loss: 2.987002372741699
Configuration saved in tuned-model/epoch_7_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_7_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_7_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_7_most_recent/special_tokens_map.json
 73%|███████▎  | 2552/3500 [2:36:37<6:48:13, 25.84s/it] 73%|███████▎  | 2553/3500 [2:36:39<4:55:12, 18.70s/it] 73%|███████▎  | 2554/3500 [2:36:41<3:36:10, 13.71s/it] 73%|███████▎  | 2555/3500 [2:36:43<2:40:54, 10.22s/it] 73%|███████▎  | 2556/3500 [2:36:45<2:02:14,  7.77s/it] 73%|███████▎  | 2557/3500 [2:36:47<1:35:12,  6.06s/it] 73%|███████▎  | 2558/3500 [2:36:49<1:16:16,  4.86s/it] 73%|███████▎  | 2559/3500 [2:36:51<1:03:02,  4.02s/it] 73%|███████▎  | 2560/3500 [2:36:53<53:47,  3.43s/it]   73%|███████▎  | 2561/3500 [2:36:56<47:17,  3.02s/it] 73%|███████▎  | 2562/3500 [2:36:58<42:44,  2.73s/it] 73%|███████▎  | 2563/3500 [2:37:00<39:32,  2.53s/it] 73%|███████▎  | 2564/3500 [2:37:02<37:18,  2.39s/it] 73%|███████▎  | 2565/3500 [2:37:04<35:43,  2.29s/it] 73%|███████▎  | 2566/3500 [2:37:06<34:37,  2.22s/it] 73%|███████▎  | 2567/3500 [2:37:08<33:49,  2.18s/it] 73%|███████▎  | 2568/3500 [2:37:10<33:15,  2.14s/it] 73%|███████▎  | 2569/3500 [2:37:12<32:51,  2.12s/it] 73%|███████▎  | 2570/3500 [2:37:14<32:34,  2.10s/it] 73%|███████▎  | 2571/3500 [2:37:16<32:20,  2.09s/it] 73%|███████▎  | 2572/3500 [2:37:18<32:11,  2.08s/it] 74%|███████▎  | 2573/3500 [2:37:20<32:03,  2.08s/it] 74%|███████▎  | 2574/3500 [2:37:22<31:57,  2.07s/it] 74%|███████▎  | 2575/3500 [2:37:24<31:53,  2.07s/it] 74%|███████▎  | 2576/3500 [2:37:26<31:49,  2.07s/it] 74%|███████▎  | 2577/3500 [2:37:28<31:45,  2.06s/it] 74%|███████▎  | 2578/3500 [2:37:31<31:43,  2.06s/it] 74%|███████▎  | 2579/3500 [2:37:33<31:40,  2.06s/it] 74%|███████▎  | 2580/3500 [2:37:35<31:38,  2.06s/it] 74%|███████▎  | 2581/3500 [2:37:37<31:36,  2.06s/it] 74%|███████▍  | 2582/3500 [2:37:39<31:34,  2.06s/it] 74%|███████▍  | 2583/3500 [2:37:41<31:31,  2.06s/it] 74%|███████▍  | 2584/3500 [2:37:43<31:29,  2.06s/it] 74%|███████▍  | 2585/3500 [2:37:45<31:26,  2.06s/it] 74%|███████▍  | 2586/3500 [2:37:47<31:24,  2.06s/it] 74%|███████▍  | 2587/3500 [2:37:49<31:22,  2.06s/it] 74%|███████▍  | 2588/3500 [2:37:51<31:20,  2.06s/it] 74%|███████▍  | 2589/3500 [2:37:53<31:18,  2.06s/it] 74%|███████▍  | 2590/3500 [2:37:55<31:16,  2.06s/it] 74%|███████▍  | 2591/3500 [2:37:57<31:14,  2.06s/it] 74%|███████▍  | 2592/3500 [2:37:59<31:12,  2.06s/it] 74%|███████▍  | 2593/3500 [2:38:01<31:10,  2.06s/it] 74%|███████▍  | 2594/3500 [2:38:04<31:08,  2.06s/it] 74%|███████▍  | 2595/3500 [2:38:06<31:06,  2.06s/it] 74%|███████▍  | 2596/3500 [2:38:08<31:04,  2.06s/it] 74%|███████▍  | 2597/3500 [2:38:10<31:02,  2.06s/it] 74%|███████▍  | 2598/3500 [2:38:12<30:59,  2.06s/it] 74%|███████▍  | 2599/3500 [2:38:14<30:57,  2.06s/it] 74%|███████▍  | 2600/3500 [2:38:16<30:55,  2.06s/it] 74%|███████▍  | 2601/3500 [2:38:18<30:53,  2.06s/it][2022-11-17 23:14:08,665][__main__][INFO] - epoch 7: perplexity: 20.072439134067395 train_loss: 0.05321550741791725 eval_loss: 2.999347686767578
Configuration saved in tuned-model/epoch_7_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_7_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_7_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_7_most_recent/special_tokens_map.json
 74%|███████▍  | 2602/3500 [2:39:40<6:28:15, 25.94s/it] 74%|███████▍  | 2603/3500 [2:39:42<4:40:43, 18.78s/it] 74%|███████▍  | 2604/3500 [2:39:44<3:25:31, 13.76s/it] 74%|███████▍  | 2605/3500 [2:39:46<2:32:56, 10.25s/it] 74%|███████▍  | 2606/3500 [2:39:48<1:56:09,  7.80s/it] 74%|███████▍  | 2607/3500 [2:39:50<1:30:24,  6.07s/it] 75%|███████▍  | 2608/3500 [2:39:52<1:12:24,  4.87s/it] 75%|███████▍  | 2609/3500 [2:39:54<59:48,  4.03s/it]   75%|███████▍  | 2610/3500 [2:39:56<50:59,  3.44s/it] 75%|███████▍  | 2611/3500 [2:39:58<44:48,  3.02s/it] 75%|███████▍  | 2612/3500 [2:40:00<40:29,  2.74s/it] 75%|███████▍  | 2613/3500 [2:40:02<37:28,  2.53s/it] 75%|███████▍  | 2614/3500 [2:40:04<35:19,  2.39s/it] 75%|███████▍  | 2615/3500 [2:40:06<33:49,  2.29s/it] 75%|███████▍  | 2616/3500 [2:40:09<32:46,  2.22s/it] 75%|███████▍  | 2617/3500 [2:40:11<32:01,  2.18s/it] 75%|███████▍  | 2618/3500 [2:40:13<31:29,  2.14s/it] 75%|███████▍  | 2619/3500 [2:40:15<31:06,  2.12s/it] 75%|███████▍  | 2620/3500 [2:40:17<30:49,  2.10s/it] 75%|███████▍  | 2621/3500 [2:40:19<30:36,  2.09s/it] 75%|███████▍  | 2622/3500 [2:40:21<30:28,  2.08s/it] 75%|███████▍  | 2623/3500 [2:40:23<30:20,  2.08s/it] 75%|███████▍  | 2624/3500 [2:40:25<30:15,  2.07s/it] 75%|███████▌  | 2625/3500 [2:40:27<30:10,  2.07s/it] 75%|███████▌  | 2626/3500 [2:40:29<30:06,  2.07s/it] 75%|███████▌  | 2627/3500 [2:40:31<30:03,  2.07s/it] 75%|███████▌  | 2628/3500 [2:40:33<30:00,  2.06s/it] 75%|███████▌  | 2629/3500 [2:40:35<29:57,  2.06s/it] 75%|███████▌  | 2630/3500 [2:40:37<29:54,  2.06s/it] 75%|███████▌  | 2631/3500 [2:40:39<29:52,  2.06s/it] 75%|███████▌  | 2632/3500 [2:40:42<29:50,  2.06s/it] 75%|███████▌  | 2633/3500 [2:40:44<29:49,  2.06s/it] 75%|███████▌  | 2634/3500 [2:40:46<29:46,  2.06s/it] 75%|███████▌  | 2635/3500 [2:40:48<29:44,  2.06s/it] 75%|███████▌  | 2636/3500 [2:40:50<29:42,  2.06s/it] 75%|███████▌  | 2637/3500 [2:40:52<29:39,  2.06s/it] 75%|███████▌  | 2638/3500 [2:40:54<29:37,  2.06s/it] 75%|███████▌  | 2639/3500 [2:40:56<29:35,  2.06s/it] 75%|███████▌  | 2640/3500 [2:40:58<29:34,  2.06s/it] 75%|███████▌  | 2641/3500 [2:41:00<29:32,  2.06s/it] 75%|███████▌  | 2642/3500 [2:41:02<29:30,  2.06s/it] 76%|███████▌  | 2643/3500 [2:41:04<29:28,  2.06s/it] 76%|███████▌  | 2644/3500 [2:41:06<29:26,  2.06s/it] 76%|███████▌  | 2645/3500 [2:41:08<29:24,  2.06s/it] 76%|███████▌  | 2646/3500 [2:41:10<29:21,  2.06s/it] 76%|███████▌  | 2647/3500 [2:41:12<29:19,  2.06s/it] 76%|███████▌  | 2648/3500 [2:41:15<29:17,  2.06s/it] 76%|███████▌  | 2649/3500 [2:41:17<29:15,  2.06s/it] 76%|███████▌  | 2650/3500 [2:41:19<29:12,  2.06s/it] 76%|███████▌  | 2651/3500 [2:41:21<29:10,  2.06s/it][2022-11-17 23:17:11,354][__main__][INFO] - epoch 7: perplexity: 20.096760134423402 train_loss: 0.0526738315820694 eval_loss: 3.000558614730835
Configuration saved in tuned-model/epoch_7_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_7_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_7_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_7_most_recent/special_tokens_map.json
 76%|███████▌  | 2652/3500 [2:42:42<6:05:29, 25.86s/it] 76%|███████▌  | 2653/3500 [2:42:44<4:24:16, 18.72s/it] 76%|███████▌  | 2654/3500 [2:42:46<3:13:29, 13.72s/it] 76%|███████▌  | 2655/3500 [2:42:48<2:23:59, 10.22s/it] 76%|███████▌  | 2656/3500 [2:42:50<1:49:22,  7.78s/it] 76%|███████▌  | 2657/3500 [2:42:52<1:25:09,  6.06s/it] 76%|███████▌  | 2658/3500 [2:42:54<1:08:12,  4.86s/it] 76%|███████▌  | 2659/3500 [2:42:57<56:21,  4.02s/it]   76%|███████▌  | 2660/3500 [2:42:59<48:04,  3.43s/it] 76%|███████▌  | 2661/3500 [2:43:01<42:15,  3.02s/it] 76%|███████▌  | 2662/3500 [2:43:03<38:10,  2.73s/it] 76%|███████▌  | 2663/3500 [2:43:05<35:18,  2.53s/it] 76%|███████▌  | 2664/3500 [2:43:07<33:18,  2.39s/it] 76%|███████▌  | 2665/3500 [2:43:09<31:54,  2.29s/it] 76%|███████▌  | 2666/3500 [2:43:11<30:54,  2.22s/it] 76%|███████▌  | 2667/3500 [2:43:13<30:11,  2.18s/it] 76%|███████▌  | 2668/3500 [2:43:15<29:41,  2.14s/it] 76%|███████▋  | 2669/3500 [2:43:17<29:19,  2.12s/it] 76%|███████▋  | 2670/3500 [2:43:19<29:03,  2.10s/it] 76%|███████▋  | 2671/3500 [2:43:21<28:51,  2.09s/it] 76%|███████▋  | 2672/3500 [2:43:23<28:42,  2.08s/it] 76%|███████▋  | 2673/3500 [2:43:25<28:36,  2.08s/it] 76%|███████▋  | 2674/3500 [2:43:27<28:30,  2.07s/it] 76%|███████▋  | 2675/3500 [2:43:30<28:26,  2.07s/it] 76%|███████▋  | 2676/3500 [2:43:32<28:22,  2.07s/it] 76%|███████▋  | 2677/3500 [2:43:34<28:19,  2.06s/it] 77%|███████▋  | 2678/3500 [2:43:36<28:16,  2.06s/it] 77%|███████▋  | 2679/3500 [2:43:38<28:13,  2.06s/it] 77%|███████▋  | 2680/3500 [2:43:40<28:11,  2.06s/it] 77%|███████▋  | 2681/3500 [2:43:42<28:08,  2.06s/it] 77%|███████▋  | 2682/3500 [2:43:44<28:06,  2.06s/it] 77%|███████▋  | 2683/3500 [2:43:46<28:05,  2.06s/it] 77%|███████▋  | 2684/3500 [2:43:48<28:03,  2.06s/it] 77%|███████▋  | 2685/3500 [2:43:50<28:01,  2.06s/it] 77%|███████▋  | 2686/3500 [2:43:52<27:59,  2.06s/it] 77%|███████▋  | 2687/3500 [2:43:54<27:56,  2.06s/it] 77%|███████▋  | 2688/3500 [2:43:56<27:54,  2.06s/it] 77%|███████▋  | 2689/3500 [2:43:58<27:53,  2.06s/it] 77%|███████▋  | 2690/3500 [2:44:00<27:50,  2.06s/it] 77%|███████▋  | 2691/3500 [2:44:03<27:48,  2.06s/it] 77%|███████▋  | 2692/3500 [2:44:05<27:46,  2.06s/it] 77%|███████▋  | 2693/3500 [2:44:07<27:44,  2.06s/it] 77%|███████▋  | 2694/3500 [2:44:09<27:42,  2.06s/it] 77%|███████▋  | 2695/3500 [2:44:11<27:40,  2.06s/it] 77%|███████▋  | 2696/3500 [2:44:13<27:38,  2.06s/it] 77%|███████▋  | 2697/3500 [2:44:15<27:36,  2.06s/it] 77%|███████▋  | 2698/3500 [2:44:17<27:34,  2.06s/it] 77%|███████▋  | 2699/3500 [2:44:19<27:31,  2.06s/it] 77%|███████▋  | 2700/3500 [2:44:21<27:29,  2.06s/it] 77%|███████▋  | 2701/3500 [2:44:23<27:27,  2.06s/it][2022-11-17 23:20:13,786][__main__][INFO] - epoch 7: perplexity: 20.295391852845498 train_loss: 0.053372617810964584 eval_loss: 3.0103938579559326
Configuration saved in tuned-model/epoch_7_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_7_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_7_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_7_most_recent/special_tokens_map.json
 77%|███████▋  | 2702/3500 [2:45:47<5:53:46, 26.60s/it] 77%|███████▋  | 2703/3500 [2:45:49<4:15:33, 19.24s/it] 77%|███████▋  | 2704/3500 [2:45:51<3:06:52, 14.09s/it] 77%|███████▋  | 2705/3500 [2:45:53<2:18:50, 10.48s/it] 77%|███████▋  | 2706/3500 [2:45:55<1:45:14,  7.95s/it] 77%|███████▋  | 2707/3500 [2:45:57<1:21:45,  6.19s/it] 77%|███████▋  | 2708/3500 [2:45:59<1:05:19,  4.95s/it] 77%|███████▋  | 2709/3500 [2:46:01<53:49,  4.08s/it]   77%|███████▋  | 2710/3500 [2:46:03<45:46,  3.48s/it] 77%|███████▋  | 2711/3500 [2:46:06<40:07,  3.05s/it] 77%|███████▋  | 2712/3500 [2:46:08<36:10,  2.75s/it] 78%|███████▊  | 2713/3500 [2:46:10<33:24,  2.55s/it] 78%|███████▊  | 2714/3500 [2:46:12<31:27,  2.40s/it] 78%|███████▊  | 2715/3500 [2:46:14<30:05,  2.30s/it] 78%|███████▊  | 2716/3500 [2:46:16<29:07,  2.23s/it] 78%|███████▊  | 2717/3500 [2:46:18<28:25,  2.18s/it] 78%|███████▊  | 2718/3500 [2:46:20<27:56,  2.14s/it] 78%|███████▊  | 2719/3500 [2:46:22<27:35,  2.12s/it] 78%|███████▊  | 2720/3500 [2:46:24<27:20,  2.10s/it] 78%|███████▊  | 2721/3500 [2:46:26<27:08,  2.09s/it] 78%|███████▊  | 2722/3500 [2:46:28<26:59,  2.08s/it] 78%|███████▊  | 2723/3500 [2:46:30<26:53,  2.08s/it] 78%|███████▊  | 2724/3500 [2:46:32<26:48,  2.07s/it] 78%|███████▊  | 2725/3500 [2:46:34<26:43,  2.07s/it] 78%|███████▊  | 2726/3500 [2:46:36<26:39,  2.07s/it] 78%|███████▊  | 2727/3500 [2:46:39<26:36,  2.07s/it] 78%|███████▊  | 2728/3500 [2:46:41<26:33,  2.06s/it] 78%|███████▊  | 2729/3500 [2:46:43<26:30,  2.06s/it] 78%|███████▊  | 2730/3500 [2:46:45<26:28,  2.06s/it] 78%|███████▊  | 2731/3500 [2:46:47<26:26,  2.06s/it] 78%|███████▊  | 2732/3500 [2:46:49<26:23,  2.06s/it] 78%|███████▊  | 2733/3500 [2:46:51<26:21,  2.06s/it] 78%|███████▊  | 2734/3500 [2:46:53<26:19,  2.06s/it] 78%|███████▊  | 2735/3500 [2:46:55<26:17,  2.06s/it] 78%|███████▊  | 2736/3500 [2:46:57<26:16,  2.06s/it] 78%|███████▊  | 2737/3500 [2:46:59<26:14,  2.06s/it] 78%|███████▊  | 2738/3500 [2:47:01<26:11,  2.06s/it] 78%|███████▊  | 2739/3500 [2:47:03<26:09,  2.06s/it] 78%|███████▊  | 2740/3500 [2:47:05<26:07,  2.06s/it] 78%|███████▊  | 2741/3500 [2:47:07<26:05,  2.06s/it] 78%|███████▊  | 2742/3500 [2:47:09<26:03,  2.06s/it] 78%|███████▊  | 2743/3500 [2:47:12<26:01,  2.06s/it] 78%|███████▊  | 2744/3500 [2:47:14<25:59,  2.06s/it] 78%|███████▊  | 2745/3500 [2:47:16<25:57,  2.06s/it] 78%|███████▊  | 2746/3500 [2:47:18<25:55,  2.06s/it] 78%|███████▊  | 2747/3500 [2:47:20<25:53,  2.06s/it] 79%|███████▊  | 2748/3500 [2:47:22<25:51,  2.06s/it] 79%|███████▊  | 2749/3500 [2:47:24<25:48,  2.06s/it] 79%|███████▊  | 2750/3500 [2:47:26<25:46,  2.06s/it] 79%|███████▊  | 2751/3500 [2:47:28<25:44,  2.06s/it][2022-11-17 23:23:18,679][__main__][INFO] - epoch 7: perplexity: 19.937237918514477 train_loss: 0.053777631372213364 eval_loss: 2.992589235305786
Configuration saved in tuned-model/epoch_7_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_7_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_7_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_7_most_recent/special_tokens_map.json
 79%|███████▊  | 2752/3500 [2:48:53<5:36:32, 27.00s/it] 79%|███████▊  | 2753/3500 [2:48:55<4:02:57, 19.52s/it] 79%|███████▊  | 2754/3500 [2:48:57<2:57:32, 14.28s/it] 79%|███████▊  | 2755/3500 [2:48:59<2:11:47, 10.61s/it] 79%|███████▊  | 2756/3500 [2:49:01<1:39:47,  8.05s/it] 79%|███████▉  | 2757/3500 [2:49:04<1:17:26,  6.25s/it] 79%|███████▉  | 2758/3500 [2:49:06<1:01:46,  5.00s/it] 79%|███████▉  | 2759/3500 [2:49:08<50:49,  4.12s/it]   79%|███████▉  | 2760/3500 [2:49:10<43:09,  3.50s/it] 79%|███████▉  | 2761/3500 [2:49:12<37:47,  3.07s/it] 79%|███████▉  | 2762/3500 [2:49:14<34:01,  2.77s/it] 79%|███████▉  | 2763/3500 [2:49:16<31:23,  2.56s/it] 79%|███████▉  | 2764/3500 [2:49:18<29:31,  2.41s/it] 79%|███████▉  | 2765/3500 [2:49:20<28:13,  2.30s/it] 79%|███████▉  | 2766/3500 [2:49:22<27:17,  2.23s/it] 79%|███████▉  | 2767/3500 [2:49:24<26:38,  2.18s/it] 79%|███████▉  | 2768/3500 [2:49:26<26:09,  2.14s/it] 79%|███████▉  | 2769/3500 [2:49:28<25:49,  2.12s/it] 79%|███████▉  | 2770/3500 [2:49:30<25:34,  2.10s/it] 79%|███████▉  | 2771/3500 [2:49:32<25:23,  2.09s/it] 79%|███████▉  | 2772/3500 [2:49:34<25:15,  2.08s/it] 79%|███████▉  | 2773/3500 [2:49:37<25:08,  2.07s/it] 79%|███████▉  | 2774/3500 [2:49:39<25:03,  2.07s/it] 79%|███████▉  | 2775/3500 [2:49:41<24:59,  2.07s/it] 79%|███████▉  | 2776/3500 [2:49:43<24:55,  2.07s/it] 79%|███████▉  | 2777/3500 [2:49:45<24:52,  2.06s/it] 79%|███████▉  | 2778/3500 [2:49:47<24:49,  2.06s/it] 79%|███████▉  | 2779/3500 [2:49:49<24:47,  2.06s/it] 79%|███████▉  | 2780/3500 [2:49:51<24:44,  2.06s/it] 79%|███████▉  | 2781/3500 [2:49:53<24:42,  2.06s/it] 79%|███████▉  | 2782/3500 [2:49:55<24:40,  2.06s/it] 80%|███████▉  | 2783/3500 [2:49:57<24:38,  2.06s/it] 80%|███████▉  | 2784/3500 [2:49:59<24:35,  2.06s/it] 80%|███████▉  | 2785/3500 [2:50:01<24:33,  2.06s/it] 80%|███████▉  | 2786/3500 [2:50:03<24:31,  2.06s/it] 80%|███████▉  | 2787/3500 [2:50:05<24:29,  2.06s/it] 80%|███████▉  | 2788/3500 [2:50:07<24:27,  2.06s/it] 80%|███████▉  | 2789/3500 [2:50:09<24:25,  2.06s/it] 80%|███████▉  | 2790/3500 [2:50:12<24:23,  2.06s/it] 80%|███████▉  | 2791/3500 [2:50:14<24:21,  2.06s/it] 80%|███████▉  | 2792/3500 [2:50:16<24:19,  2.06s/it] 80%|███████▉  | 2793/3500 [2:50:18<24:17,  2.06s/it] 80%|███████▉  | 2794/3500 [2:50:20<24:15,  2.06s/it] 80%|███████▉  | 2795/3500 [2:50:22<24:13,  2.06s/it] 80%|███████▉  | 2796/3500 [2:50:24<24:11,  2.06s/it] 80%|███████▉  | 2797/3500 [2:50:26<24:09,  2.06s/it] 80%|███████▉  | 2798/3500 [2:50:28<24:07,  2.06s/it] 80%|███████▉  | 2799/3500 [2:50:30<24:05,  2.06s/it] 80%|████████  | 2800/3500 [2:50:32<24:02,  2.06s/it][2022-11-17 23:25:34,496][__main__][INFO] - done epoch 7
 80%|████████  | 2801/3500 [2:50:34<24:14,  2.08s/it][2022-11-17 23:26:25,325][__main__][INFO] - epoch 8: perplexity: 19.963422609070097 train_loss: 0.03730222210288048 eval_loss: 2.9939017295837402
Configuration saved in tuned-model/epoch_8_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_8_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_8_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_8_most_recent/special_tokens_map.json
 80%|████████  | 2802/3500 [2:51:41<4:11:00, 21.58s/it] 80%|████████  | 2803/3500 [2:51:43<3:02:38, 15.72s/it] 80%|████████  | 2804/3500 [2:51:45<2:14:50, 11.62s/it] 80%|████████  | 2805/3500 [2:51:48<1:41:25,  8.76s/it] 80%|████████  | 2806/3500 [2:51:50<1:18:02,  6.75s/it] 80%|████████  | 2807/3500 [2:51:52<1:01:41,  5.34s/it] 80%|████████  | 2808/3500 [2:51:54<50:15,  4.36s/it]   80%|████████  | 2809/3500 [2:51:56<42:16,  3.67s/it] 80%|████████  | 2810/3500 [2:51:58<36:40,  3.19s/it] 80%|████████  | 2811/3500 [2:52:00<32:46,  2.85s/it] 80%|████████  | 2812/3500 [2:52:02<30:01,  2.62s/it] 80%|████████  | 2813/3500 [2:52:04<28:06,  2.45s/it] 80%|████████  | 2814/3500 [2:52:06<26:44,  2.34s/it] 80%|████████  | 2815/3500 [2:52:08<25:45,  2.26s/it] 80%|████████  | 2816/3500 [2:52:10<25:03,  2.20s/it] 80%|████████  | 2817/3500 [2:52:12<24:33,  2.16s/it] 81%|████████  | 2818/3500 [2:52:14<24:12,  2.13s/it] 81%|████████  | 2819/3500 [2:52:16<23:56,  2.11s/it] 81%|████████  | 2820/3500 [2:52:19<23:44,  2.09s/it] 81%|████████  | 2821/3500 [2:52:21<23:35,  2.08s/it] 81%|████████  | 2822/3500 [2:52:23<23:28,  2.08s/it] 81%|████████  | 2823/3500 [2:52:25<23:23,  2.07s/it] 81%|████████  | 2824/3500 [2:52:27<23:18,  2.07s/it] 81%|████████  | 2825/3500 [2:52:29<23:15,  2.07s/it] 81%|████████  | 2826/3500 [2:52:31<23:11,  2.06s/it] 81%|████████  | 2827/3500 [2:52:33<23:08,  2.06s/it] 81%|████████  | 2828/3500 [2:52:35<23:06,  2.06s/it] 81%|████████  | 2829/3500 [2:52:37<23:03,  2.06s/it] 81%|████████  | 2830/3500 [2:52:39<23:01,  2.06s/it] 81%|████████  | 2831/3500 [2:52:41<22:59,  2.06s/it] 81%|████████  | 2832/3500 [2:52:43<22:57,  2.06s/it] 81%|████████  | 2833/3500 [2:52:45<22:55,  2.06s/it] 81%|████████  | 2834/3500 [2:52:47<22:53,  2.06s/it] 81%|████████  | 2835/3500 [2:52:49<22:50,  2.06s/it] 81%|████████  | 2836/3500 [2:52:52<22:48,  2.06s/it] 81%|████████  | 2837/3500 [2:52:54<22:46,  2.06s/it] 81%|████████  | 2838/3500 [2:52:56<22:44,  2.06s/it] 81%|████████  | 2839/3500 [2:52:58<22:42,  2.06s/it] 81%|████████  | 2840/3500 [2:53:00<22:40,  2.06s/it] 81%|████████  | 2841/3500 [2:53:02<22:39,  2.06s/it] 81%|████████  | 2842/3500 [2:53:04<22:37,  2.06s/it] 81%|████████  | 2843/3500 [2:53:06<22:34,  2.06s/it] 81%|████████▏ | 2844/3500 [2:53:08<22:32,  2.06s/it] 81%|████████▏ | 2845/3500 [2:53:10<22:30,  2.06s/it] 81%|████████▏ | 2846/3500 [2:53:12<22:28,  2.06s/it] 81%|████████▏ | 2847/3500 [2:53:14<22:26,  2.06s/it] 81%|████████▏ | 2848/3500 [2:53:16<22:24,  2.06s/it] 81%|████████▏ | 2849/3500 [2:53:18<22:22,  2.06s/it] 81%|████████▏ | 2850/3500 [2:53:20<22:19,  2.06s/it] 81%|████████▏ | 2851/3500 [2:53:22<22:17,  2.06s/it][2022-11-17 23:29:13,109][__main__][INFO] - epoch 8: perplexity: 22.04643374982818 train_loss: 0.03600503131747246 eval_loss: 3.0931508541107178
Configuration saved in tuned-model/epoch_8_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_8_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_8_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_8_most_recent/special_tokens_map.json
 81%|████████▏ | 2852/3500 [2:54:49<4:55:13, 27.34s/it] 82%|████████▏ | 2853/3500 [2:54:51<3:33:00, 19.75s/it] 82%|████████▏ | 2854/3500 [2:54:53<2:35:31, 14.45s/it] 82%|████████▏ | 2855/3500 [2:54:55<1:55:21, 10.73s/it] 82%|████████▏ | 2856/3500 [2:54:57<1:27:15,  8.13s/it] 82%|████████▏ | 2857/3500 [2:54:59<1:07:37,  6.31s/it] 82%|████████▏ | 2858/3500 [2:55:01<53:52,  5.04s/it]   82%|████████▏ | 2859/3500 [2:55:03<44:16,  4.14s/it] 82%|████████▏ | 2860/3500 [2:55:05<37:32,  3.52s/it] 82%|████████▏ | 2861/3500 [2:55:07<32:49,  3.08s/it] 82%|████████▏ | 2862/3500 [2:55:09<29:31,  2.78s/it] 82%|████████▏ | 2863/3500 [2:55:11<27:12,  2.56s/it] 82%|████████▏ | 2864/3500 [2:55:13<25:34,  2.41s/it] 82%|████████▏ | 2865/3500 [2:55:16<24:25,  2.31s/it] 82%|████████▏ | 2866/3500 [2:55:18<23:36,  2.23s/it] 82%|████████▏ | 2867/3500 [2:55:20<23:01,  2.18s/it] 82%|████████▏ | 2868/3500 [2:55:22<22:36,  2.15s/it] 82%|████████▏ | 2869/3500 [2:55:24<22:18,  2.12s/it] 82%|████████▏ | 2870/3500 [2:55:26<22:05,  2.10s/it] 82%|████████▏ | 2871/3500 [2:55:28<21:55,  2.09s/it] 82%|████████▏ | 2872/3500 [2:55:30<21:48,  2.08s/it] 82%|████████▏ | 2873/3500 [2:55:32<21:42,  2.08s/it] 82%|████████▏ | 2874/3500 [2:55:34<21:37,  2.07s/it] 82%|████████▏ | 2875/3500 [2:55:36<21:33,  2.07s/it] 82%|████████▏ | 2876/3500 [2:55:38<21:30,  2.07s/it] 82%|████████▏ | 2877/3500 [2:55:40<21:27,  2.07s/it] 82%|████████▏ | 2878/3500 [2:55:42<21:24,  2.07s/it] 82%|████████▏ | 2879/3500 [2:55:44<21:22,  2.06s/it] 82%|████████▏ | 2880/3500 [2:55:46<21:19,  2.06s/it] 82%|████████▏ | 2881/3500 [2:55:49<21:17,  2.06s/it] 82%|████████▏ | 2882/3500 [2:55:51<21:15,  2.06s/it] 82%|████████▏ | 2883/3500 [2:55:53<21:12,  2.06s/it] 82%|████████▏ | 2884/3500 [2:55:55<21:10,  2.06s/it] 82%|████████▏ | 2885/3500 [2:55:57<21:08,  2.06s/it] 82%|████████▏ | 2886/3500 [2:55:59<21:06,  2.06s/it] 82%|████████▏ | 2887/3500 [2:56:01<21:03,  2.06s/it] 83%|████████▎ | 2888/3500 [2:56:03<21:01,  2.06s/it] 83%|████████▎ | 2889/3500 [2:56:05<21:00,  2.06s/it] 83%|████████▎ | 2890/3500 [2:56:07<20:57,  2.06s/it] 83%|████████▎ | 2891/3500 [2:56:09<20:55,  2.06s/it] 83%|████████▎ | 2892/3500 [2:56:11<20:53,  2.06s/it] 83%|████████▎ | 2893/3500 [2:56:13<20:51,  2.06s/it] 83%|████████▎ | 2894/3500 [2:56:15<20:49,  2.06s/it] 83%|████████▎ | 2895/3500 [2:56:17<20:47,  2.06s/it] 83%|████████▎ | 2896/3500 [2:56:19<20:45,  2.06s/it] 83%|████████▎ | 2897/3500 [2:56:22<20:43,  2.06s/it] 83%|████████▎ | 2898/3500 [2:56:24<20:41,  2.06s/it] 83%|████████▎ | 2899/3500 [2:56:26<20:39,  2.06s/it] 83%|████████▎ | 2900/3500 [2:56:28<20:37,  2.06s/it] 83%|████████▎ | 2901/3500 [2:56:30<20:35,  2.06s/it][2022-11-17 23:32:20,446][__main__][INFO] - epoch 8: perplexity: 21.913714313277794 train_loss: 0.0342477411031723 eval_loss: 3.0871126651763916
Configuration saved in tuned-model/epoch_8_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_8_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_8_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_8_most_recent/special_tokens_map.json
 83%|████████▎ | 2902/3500 [2:57:53<4:22:06, 26.30s/it] 83%|████████▎ | 2903/3500 [2:57:55<3:09:19, 19.03s/it] 83%|████████▎ | 2904/3500 [2:57:57<2:18:26, 13.94s/it] 83%|████████▎ | 2905/3500 [2:57:59<1:42:52, 10.37s/it] 83%|████████▎ | 2906/3500 [2:58:01<1:18:01,  7.88s/it] 83%|████████▎ | 2907/3500 [2:58:03<1:00:38,  6.14s/it] 83%|████████▎ | 2908/3500 [2:58:05<48:28,  4.91s/it]   83%|████████▎ | 2909/3500 [2:58:07<39:57,  4.06s/it] 83%|████████▎ | 2910/3500 [2:58:09<34:00,  3.46s/it] 83%|████████▎ | 2911/3500 [2:58:11<29:50,  3.04s/it] 83%|████████▎ | 2912/3500 [2:58:13<26:54,  2.75s/it] 83%|████████▎ | 2913/3500 [2:58:15<24:51,  2.54s/it] 83%|████████▎ | 2914/3500 [2:58:17<23:24,  2.40s/it] 83%|████████▎ | 2915/3500 [2:58:19<22:22,  2.30s/it] 83%|████████▎ | 2916/3500 [2:58:21<21:39,  2.23s/it] 83%|████████▎ | 2917/3500 [2:58:24<21:08,  2.18s/it] 83%|████████▎ | 2918/3500 [2:58:26<20:46,  2.14s/it] 83%|████████▎ | 2919/3500 [2:58:28<20:30,  2.12s/it] 83%|████████▎ | 2920/3500 [2:58:30<20:18,  2.10s/it] 83%|████████▎ | 2921/3500 [2:58:32<20:09,  2.09s/it] 83%|████████▎ | 2922/3500 [2:58:34<20:02,  2.08s/it] 84%|████████▎ | 2923/3500 [2:58:36<19:56,  2.07s/it] 84%|████████▎ | 2924/3500 [2:58:38<19:52,  2.07s/it] 84%|████████▎ | 2925/3500 [2:58:40<19:48,  2.07s/it] 84%|████████▎ | 2926/3500 [2:58:42<19:45,  2.07s/it] 84%|████████▎ | 2927/3500 [2:58:44<19:42,  2.06s/it] 84%|████████▎ | 2928/3500 [2:58:46<19:40,  2.06s/it] 84%|████████▎ | 2929/3500 [2:58:48<19:37,  2.06s/it] 84%|████████▎ | 2930/3500 [2:58:50<19:35,  2.06s/it] 84%|████████▎ | 2931/3500 [2:58:52<19:33,  2.06s/it] 84%|████████▍ | 2932/3500 [2:58:54<19:31,  2.06s/it] 84%|████████▍ | 2933/3500 [2:58:57<19:29,  2.06s/it] 84%|████████▍ | 2934/3500 [2:58:59<19:27,  2.06s/it] 84%|████████▍ | 2935/3500 [2:59:01<19:25,  2.06s/it] 84%|████████▍ | 2936/3500 [2:59:03<19:23,  2.06s/it] 84%|████████▍ | 2937/3500 [2:59:05<19:20,  2.06s/it] 84%|████████▍ | 2938/3500 [2:59:07<19:19,  2.06s/it] 84%|████████▍ | 2939/3500 [2:59:09<19:17,  2.06s/it] 84%|████████▍ | 2940/3500 [2:59:11<19:15,  2.06s/it] 84%|████████▍ | 2941/3500 [2:59:13<19:12,  2.06s/it] 84%|████████▍ | 2942/3500 [2:59:15<19:10,  2.06s/it] 84%|████████▍ | 2943/3500 [2:59:17<19:08,  2.06s/it] 84%|████████▍ | 2944/3500 [2:59:19<19:06,  2.06s/it] 84%|████████▍ | 2945/3500 [2:59:21<19:04,  2.06s/it] 84%|████████▍ | 2946/3500 [2:59:23<19:02,  2.06s/it] 84%|████████▍ | 2947/3500 [2:59:25<19:00,  2.06s/it] 84%|████████▍ | 2948/3500 [2:59:27<18:58,  2.06s/it] 84%|████████▍ | 2949/3500 [2:59:30<18:56,  2.06s/it] 84%|████████▍ | 2950/3500 [2:59:32<18:54,  2.06s/it] 84%|████████▍ | 2951/3500 [2:59:34<18:51,  2.06s/it][2022-11-17 23:35:24,311][__main__][INFO] - epoch 8: perplexity: 22.127307663907462 train_loss: 0.03398061916232109 eval_loss: 3.0968124866485596
Configuration saved in tuned-model/epoch_8_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_8_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_8_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_8_most_recent/special_tokens_map.json
 84%|████████▍ | 2952/3500 [3:00:58<4:04:24, 26.76s/it] 84%|████████▍ | 2953/3500 [3:01:00<2:56:24, 19.35s/it] 84%|████████▍ | 2954/3500 [3:01:02<2:08:52, 14.16s/it] 84%|████████▍ | 2955/3500 [3:01:04<1:35:39, 10.53s/it] 84%|████████▍ | 2956/3500 [3:01:06<1:12:27,  7.99s/it] 84%|████████▍ | 2957/3500 [3:01:08<56:13,  6.21s/it]   85%|████████▍ | 2958/3500 [3:01:10<44:52,  4.97s/it] 85%|████████▍ | 2959/3500 [3:01:12<36:55,  4.10s/it] 85%|████████▍ | 2960/3500 [3:01:15<31:22,  3.49s/it] 85%|████████▍ | 2961/3500 [3:01:17<27:28,  3.06s/it] 85%|████████▍ | 2962/3500 [3:01:19<24:44,  2.76s/it] 85%|████████▍ | 2963/3500 [3:01:21<22:49,  2.55s/it] 85%|████████▍ | 2964/3500 [3:01:23<21:28,  2.40s/it] 85%|████████▍ | 2965/3500 [3:01:25<20:30,  2.30s/it] 85%|████████▍ | 2966/3500 [3:01:27<19:50,  2.23s/it] 85%|████████▍ | 2967/3500 [3:01:29<19:21,  2.18s/it] 85%|████████▍ | 2968/3500 [3:01:31<19:00,  2.14s/it] 85%|████████▍ | 2969/3500 [3:01:33<18:45,  2.12s/it] 85%|████████▍ | 2970/3500 [3:01:35<18:33,  2.10s/it] 85%|████████▍ | 2971/3500 [3:01:37<18:24,  2.09s/it] 85%|████████▍ | 2972/3500 [3:01:39<18:18,  2.08s/it] 85%|████████▍ | 2973/3500 [3:01:41<18:13,  2.07s/it] 85%|████████▍ | 2974/3500 [3:01:43<18:09,  2.07s/it] 85%|████████▌ | 2975/3500 [3:01:45<18:05,  2.07s/it] 85%|████████▌ | 2976/3500 [3:01:48<18:02,  2.07s/it] 85%|████████▌ | 2977/3500 [3:01:50<17:59,  2.06s/it] 85%|████████▌ | 2978/3500 [3:01:52<17:57,  2.06s/it] 85%|████████▌ | 2979/3500 [3:01:54<17:54,  2.06s/it] 85%|████████▌ | 2980/3500 [3:01:56<17:52,  2.06s/it] 85%|████████▌ | 2981/3500 [3:01:58<17:50,  2.06s/it] 85%|████████▌ | 2982/3500 [3:02:00<17:48,  2.06s/it] 85%|████████▌ | 2983/3500 [3:02:02<17:45,  2.06s/it] 85%|████████▌ | 2984/3500 [3:02:04<17:43,  2.06s/it] 85%|████████▌ | 2985/3500 [3:02:06<17:41,  2.06s/it] 85%|████████▌ | 2986/3500 [3:02:08<17:39,  2.06s/it] 85%|████████▌ | 2987/3500 [3:02:10<17:37,  2.06s/it] 85%|████████▌ | 2988/3500 [3:02:12<17:35,  2.06s/it] 85%|████████▌ | 2989/3500 [3:02:14<17:33,  2.06s/it] 85%|████████▌ | 2990/3500 [3:02:16<17:31,  2.06s/it] 85%|████████▌ | 2991/3500 [3:02:18<17:28,  2.06s/it] 85%|████████▌ | 2992/3500 [3:02:20<17:27,  2.06s/it] 86%|████████▌ | 2993/3500 [3:02:23<17:25,  2.06s/it] 86%|████████▌ | 2994/3500 [3:02:25<17:23,  2.06s/it] 86%|████████▌ | 2995/3500 [3:02:27<17:20,  2.06s/it] 86%|████████▌ | 2996/3500 [3:02:29<17:18,  2.06s/it] 86%|████████▌ | 2997/3500 [3:02:31<17:16,  2.06s/it] 86%|████████▌ | 2998/3500 [3:02:33<17:14,  2.06s/it] 86%|████████▌ | 2999/3500 [3:02:35<17:13,  2.06s/it] 86%|████████▌ | 3000/3500 [3:02:37<17:10,  2.06s/it] 86%|████████▌ | 3001/3500 [3:02:39<17:08,  2.06s/it][2022-11-17 23:38:29,698][__main__][INFO] - epoch 8: perplexity: 21.932687953516293 train_loss: 0.034141216427087784 eval_loss: 3.0879781246185303
Configuration saved in tuned-model/epoch_8_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_8_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_8_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_8_most_recent/special_tokens_map.json
 86%|████████▌ | 3002/3500 [3:04:05<3:46:16, 27.26s/it] 86%|████████▌ | 3003/3500 [3:04:07<2:43:11, 19.70s/it] 86%|████████▌ | 3004/3500 [3:04:09<1:59:07, 14.41s/it] 86%|████████▌ | 3005/3500 [3:04:11<1:28:19, 10.71s/it] 86%|████████▌ | 3006/3500 [3:04:13<1:06:47,  8.11s/it] 86%|████████▌ | 3007/3500 [3:04:15<51:44,  6.30s/it]   86%|████████▌ | 3008/3500 [3:04:17<41:13,  5.03s/it] 86%|████████▌ | 3009/3500 [3:04:20<33:51,  4.14s/it] 86%|████████▌ | 3010/3500 [3:04:22<28:42,  3.52s/it] 86%|████████▌ | 3011/3500 [3:04:24<25:05,  3.08s/it] 86%|████████▌ | 3012/3500 [3:04:26<22:34,  2.78s/it] 86%|████████▌ | 3013/3500 [3:04:28<20:47,  2.56s/it] 86%|████████▌ | 3014/3500 [3:04:30<19:32,  2.41s/it] 86%|████████▌ | 3015/3500 [3:04:32<18:38,  2.31s/it] 86%|████████▌ | 3016/3500 [3:04:34<18:00,  2.23s/it] 86%|████████▌ | 3017/3500 [3:04:36<17:33,  2.18s/it] 86%|████████▌ | 3018/3500 [3:04:38<17:14,  2.15s/it] 86%|████████▋ | 3019/3500 [3:04:40<16:59,  2.12s/it] 86%|████████▋ | 3020/3500 [3:04:42<16:49,  2.10s/it] 86%|████████▋ | 3021/3500 [3:04:44<16:41,  2.09s/it] 86%|████████▋ | 3022/3500 [3:04:46<16:35,  2.08s/it] 86%|████████▋ | 3023/3500 [3:04:48<16:30,  2.08s/it] 86%|████████▋ | 3024/3500 [3:04:50<16:26,  2.07s/it] 86%|████████▋ | 3025/3500 [3:04:53<16:23,  2.07s/it] 86%|████████▋ | 3026/3500 [3:04:55<16:19,  2.07s/it] 86%|████████▋ | 3027/3500 [3:04:57<16:16,  2.07s/it] 87%|████████▋ | 3028/3500 [3:04:59<16:14,  2.06s/it] 87%|████████▋ | 3029/3500 [3:05:01<16:12,  2.06s/it] 87%|████████▋ | 3030/3500 [3:05:03<16:09,  2.06s/it] 87%|████████▋ | 3031/3500 [3:05:05<16:07,  2.06s/it] 87%|████████▋ | 3032/3500 [3:05:07<16:05,  2.06s/it] 87%|████████▋ | 3033/3500 [3:05:09<16:03,  2.06s/it] 87%|████████▋ | 3034/3500 [3:05:11<16:01,  2.06s/it] 87%|████████▋ | 3035/3500 [3:05:13<15:58,  2.06s/it] 87%|████████▋ | 3036/3500 [3:05:15<15:56,  2.06s/it] 87%|████████▋ | 3037/3500 [3:05:17<15:54,  2.06s/it] 87%|████████▋ | 3038/3500 [3:05:19<15:52,  2.06s/it] 87%|████████▋ | 3039/3500 [3:05:21<15:50,  2.06s/it] 87%|████████▋ | 3040/3500 [3:05:23<15:48,  2.06s/it] 87%|████████▋ | 3041/3500 [3:05:26<15:46,  2.06s/it] 87%|████████▋ | 3042/3500 [3:05:28<15:43,  2.06s/it] 87%|████████▋ | 3043/3500 [3:05:30<15:41,  2.06s/it] 87%|████████▋ | 3044/3500 [3:05:32<15:39,  2.06s/it] 87%|████████▋ | 3045/3500 [3:05:34<15:37,  2.06s/it] 87%|████████▋ | 3046/3500 [3:05:36<15:35,  2.06s/it] 87%|████████▋ | 3047/3500 [3:05:38<15:33,  2.06s/it] 87%|████████▋ | 3048/3500 [3:05:40<15:31,  2.06s/it] 87%|████████▋ | 3049/3500 [3:05:42<15:29,  2.06s/it] 87%|████████▋ | 3050/3500 [3:05:44<15:27,  2.06s/it] 87%|████████▋ | 3051/3500 [3:05:46<15:25,  2.06s/it][2022-11-17 23:41:36,790][__main__][INFO] - epoch 8: perplexity: 22.20328454829674 train_loss: 0.034344837069511414 eval_loss: 3.1002402305603027
Configuration saved in tuned-model/epoch_8_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_8_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_8_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_8_most_recent/special_tokens_map.json
 87%|████████▋ | 3052/3500 [3:07:10<3:18:48, 26.63s/it] 87%|████████▋ | 3053/3500 [3:07:12<2:23:27, 19.26s/it] 87%|████████▋ | 3054/3500 [3:07:14<1:44:47, 14.10s/it] 87%|████████▋ | 3055/3500 [3:07:16<1:17:46, 10.49s/it] 87%|████████▋ | 3056/3500 [3:07:18<58:54,  7.96s/it]   87%|████████▋ | 3057/3500 [3:07:20<45:42,  6.19s/it] 87%|████████▋ | 3058/3500 [3:07:22<36:28,  4.95s/it] 87%|████████▋ | 3059/3500 [3:07:25<30:01,  4.09s/it] 87%|████████▋ | 3060/3500 [3:07:27<25:30,  3.48s/it] 87%|████████▋ | 3061/3500 [3:07:29<22:20,  3.05s/it] 87%|████████▋ | 3062/3500 [3:07:31<20:06,  2.76s/it] 88%|████████▊ | 3063/3500 [3:07:33<18:33,  2.55s/it] 88%|████████▊ | 3064/3500 [3:07:35<17:27,  2.40s/it] 88%|████████▊ | 3065/3500 [3:07:37<16:40,  2.30s/it] 88%|████████▊ | 3066/3500 [3:07:39<16:07,  2.23s/it] 88%|████████▊ | 3067/3500 [3:07:41<15:43,  2.18s/it] 88%|████████▊ | 3068/3500 [3:07:43<15:26,  2.14s/it] 88%|████████▊ | 3069/3500 [3:07:45<15:13,  2.12s/it] 88%|████████▊ | 3070/3500 [3:07:47<15:04,  2.10s/it] 88%|████████▊ | 3071/3500 [3:07:49<14:57,  2.09s/it] 88%|████████▊ | 3072/3500 [3:07:51<14:51,  2.08s/it] 88%|████████▊ | 3073/3500 [3:07:53<14:46,  2.08s/it] 88%|████████▊ | 3074/3500 [3:07:55<14:42,  2.07s/it] 88%|████████▊ | 3075/3500 [3:07:58<14:39,  2.07s/it] 88%|████████▊ | 3076/3500 [3:08:00<14:36,  2.07s/it] 88%|████████▊ | 3077/3500 [3:08:02<14:34,  2.07s/it] 88%|████████▊ | 3078/3500 [3:08:04<14:31,  2.07s/it] 88%|████████▊ | 3079/3500 [3:08:06<14:29,  2.06s/it] 88%|████████▊ | 3080/3500 [3:08:08<14:27,  2.06s/it] 88%|████████▊ | 3081/3500 [3:08:10<14:24,  2.06s/it] 88%|████████▊ | 3082/3500 [3:08:12<14:23,  2.06s/it] 88%|████████▊ | 3083/3500 [3:08:14<14:20,  2.06s/it] 88%|████████▊ | 3084/3500 [3:08:16<14:18,  2.06s/it] 88%|████████▊ | 3085/3500 [3:08:18<14:16,  2.06s/it] 88%|████████▊ | 3086/3500 [3:08:20<14:14,  2.06s/it] 88%|████████▊ | 3087/3500 [3:08:22<14:12,  2.06s/it] 88%|████████▊ | 3088/3500 [3:08:24<14:10,  2.06s/it] 88%|████████▊ | 3089/3500 [3:08:26<14:08,  2.06s/it] 88%|████████▊ | 3090/3500 [3:08:28<14:06,  2.06s/it] 88%|████████▊ | 3091/3500 [3:08:31<14:03,  2.06s/it] 88%|████████▊ | 3092/3500 [3:08:33<14:01,  2.06s/it] 88%|████████▊ | 3093/3500 [3:08:35<13:59,  2.06s/it] 88%|████████▊ | 3094/3500 [3:08:37<13:57,  2.06s/it] 88%|████████▊ | 3095/3500 [3:08:39<13:55,  2.06s/it] 88%|████████▊ | 3096/3500 [3:08:41<13:52,  2.06s/it] 88%|████████▊ | 3097/3500 [3:08:43<13:51,  2.06s/it] 89%|████████▊ | 3098/3500 [3:08:45<13:49,  2.06s/it] 89%|████████▊ | 3099/3500 [3:08:47<13:46,  2.06s/it] 89%|████████▊ | 3100/3500 [3:08:49<13:44,  2.06s/it] 89%|████████▊ | 3101/3500 [3:08:51<13:42,  2.06s/it][2022-11-17 23:44:41,870][__main__][INFO] - epoch 8: perplexity: 22.30913429803449 train_loss: 0.03450436145067215 eval_loss: 3.1049962043762207
Configuration saved in tuned-model/epoch_8_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_8_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_8_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_8_most_recent/special_tokens_map.json
 89%|████████▊ | 3102/3500 [3:10:16<2:59:21, 27.04s/it] 89%|████████▊ | 3103/3500 [3:10:19<2:09:19, 19.55s/it] 89%|████████▊ | 3104/3500 [3:10:21<1:34:22, 14.30s/it] 89%|████████▊ | 3105/3500 [3:10:23<1:09:58, 10.63s/it] 89%|████████▊ | 3106/3500 [3:10:25<52:55,  8.06s/it]   89%|████████▉ | 3107/3500 [3:10:27<41:00,  6.26s/it] 89%|████████▉ | 3108/3500 [3:10:29<32:40,  5.00s/it] 89%|████████▉ | 3109/3500 [3:10:31<26:50,  4.12s/it] 89%|████████▉ | 3110/3500 [3:10:33<22:45,  3.50s/it] 89%|████████▉ | 3111/3500 [3:10:35<19:54,  3.07s/it] 89%|████████▉ | 3112/3500 [3:10:37<17:53,  2.77s/it] 89%|████████▉ | 3113/3500 [3:10:39<16:28,  2.56s/it] 89%|████████▉ | 3114/3500 [3:10:41<15:29,  2.41s/it] 89%|████████▉ | 3115/3500 [3:10:43<14:46,  2.30s/it] 89%|████████▉ | 3116/3500 [3:10:45<14:16,  2.23s/it] 89%|████████▉ | 3117/3500 [3:10:47<13:55,  2.18s/it] 89%|████████▉ | 3118/3500 [3:10:49<13:39,  2.14s/it] 89%|████████▉ | 3119/3500 [3:10:52<13:27,  2.12s/it] 89%|████████▉ | 3120/3500 [3:10:54<13:18,  2.10s/it] 89%|████████▉ | 3121/3500 [3:10:56<13:12,  2.09s/it] 89%|████████▉ | 3122/3500 [3:10:58<13:07,  2.08s/it] 89%|████████▉ | 3123/3500 [3:11:00<13:03,  2.08s/it] 89%|████████▉ | 3124/3500 [3:11:02<12:59,  2.07s/it] 89%|████████▉ | 3125/3500 [3:11:04<12:55,  2.07s/it] 89%|████████▉ | 3126/3500 [3:11:06<12:52,  2.07s/it] 89%|████████▉ | 3127/3500 [3:11:08<12:50,  2.07s/it] 89%|████████▉ | 3128/3500 [3:11:10<12:47,  2.06s/it] 89%|████████▉ | 3129/3500 [3:11:12<12:45,  2.06s/it] 89%|████████▉ | 3130/3500 [3:11:14<12:43,  2.06s/it] 89%|████████▉ | 3131/3500 [3:11:16<12:41,  2.06s/it] 89%|████████▉ | 3132/3500 [3:11:18<12:39,  2.06s/it] 90%|████████▉ | 3133/3500 [3:11:20<12:36,  2.06s/it] 90%|████████▉ | 3134/3500 [3:11:22<12:34,  2.06s/it] 90%|████████▉ | 3135/3500 [3:11:25<12:33,  2.06s/it] 90%|████████▉ | 3136/3500 [3:11:27<12:31,  2.06s/it] 90%|████████▉ | 3137/3500 [3:11:29<12:29,  2.06s/it] 90%|████████▉ | 3138/3500 [3:11:31<12:27,  2.06s/it] 90%|████████▉ | 3139/3500 [3:11:33<12:25,  2.06s/it] 90%|████████▉ | 3140/3500 [3:11:35<12:22,  2.06s/it] 90%|████████▉ | 3141/3500 [3:11:37<12:20,  2.06s/it] 90%|████████▉ | 3142/3500 [3:11:39<12:18,  2.06s/it] 90%|████████▉ | 3143/3500 [3:11:41<12:16,  2.06s/it] 90%|████████▉ | 3144/3500 [3:11:43<12:14,  2.06s/it] 90%|████████▉ | 3145/3500 [3:11:45<12:12,  2.06s/it] 90%|████████▉ | 3146/3500 [3:11:47<12:10,  2.06s/it] 90%|████████▉ | 3147/3500 [3:11:49<12:07,  2.06s/it] 90%|████████▉ | 3148/3500 [3:11:51<12:05,  2.06s/it] 90%|████████▉ | 3149/3500 [3:11:53<12:03,  2.06s/it] 90%|█████████ | 3150/3500 [3:11:55<12:01,  2.06s/it][2022-11-17 23:46:57,793][__main__][INFO] - done epoch 8
 90%|█████████ | 3151/3500 [3:11:58<12:06,  2.08s/it][2022-11-17 23:47:48,627][__main__][INFO] - epoch 9: perplexity: 22.37308997464469 train_loss: 0.03574206307530403 eval_loss: 3.107858896255493
Configuration saved in tuned-model/epoch_9_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_9_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_9_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_9_most_recent/special_tokens_map.json
 90%|█████████ | 3152/3500 [3:13:03<2:02:58, 21.20s/it] 90%|█████████ | 3153/3500 [3:13:05<1:29:24, 15.46s/it] 90%|█████████ | 3154/3500 [3:13:08<1:05:58, 11.44s/it] 90%|█████████ | 3155/3500 [3:13:10<49:36,  8.63s/it]   90%|█████████ | 3156/3500 [3:13:12<38:10,  6.66s/it] 90%|█████████ | 3157/3500 [3:13:14<30:11,  5.28s/it] 90%|█████████ | 3158/3500 [3:13:16<24:35,  4.32s/it] 90%|█████████ | 3159/3500 [3:13:18<20:41,  3.64s/it] 90%|█████████ | 3160/3500 [3:13:20<17:56,  3.17s/it] 90%|█████████ | 3161/3500 [3:13:22<16:01,  2.84s/it] 90%|█████████ | 3162/3500 [3:13:24<14:40,  2.60s/it] 90%|█████████ | 3163/3500 [3:13:26<13:43,  2.44s/it] 90%|█████████ | 3164/3500 [3:13:28<13:02,  2.33s/it] 90%|█████████ | 3165/3500 [3:13:30<12:33,  2.25s/it] 90%|█████████ | 3166/3500 [3:13:32<12:12,  2.19s/it] 90%|█████████ | 3167/3500 [3:13:34<11:57,  2.15s/it] 91%|█████████ | 3168/3500 [3:13:36<11:45,  2.13s/it] 91%|█████████ | 3169/3500 [3:13:38<11:37,  2.11s/it] 91%|█████████ | 3170/3500 [3:13:41<11:30,  2.09s/it] 91%|█████████ | 3171/3500 [3:13:43<11:25,  2.08s/it] 91%|█████████ | 3172/3500 [3:13:45<11:21,  2.08s/it] 91%|█████████ | 3173/3500 [3:13:47<11:18,  2.07s/it] 91%|█████████ | 3174/3500 [3:13:49<11:14,  2.07s/it] 91%|█████████ | 3175/3500 [3:13:51<11:12,  2.07s/it] 91%|█████████ | 3176/3500 [3:13:53<11:09,  2.07s/it] 91%|█████████ | 3177/3500 [3:13:55<11:07,  2.07s/it] 91%|█████████ | 3178/3500 [3:13:57<11:04,  2.06s/it] 91%|█████████ | 3179/3500 [3:13:59<11:02,  2.06s/it] 91%|█████████ | 3180/3500 [3:14:01<11:00,  2.06s/it] 91%|█████████ | 3181/3500 [3:14:03<10:58,  2.06s/it] 91%|█████████ | 3182/3500 [3:14:05<10:56,  2.06s/it] 91%|█████████ | 3183/3500 [3:14:07<10:53,  2.06s/it] 91%|█████████ | 3184/3500 [3:14:09<10:51,  2.06s/it] 91%|█████████ | 3185/3500 [3:14:11<10:49,  2.06s/it] 91%|█████████ | 3186/3500 [3:14:14<10:47,  2.06s/it] 91%|█████████ | 3187/3500 [3:14:16<10:45,  2.06s/it] 91%|█████████ | 3188/3500 [3:14:18<10:43,  2.06s/it] 91%|█████████ | 3189/3500 [3:14:20<10:41,  2.06s/it] 91%|█████████ | 3190/3500 [3:14:22<10:39,  2.06s/it] 91%|█████████ | 3191/3500 [3:14:24<10:37,  2.06s/it] 91%|█████████ | 3192/3500 [3:14:26<10:35,  2.06s/it] 91%|█████████ | 3193/3500 [3:14:28<10:32,  2.06s/it] 91%|█████████▏| 3194/3500 [3:14:30<10:30,  2.06s/it] 91%|█████████▏| 3195/3500 [3:14:32<10:28,  2.06s/it] 91%|█████████▏| 3196/3500 [3:14:34<10:26,  2.06s/it] 91%|█████████▏| 3197/3500 [3:14:36<10:24,  2.06s/it] 91%|█████████▏| 3198/3500 [3:14:38<10:22,  2.06s/it] 91%|█████████▏| 3199/3500 [3:14:40<10:20,  2.06s/it] 91%|█████████▏| 3200/3500 [3:14:42<10:18,  2.06s/it] 91%|█████████▏| 3201/3500 [3:14:44<10:16,  2.06s/it][2022-11-17 23:50:35,174][__main__][INFO] - epoch 9: perplexity: 23.621623495568134 train_loss: 0.02775757573544979 eval_loss: 3.1621625423431396
Configuration saved in tuned-model/epoch_9_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_9_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_9_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_9_most_recent/special_tokens_map.json
 91%|█████████▏| 3202/3500 [3:16:08<2:11:12, 26.42s/it] 92%|█████████▏| 3203/3500 [3:16:10<1:34:35, 19.11s/it] 92%|█████████▏| 3204/3500 [3:16:12<1:09:02, 14.00s/it] 92%|█████████▏| 3205/3500 [3:16:14<51:12, 10.42s/it]   92%|█████████▏| 3206/3500 [3:16:16<38:45,  7.91s/it] 92%|█████████▏| 3207/3500 [3:16:18<30:03,  6.16s/it] 92%|█████████▏| 3208/3500 [3:16:20<23:58,  4.93s/it] 92%|█████████▏| 3209/3500 [3:16:22<19:43,  4.07s/it] 92%|█████████▏| 3210/3500 [3:16:24<16:45,  3.47s/it] 92%|█████████▏| 3211/3500 [3:16:26<14:40,  3.05s/it] 92%|█████████▏| 3212/3500 [3:16:28<13:12,  2.75s/it] 92%|█████████▏| 3213/3500 [3:16:30<12:09,  2.54s/it] 92%|█████████▏| 3214/3500 [3:16:32<11:25,  2.40s/it] 92%|█████████▏| 3215/3500 [3:16:35<10:54,  2.30s/it] 92%|█████████▏| 3216/3500 [3:16:37<10:32,  2.23s/it] 92%|█████████▏| 3217/3500 [3:16:39<10:16,  2.18s/it] 92%|█████████▏| 3218/3500 [3:16:41<10:04,  2.14s/it] 92%|█████████▏| 3219/3500 [3:16:43<09:55,  2.12s/it] 92%|█████████▏| 3220/3500 [3:16:45<09:48,  2.10s/it] 92%|█████████▏| 3221/3500 [3:16:47<09:43,  2.09s/it] 92%|█████████▏| 3222/3500 [3:16:49<09:38,  2.08s/it] 92%|█████████▏| 3223/3500 [3:16:51<09:35,  2.08s/it] 92%|█████████▏| 3224/3500 [3:16:53<09:31,  2.07s/it] 92%|█████████▏| 3225/3500 [3:16:55<09:28,  2.07s/it] 92%|█████████▏| 3226/3500 [3:16:57<09:26,  2.07s/it] 92%|█████████▏| 3227/3500 [3:16:59<09:23,  2.07s/it] 92%|█████████▏| 3228/3500 [3:17:01<09:21,  2.06s/it] 92%|█████████▏| 3229/3500 [3:17:03<09:19,  2.06s/it] 92%|█████████▏| 3230/3500 [3:17:05<09:17,  2.06s/it] 92%|█████████▏| 3231/3500 [3:17:08<09:14,  2.06s/it] 92%|█████████▏| 3232/3500 [3:17:10<09:12,  2.06s/it] 92%|█████████▏| 3233/3500 [3:17:12<09:10,  2.06s/it] 92%|█████████▏| 3234/3500 [3:17:14<09:08,  2.06s/it] 92%|█████████▏| 3235/3500 [3:17:16<09:06,  2.06s/it] 92%|█████████▏| 3236/3500 [3:17:18<09:04,  2.06s/it] 92%|█████████▏| 3237/3500 [3:17:20<09:02,  2.06s/it] 93%|█████████▎| 3238/3500 [3:17:22<09:00,  2.06s/it] 93%|█████████▎| 3239/3500 [3:17:24<08:58,  2.06s/it] 93%|█████████▎| 3240/3500 [3:17:26<08:56,  2.06s/it] 93%|█████████▎| 3241/3500 [3:17:28<08:54,  2.06s/it] 93%|█████████▎| 3242/3500 [3:17:30<08:51,  2.06s/it] 93%|█████████▎| 3243/3500 [3:17:32<08:49,  2.06s/it] 93%|█████████▎| 3244/3500 [3:17:34<08:47,  2.06s/it] 93%|█████████▎| 3245/3500 [3:17:36<08:45,  2.06s/it] 93%|█████████▎| 3246/3500 [3:17:38<08:43,  2.06s/it] 93%|█████████▎| 3247/3500 [3:17:41<08:41,  2.06s/it] 93%|█████████▎| 3248/3500 [3:17:43<08:39,  2.06s/it] 93%|█████████▎| 3249/3500 [3:17:45<08:37,  2.06s/it] 93%|█████████▎| 3250/3500 [3:17:47<08:35,  2.06s/it] 93%|█████████▎| 3251/3500 [3:17:49<08:33,  2.06s/it][2022-11-17 23:53:39,404][__main__][INFO] - epoch 9: perplexity: 23.699730718534386 train_loss: 0.02698400989174843 eval_loss: 3.16546368598938
Configuration saved in tuned-model/epoch_9_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_9_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_9_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_9_most_recent/special_tokens_map.json
 93%|█████████▎| 3252/3500 [3:19:10<1:46:53, 25.86s/it] 93%|█████████▎| 3253/3500 [3:19:12<1:17:04, 18.72s/it] 93%|█████████▎| 3254/3500 [3:19:14<56:15, 13.72s/it]   93%|█████████▎| 3255/3500 [3:19:16<41:44, 10.22s/it] 93%|█████████▎| 3256/3500 [3:19:18<31:37,  7.77s/it] 93%|█████████▎| 3257/3500 [3:19:20<24:32,  6.06s/it] 93%|█████████▎| 3258/3500 [3:19:23<19:36,  4.86s/it] 93%|█████████▎| 3259/3500 [3:19:25<16:09,  4.02s/it] 93%|█████████▎| 3260/3500 [3:19:27<13:43,  3.43s/it] 93%|█████████▎| 3261/3500 [3:19:29<12:02,  3.02s/it] 93%|█████████▎| 3262/3500 [3:19:31<10:50,  2.73s/it] 93%|█████████▎| 3263/3500 [3:19:33<10:00,  2.53s/it] 93%|█████████▎| 3264/3500 [3:19:35<09:24,  2.39s/it] 93%|█████████▎| 3265/3500 [3:19:37<08:58,  2.29s/it] 93%|█████████▎| 3266/3500 [3:19:39<08:40,  2.22s/it] 93%|█████████▎| 3267/3500 [3:19:41<08:26,  2.17s/it] 93%|█████████▎| 3268/3500 [3:19:43<08:16,  2.14s/it] 93%|█████████▎| 3269/3500 [3:19:45<08:09,  2.12s/it] 93%|█████████▎| 3270/3500 [3:19:47<08:03,  2.10s/it] 93%|█████████▎| 3271/3500 [3:19:49<07:58,  2.09s/it] 93%|█████████▎| 3272/3500 [3:19:51<07:54,  2.08s/it] 94%|█████████▎| 3273/3500 [3:19:53<07:50,  2.07s/it] 94%|█████████▎| 3274/3500 [3:19:56<07:48,  2.07s/it] 94%|█████████▎| 3275/3500 [3:19:58<07:45,  2.07s/it] 94%|█████████▎| 3276/3500 [3:20:00<07:43,  2.07s/it] 94%|█████████▎| 3277/3500 [3:20:02<07:40,  2.07s/it] 94%|█████████▎| 3278/3500 [3:20:04<07:38,  2.07s/it] 94%|█████████▎| 3279/3500 [3:20:06<07:36,  2.06s/it] 94%|█████████▎| 3280/3500 [3:20:08<07:34,  2.06s/it] 94%|█████████▎| 3281/3500 [3:20:10<07:31,  2.06s/it] 94%|█████████▍| 3282/3500 [3:20:12<07:29,  2.06s/it] 94%|█████████▍| 3283/3500 [3:20:14<07:27,  2.06s/it] 94%|█████████▍| 3284/3500 [3:20:16<07:25,  2.06s/it] 94%|█████████▍| 3285/3500 [3:20:18<07:23,  2.06s/it] 94%|█████████▍| 3286/3500 [3:20:20<07:21,  2.06s/it] 94%|█████████▍| 3287/3500 [3:20:22<07:19,  2.06s/it] 94%|█████████▍| 3288/3500 [3:20:24<07:17,  2.06s/it] 94%|█████████▍| 3289/3500 [3:20:26<07:14,  2.06s/it] 94%|█████████▍| 3290/3500 [3:20:29<07:12,  2.06s/it] 94%|█████████▍| 3291/3500 [3:20:31<07:10,  2.06s/it] 94%|█████████▍| 3292/3500 [3:20:33<07:08,  2.06s/it] 94%|█████████▍| 3293/3500 [3:20:35<07:06,  2.06s/it] 94%|█████████▍| 3294/3500 [3:20:37<07:04,  2.06s/it] 94%|█████████▍| 3295/3500 [3:20:39<07:02,  2.06s/it] 94%|█████████▍| 3296/3500 [3:20:41<07:00,  2.06s/it] 94%|█████████▍| 3297/3500 [3:20:43<06:58,  2.06s/it] 94%|█████████▍| 3298/3500 [3:20:45<06:56,  2.06s/it] 94%|█████████▍| 3299/3500 [3:20:47<06:54,  2.06s/it] 94%|█████████▍| 3300/3500 [3:20:49<06:52,  2.06s/it] 94%|█████████▍| 3301/3500 [3:20:51<06:50,  2.06s/it][2022-11-17 23:56:41,837][__main__][INFO] - epoch 9: perplexity: 23.59703644989844 train_loss: 0.02822788804769516 eval_loss: 3.161121129989624
Configuration saved in tuned-model/epoch_9_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_9_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_9_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_9_most_recent/special_tokens_map.json
 94%|█████████▍| 3302/3500 [3:22:14<1:27:06, 26.40s/it] 94%|█████████▍| 3303/3500 [3:22:16<1:02:41, 19.10s/it] 94%|█████████▍| 3304/3500 [3:22:18<45:41, 13.99s/it]   94%|█████████▍| 3305/3500 [3:22:21<33:49, 10.41s/it] 94%|█████████▍| 3306/3500 [3:22:23<25:33,  7.90s/it] 94%|█████████▍| 3307/3500 [3:22:25<19:47,  6.15s/it] 95%|█████████▍| 3308/3500 [3:22:27<15:45,  4.93s/it] 95%|█████████▍| 3309/3500 [3:22:29<12:56,  4.07s/it] 95%|█████████▍| 3310/3500 [3:22:31<10:58,  3.47s/it] 95%|█████████▍| 3311/3500 [3:22:33<09:35,  3.04s/it] 95%|█████████▍| 3312/3500 [3:22:35<08:36,  2.75s/it] 95%|█████████▍| 3313/3500 [3:22:37<07:55,  2.54s/it] 95%|█████████▍| 3314/3500 [3:22:39<07:26,  2.40s/it] 95%|█████████▍| 3315/3500 [3:22:41<07:05,  2.30s/it] 95%|█████████▍| 3316/3500 [3:22:43<06:49,  2.23s/it] 95%|█████████▍| 3317/3500 [3:22:45<06:38,  2.18s/it] 95%|█████████▍| 3318/3500 [3:22:47<06:30,  2.14s/it] 95%|█████████▍| 3319/3500 [3:22:49<06:23,  2.12s/it] 95%|█████████▍| 3320/3500 [3:22:51<06:18,  2.10s/it] 95%|█████████▍| 3321/3500 [3:22:54<06:13,  2.09s/it] 95%|█████████▍| 3322/3500 [3:22:56<06:10,  2.08s/it] 95%|█████████▍| 3323/3500 [3:22:58<06:07,  2.07s/it] 95%|█████████▍| 3324/3500 [3:23:00<06:04,  2.07s/it] 95%|█████████▌| 3325/3500 [3:23:02<06:01,  2.07s/it] 95%|█████████▌| 3326/3500 [3:23:04<05:59,  2.07s/it] 95%|█████████▌| 3327/3500 [3:23:06<05:57,  2.06s/it] 95%|█████████▌| 3328/3500 [3:23:08<05:54,  2.06s/it] 95%|█████████▌| 3329/3500 [3:23:10<05:52,  2.06s/it] 95%|█████████▌| 3330/3500 [3:23:12<05:50,  2.06s/it] 95%|█████████▌| 3331/3500 [3:23:14<05:48,  2.06s/it] 95%|█████████▌| 3332/3500 [3:23:16<05:46,  2.06s/it] 95%|█████████▌| 3333/3500 [3:23:18<05:44,  2.06s/it] 95%|█████████▌| 3334/3500 [3:23:20<05:42,  2.06s/it] 95%|█████████▌| 3335/3500 [3:23:22<05:40,  2.06s/it] 95%|█████████▌| 3336/3500 [3:23:24<05:38,  2.06s/it] 95%|█████████▌| 3337/3500 [3:23:27<05:36,  2.06s/it] 95%|█████████▌| 3338/3500 [3:23:29<05:33,  2.06s/it] 95%|█████████▌| 3339/3500 [3:23:31<05:31,  2.06s/it] 95%|█████████▌| 3340/3500 [3:23:33<05:29,  2.06s/it] 95%|█████████▌| 3341/3500 [3:23:35<05:27,  2.06s/it] 95%|█████████▌| 3342/3500 [3:23:37<05:25,  2.06s/it] 96%|█████████▌| 3343/3500 [3:23:39<05:23,  2.06s/it] 96%|█████████▌| 3344/3500 [3:23:41<05:21,  2.06s/it] 96%|█████████▌| 3345/3500 [3:23:43<05:19,  2.06s/it] 96%|█████████▌| 3346/3500 [3:23:45<05:17,  2.06s/it] 96%|█████████▌| 3347/3500 [3:23:47<05:15,  2.06s/it] 96%|█████████▌| 3348/3500 [3:23:49<05:13,  2.06s/it] 96%|█████████▌| 3349/3500 [3:23:51<05:11,  2.06s/it] 96%|█████████▌| 3350/3500 [3:23:53<05:09,  2.06s/it] 96%|█████████▌| 3351/3500 [3:23:55<05:07,  2.06s/it][2022-11-17 23:59:46,056][__main__][INFO] - epoch 9: perplexity: 24.01179092312796 train_loss: 0.028141016140580177 eval_loss: 3.1785449981689453
Configuration saved in tuned-model/epoch_9_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_9_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_9_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_9_most_recent/special_tokens_map.json
 96%|█████████▌| 3352/3500 [3:25:14<1:01:55, 25.10s/it] 96%|█████████▌| 3353/3500 [3:25:16<44:33, 18.19s/it]   96%|█████████▌| 3354/3500 [3:25:18<32:29, 13.35s/it] 96%|█████████▌| 3355/3500 [3:25:20<24:04,  9.97s/it] 96%|█████████▌| 3356/3500 [3:25:23<18:13,  7.59s/it] 96%|█████████▌| 3357/3500 [3:25:25<14:08,  5.93s/it] 96%|█████████▌| 3358/3500 [3:25:27<11:17,  4.77s/it] 96%|█████████▌| 3359/3500 [3:25:29<09:18,  3.96s/it] 96%|█████████▌| 3360/3500 [3:25:31<07:54,  3.39s/it] 96%|█████████▌| 3361/3500 [3:25:33<06:56,  2.99s/it] 96%|█████████▌| 3362/3500 [3:25:35<06:14,  2.71s/it] 96%|█████████▌| 3363/3500 [3:25:37<05:44,  2.52s/it] 96%|█████████▌| 3364/3500 [3:25:39<05:23,  2.38s/it] 96%|█████████▌| 3365/3500 [3:25:41<05:08,  2.28s/it] 96%|█████████▌| 3366/3500 [3:25:43<04:57,  2.22s/it] 96%|█████████▌| 3367/3500 [3:25:45<04:48,  2.17s/it] 96%|█████████▌| 3368/3500 [3:25:47<04:42,  2.14s/it] 96%|█████████▋| 3369/3500 [3:25:49<04:37,  2.12s/it] 96%|█████████▋| 3370/3500 [3:25:51<04:32,  2.10s/it] 96%|█████████▋| 3371/3500 [3:25:53<04:29,  2.09s/it] 96%|█████████▋| 3372/3500 [3:25:56<04:26,  2.08s/it] 96%|█████████▋| 3373/3500 [3:25:58<04:23,  2.07s/it] 96%|█████████▋| 3374/3500 [3:26:00<04:20,  2.07s/it] 96%|█████████▋| 3375/3500 [3:26:02<04:18,  2.07s/it] 96%|█████████▋| 3376/3500 [3:26:04<04:16,  2.07s/it] 96%|█████████▋| 3377/3500 [3:26:06<04:13,  2.06s/it] 97%|█████████▋| 3378/3500 [3:26:08<04:11,  2.06s/it] 97%|█████████▋| 3379/3500 [3:26:10<04:09,  2.06s/it] 97%|█████████▋| 3380/3500 [3:26:12<04:07,  2.06s/it] 97%|█████████▋| 3381/3500 [3:26:14<04:05,  2.06s/it] 97%|█████████▋| 3382/3500 [3:26:16<04:03,  2.06s/it] 97%|█████████▋| 3383/3500 [3:26:18<04:01,  2.06s/it] 97%|█████████▋| 3384/3500 [3:26:20<03:59,  2.06s/it] 97%|█████████▋| 3385/3500 [3:26:22<03:57,  2.06s/it] 97%|█████████▋| 3386/3500 [3:26:24<03:55,  2.06s/it] 97%|█████████▋| 3387/3500 [3:26:26<03:52,  2.06s/it] 97%|█████████▋| 3388/3500 [3:26:28<03:50,  2.06s/it] 97%|█████████▋| 3389/3500 [3:26:31<03:48,  2.06s/it] 97%|█████████▋| 3390/3500 [3:26:33<03:46,  2.06s/it] 97%|█████████▋| 3391/3500 [3:26:35<03:44,  2.06s/it] 97%|█████████▋| 3392/3500 [3:26:37<03:42,  2.06s/it] 97%|█████████▋| 3393/3500 [3:26:39<03:40,  2.06s/it] 97%|█████████▋| 3394/3500 [3:26:41<03:38,  2.06s/it] 97%|█████████▋| 3395/3500 [3:26:43<03:36,  2.06s/it] 97%|█████████▋| 3396/3500 [3:26:45<03:34,  2.06s/it] 97%|█████████▋| 3397/3500 [3:26:47<03:32,  2.06s/it] 97%|█████████▋| 3398/3500 [3:26:49<03:30,  2.06s/it] 97%|█████████▋| 3399/3500 [3:26:51<03:28,  2.06s/it] 97%|█████████▋| 3400/3500 [3:26:53<03:26,  2.06s/it] 97%|█████████▋| 3401/3500 [3:26:55<03:24,  2.06s/it][2022-11-18 00:02:45,960][__main__][INFO] - epoch 9: perplexity: 23.771741954059234 train_loss: 0.028038207441568375 eval_loss: 3.1684975624084473
Configuration saved in tuned-model/epoch_9_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_9_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_9_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_9_most_recent/special_tokens_map.json
 97%|█████████▋| 3402/3500 [3:28:16<41:54, 25.66s/it] 97%|█████████▋| 3403/3500 [3:28:18<30:02, 18.58s/it] 97%|█████████▋| 3404/3500 [3:28:20<21:47, 13.62s/it] 97%|█████████▋| 3405/3500 [3:28:22<16:04, 10.16s/it] 97%|█████████▋| 3406/3500 [3:28:24<12:06,  7.73s/it] 97%|█████████▋| 3407/3500 [3:28:26<09:20,  6.03s/it] 97%|█████████▋| 3408/3500 [3:28:28<07:25,  4.84s/it] 97%|█████████▋| 3409/3500 [3:28:30<06:04,  4.01s/it] 97%|█████████▋| 3410/3500 [3:28:33<05:08,  3.42s/it] 97%|█████████▋| 3411/3500 [3:28:35<04:28,  3.02s/it] 97%|█████████▋| 3412/3500 [3:28:37<04:00,  2.73s/it] 98%|█████████▊| 3413/3500 [3:28:39<03:40,  2.53s/it] 98%|█████████▊| 3414/3500 [3:28:41<03:25,  2.39s/it] 98%|█████████▊| 3415/3500 [3:28:43<03:14,  2.29s/it] 98%|█████████▊| 3416/3500 [3:28:45<03:06,  2.22s/it] 98%|█████████▊| 3417/3500 [3:28:47<03:00,  2.17s/it] 98%|█████████▊| 3418/3500 [3:28:49<02:55,  2.14s/it] 98%|█████████▊| 3419/3500 [3:28:51<02:51,  2.12s/it] 98%|█████████▊| 3420/3500 [3:28:53<02:48,  2.10s/it] 98%|█████████▊| 3421/3500 [3:28:55<02:45,  2.09s/it] 98%|█████████▊| 3422/3500 [3:28:57<02:42,  2.08s/it] 98%|█████████▊| 3423/3500 [3:28:59<02:39,  2.08s/it] 98%|█████████▊| 3424/3500 [3:29:01<02:37,  2.07s/it] 98%|█████████▊| 3425/3500 [3:29:03<02:35,  2.07s/it] 98%|█████████▊| 3426/3500 [3:29:06<02:32,  2.07s/it] 98%|█████████▊| 3427/3500 [3:29:08<02:30,  2.07s/it] 98%|█████████▊| 3428/3500 [3:29:10<02:28,  2.06s/it] 98%|█████████▊| 3429/3500 [3:29:12<02:26,  2.06s/it] 98%|█████████▊| 3430/3500 [3:29:14<02:24,  2.06s/it] 98%|█████████▊| 3431/3500 [3:29:16<02:22,  2.06s/it] 98%|█████████▊| 3432/3500 [3:29:18<02:20,  2.06s/it] 98%|█████████▊| 3433/3500 [3:29:20<02:18,  2.06s/it] 98%|█████████▊| 3434/3500 [3:29:22<02:16,  2.06s/it] 98%|█████████▊| 3435/3500 [3:29:24<02:14,  2.06s/it] 98%|█████████▊| 3436/3500 [3:29:26<02:11,  2.06s/it] 98%|█████████▊| 3437/3500 [3:29:28<02:09,  2.06s/it] 98%|█████████▊| 3438/3500 [3:29:30<02:07,  2.06s/it] 98%|█████████▊| 3439/3500 [3:29:32<02:05,  2.06s/it] 98%|█████████▊| 3440/3500 [3:29:34<02:03,  2.06s/it] 98%|█████████▊| 3441/3500 [3:29:36<02:01,  2.06s/it] 98%|█████████▊| 3442/3500 [3:29:38<01:59,  2.06s/it] 98%|█████████▊| 3443/3500 [3:29:41<01:57,  2.06s/it] 98%|█████████▊| 3444/3500 [3:29:43<01:55,  2.06s/it] 98%|█████████▊| 3445/3500 [3:29:45<01:53,  2.06s/it] 98%|█████████▊| 3446/3500 [3:29:47<01:51,  2.06s/it] 98%|█████████▊| 3447/3500 [3:29:49<01:49,  2.06s/it] 99%|█████████▊| 3448/3500 [3:29:51<01:47,  2.06s/it] 99%|█████████▊| 3449/3500 [3:29:53<01:45,  2.06s/it] 99%|█████████▊| 3450/3500 [3:29:55<01:43,  2.06s/it] 99%|█████████▊| 3451/3500 [3:29:57<01:40,  2.06s/it][2022-11-18 00:05:47,706][__main__][INFO] - epoch 9: perplexity: 24.219852577327913 train_loss: 0.028138523921370506 eval_loss: 3.1871726512908936
Configuration saved in tuned-model/epoch_9_most_recent/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/epoch_9_most_recent/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/epoch_9_most_recent/tokenizer_config.json
Special tokens file saved in tuned-model/epoch_9_most_recent/special_tokens_map.json
 99%|█████████▊| 3452/3500 [3:31:21<21:14, 26.56s/it] 99%|█████████▊| 3453/3500 [3:31:23<15:02, 19.21s/it] 99%|█████████▊| 3454/3500 [3:31:25<10:46, 14.06s/it] 99%|█████████▊| 3455/3500 [3:31:27<07:50, 10.46s/it] 99%|█████████▊| 3456/3500 [3:31:29<05:49,  7.94s/it] 99%|█████████▉| 3457/3500 [3:31:31<04:25,  6.18s/it] 99%|█████████▉| 3458/3500 [3:31:33<03:27,  4.94s/it] 99%|█████████▉| 3459/3500 [3:31:35<02:47,  4.08s/it] 99%|█████████▉| 3460/3500 [3:31:37<02:18,  3.47s/it] 99%|█████████▉| 3461/3500 [3:31:39<01:58,  3.05s/it] 99%|█████████▉| 3462/3500 [3:31:41<01:44,  2.75s/it] 99%|█████████▉| 3463/3500 [3:31:43<01:34,  2.55s/it] 99%|█████████▉| 3464/3500 [3:31:46<01:26,  2.40s/it] 99%|█████████▉| 3465/3500 [3:31:48<01:20,  2.30s/it] 99%|█████████▉| 3466/3500 [3:31:50<01:15,  2.23s/it] 99%|█████████▉| 3467/3500 [3:31:52<01:11,  2.18s/it] 99%|█████████▉| 3468/3500 [3:31:54<01:08,  2.14s/it] 99%|█████████▉| 3469/3500 [3:31:56<01:05,  2.12s/it] 99%|█████████▉| 3470/3500 [3:31:58<01:03,  2.10s/it] 99%|█████████▉| 3471/3500 [3:32:00<01:00,  2.09s/it] 99%|█████████▉| 3472/3500 [3:32:02<00:58,  2.08s/it] 99%|█████████▉| 3473/3500 [3:32:04<00:56,  2.08s/it] 99%|█████████▉| 3474/3500 [3:32:06<00:53,  2.07s/it] 99%|█████████▉| 3475/3500 [3:32:08<00:51,  2.07s/it] 99%|█████████▉| 3476/3500 [3:32:10<00:49,  2.07s/it] 99%|█████████▉| 3477/3500 [3:32:12<00:47,  2.06s/it] 99%|█████████▉| 3478/3500 [3:32:14<00:45,  2.06s/it] 99%|█████████▉| 3479/3500 [3:32:16<00:43,  2.06s/it] 99%|█████████▉| 3480/3500 [3:32:18<00:41,  2.06s/it] 99%|█████████▉| 3481/3500 [3:32:21<00:39,  2.06s/it] 99%|█████████▉| 3482/3500 [3:32:23<00:37,  2.06s/it]100%|█████████▉| 3483/3500 [3:32:25<00:35,  2.06s/it]100%|█████████▉| 3484/3500 [3:32:27<00:32,  2.06s/it]100%|█████████▉| 3485/3500 [3:32:29<00:30,  2.06s/it]100%|█████████▉| 3486/3500 [3:32:31<00:28,  2.06s/it]100%|█████████▉| 3487/3500 [3:32:33<00:26,  2.06s/it]100%|█████████▉| 3488/3500 [3:32:35<00:24,  2.06s/it]100%|█████████▉| 3489/3500 [3:32:37<00:22,  2.06s/it]100%|█████████▉| 3490/3500 [3:32:39<00:20,  2.06s/it]100%|█████████▉| 3491/3500 [3:32:41<00:18,  2.06s/it]100%|█████████▉| 3492/3500 [3:32:43<00:16,  2.06s/it]100%|█████████▉| 3493/3500 [3:32:45<00:14,  2.06s/it]100%|█████████▉| 3494/3500 [3:32:47<00:12,  2.06s/it]100%|█████████▉| 3495/3500 [3:32:49<00:10,  2.06s/it]100%|█████████▉| 3496/3500 [3:32:51<00:08,  2.06s/it]100%|█████████▉| 3497/3500 [3:32:54<00:06,  2.06s/it]100%|█████████▉| 3498/3500 [3:32:56<00:04,  2.06s/it]100%|█████████▉| 3499/3500 [3:32:58<00:02,  2.06s/it]100%|██████████| 3500/3500 [3:33:00<00:00,  2.06s/it][2022-11-18 00:08:02,054][__main__][INFO] - done epoch 9
Configuration saved in tuned-model/config.json
The model is bigger than the maximum size per checkpoint (10GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at tuned-model/pytorch_model.bin.index.json.
tokenizer config file saved in tuned-model/tokenizer_config.json
Special tokens file saved in tuned-model/special_tokens_map.json
100%|██████████| 3500/3500 [3:33:15<00:00,  3.66s/it]
wandb: Waiting for W&B process to finish... (success).
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.626 MB uploaded (0.000 MB deduped)wandb: - 0.245 MB of 0.626 MB uploaded (0.000 MB deduped)wandb: \ 0.626 MB of 0.626 MB uploaded (0.000 MB deduped)wandb: | 0.626 MB of 0.626 MB uploaded (0.000 MB deduped)wandb: / 0.626 MB of 0.626 MB uploaded (0.000 MB deduped)wandb: - 0.626 MB of 0.626 MB uploaded (0.000 MB deduped)wandb: \ 0.626 MB of 0.626 MB uploaded (0.000 MB deduped)wandb: | 0.626 MB of 0.626 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:      epoch ▁▂▃▃▄▅▆▆▇█
wandb:  eval_loss ▁▁▂▃▄▅▆▇██
wandb: perplexity ▁▁▁▂▃▄▅▆▇█
wandb:       step ▁▂▃▃▄▅▆▆▇█
wandb: train_loss █▇▅▄▃▂▁▁▁▁
wandb: 
wandb: Run summary:
wandb:      epoch 9
wandb:  eval_loss 3.18717
wandb: perplexity 24.21985
wandb:       step 3500
wandb: train_loss 0.02829
wandb: 
wandb: Synced distinctive-snow-44: https://wandb.ai/joshuapaperspace/finetune_using_clm/runs/3irnetjb
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221117_203501-3irnetjb/logs
