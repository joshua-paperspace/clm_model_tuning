[2022-11-18 16:01:30,514][__main__][INFO] - Setting random seed to 17
[2022-11-18 16:01:30,515][__main__][INFO] - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

[2022-11-18 16:01:30,517][__main__][INFO] - output_dir: tuned-model
bittensor:
  network: nobunaga
dataset:
  name: ViktorThink/mountain_combined_813306
  config_name: null
  num_batches: 5000
  block_size: 256
  overwrite_cache: false
  keep_linebreaks: true
  concatenate_raw: false
  load_tokenized_data: false
model:
  name: facebook/opt-2.7b
  config_name: null
tokenizer:
  name: null
  use_fast: true
  preprocessing_num_workers: null
  pad_token: '[PAD]'
training:
  seed: 17
  val_split_percent: 20
  train_batch_size: 32
  eval_batch_size: 16
  learning_rate: 3.0e-06
  weight_decay: 0.05
  num_epochs: 4
  max_train_steps: null
  gradient_accumulation_steps: 2
  lr_scheduler: constant
  lr_warmup_steps: 5
  eval_every: 250
  max_eval_steps: 500
  checkpoint:
    resume_from_checkpoint: 0
    every_n_steps: null
tracking:
  enabled: true
  report_to: all
testing:
  enabled: false

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/config.json
Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-2.7B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      16
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 20,
  "num_layers": 32,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading file vocab.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/vocab.json
loading file merges.txt from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/merges.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/tokenizer_config.json
loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading weights file flax_model.msgpack from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Loading Flax weights from /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Some weights of the Flax model were not used when initializing the PyTorch model GPTNeoForCausalLM: ['model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.final_layer_norm.weight', 'model.decoder.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.12.self_attn.k_proj.weight', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.weight', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.weight', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.12.self_attn_layer_norm.weight', 'model.decoder.layers.12.self_attn_layer_norm.bias', 'model.decoder.layers.12.fc1.weight', 'model.decoder.layers.12.fc1.bias', 'model.decoder.layers.12.fc2.weight', 'model.decoder.layers.12.fc2.bias', 'model.decoder.layers.12.final_layer_norm.weight', 'model.decoder.layers.12.final_layer_norm.bias', 'model.decoder.layers.13.self_attn.k_proj.weight', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.weight', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.weight', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.weight', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn_layer_norm.weight', 'model.decoder.layers.13.self_attn_layer_norm.bias', 'model.decoder.layers.13.fc1.weight', 'model.decoder.layers.13.fc1.bias', 'model.decoder.layers.13.fc2.weight', 'model.decoder.layers.13.fc2.bias', 'model.decoder.layers.13.final_layer_norm.weight', 'model.decoder.layers.13.final_layer_norm.bias', 'model.decoder.layers.14.self_attn.k_proj.weight', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.weight', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.weight', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.weight', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.self_attn_layer_norm.weight', 'model.decoder.layers.14.self_attn_layer_norm.bias', 'model.decoder.layers.14.fc1.weight', 'model.decoder.layers.14.fc1.bias', 'model.decoder.layers.14.fc2.weight', 'model.decoder.layers.14.fc2.bias', 'model.decoder.layers.14.final_layer_norm.weight', 'model.decoder.layers.14.final_layer_norm.bias', 'model.decoder.layers.15.self_attn.k_proj.weight', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.weight', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.weight', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.weight', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn_layer_norm.weight', 'model.decoder.layers.15.self_attn_layer_norm.bias', 'model.decoder.layers.15.fc1.weight', 'model.decoder.layers.15.fc1.bias', 'model.decoder.layers.15.fc2.weight', 'model.decoder.layers.15.fc2.bias', 'model.decoder.layers.15.final_layer_norm.weight', 'model.decoder.layers.15.final_layer_norm.bias', 'model.decoder.layers.16.self_attn.k_proj.weight', 'model.decoder.layers.16.self_attn.k_proj.bias', 'model.decoder.layers.16.self_attn.v_proj.weight', 'model.decoder.layers.16.self_attn.v_proj.bias', 'model.decoder.layers.16.self_attn.q_proj.weight', 'model.decoder.layers.16.self_attn.q_proj.bias', 'model.decoder.layers.16.self_attn.out_proj.weight', 'model.decoder.layers.16.self_attn.out_proj.bias', 'model.decoder.layers.16.self_attn_layer_norm.weight', 'model.decoder.layers.16.self_attn_layer_norm.bias', 'model.decoder.layers.16.fc1.weight', 'model.decoder.layers.16.fc1.bias', 'model.decoder.layers.16.fc2.weight', 'model.decoder.layers.16.fc2.bias', 'model.decoder.layers.16.final_layer_norm.weight', 'model.decoder.layers.16.final_layer_norm.bias', 'model.decoder.layers.17.self_attn.k_proj.weight', 'model.decoder.layers.17.self_attn.k_proj.bias', 'model.decoder.layers.17.self_attn.v_proj.weight', 'model.decoder.layers.17.self_attn.v_proj.bias', 'model.decoder.layers.17.self_attn.q_proj.weight', 'model.decoder.layers.17.self_attn.q_proj.bias', 'model.decoder.layers.17.self_attn.out_proj.weight', 'model.decoder.layers.17.self_attn.out_proj.bias', 'model.decoder.layers.17.self_attn_layer_norm.weight', 'model.decoder.layers.17.self_attn_layer_norm.bias', 'model.decoder.layers.17.fc1.weight', 'model.decoder.layers.17.fc1.bias', 'model.decoder.layers.17.fc2.weight', 'model.decoder.layers.17.fc2.bias', 'model.decoder.layers.17.final_layer_norm.weight', 'model.decoder.layers.17.final_layer_norm.bias', 'model.decoder.layers.18.self_attn.k_proj.weight', 'model.decoder.layers.18.self_attn.k_proj.bias', 'model.decoder.layers.18.self_attn.v_proj.weight', 'model.decoder.layers.18.self_attn.v_proj.bias', 'model.decoder.layers.18.self_attn.q_proj.weight', 'model.decoder.layers.18.self_attn.q_proj.bias', 'model.decoder.layers.18.self_attn.out_proj.weight', 'model.decoder.layers.18.self_attn.out_proj.bias', 'model.decoder.layers.18.self_attn_layer_norm.weight', 'model.decoder.layers.18.self_attn_layer_norm.bias', 'model.decoder.layers.18.fc1.weight', 'model.decoder.layers.18.fc1.bias', 'model.decoder.layers.18.fc2.weight', 'model.decoder.layers.18.fc2.bias', 'model.decoder.layers.18.final_layer_norm.weight', 'model.decoder.layers.18.final_layer_norm.bias', 'model.decoder.layers.19.self_attn.k_proj.weight', 'model.decoder.layers.19.self_attn.k_proj.bias', 'model.decoder.layers.19.self_attn.v_proj.weight', 'model.decoder.layers.19.self_attn.v_proj.bias', 'model.decoder.layers.19.self_attn.q_proj.weight', 'model.decoder.layers.19.self_attn.q_proj.bias', 'model.decoder.layers.19.self_attn.out_proj.weight', 'model.decoder.layers.19.self_attn.out_proj.bias', 'model.decoder.layers.19.self_attn_layer_norm.weight', 'model.decoder.layers.19.self_attn_layer_norm.bias', 'model.decoder.layers.19.fc1.weight', 'model.decoder.layers.19.fc1.bias', 'model.decoder.layers.19.fc2.weight', 'model.decoder.layers.19.fc2.bias', 'model.decoder.layers.19.final_layer_norm.weight', 'model.decoder.layers.19.final_layer_norm.bias', 'model.decoder.layers.20.self_attn.k_proj.weight', 'model.decoder.layers.20.self_attn.k_proj.bias', 'model.decoder.layers.20.self_attn.v_proj.weight', 'model.decoder.layers.20.self_attn.v_proj.bias', 'model.decoder.layers.20.self_attn.q_proj.weight', 'model.decoder.layers.20.self_attn.q_proj.bias', 'model.decoder.layers.20.self_attn.out_proj.weight', 'model.decoder.layers.20.self_attn.out_proj.bias', 'model.decoder.layers.20.self_attn_layer_norm.weight', 'model.decoder.layers.20.self_attn_layer_norm.bias', 'model.decoder.layers.20.fc1.weight', 'model.decoder.layers.20.fc1.bias', 'model.decoder.layers.20.fc2.weight', 'model.decoder.layers.20.fc2.bias', 'model.decoder.layers.20.final_layer_norm.weight', 'model.decoder.layers.20.final_layer_norm.bias', 'model.decoder.layers.21.self_attn.k_proj.weight', 'model.decoder.layers.21.self_attn.k_proj.bias', 'model.decoder.layers.21.self_attn.v_proj.weight', 'model.decoder.layers.21.self_attn.v_proj.bias', 'model.decoder.layers.21.self_attn.q_proj.weight', 'model.decoder.layers.21.self_attn.q_proj.bias', 'model.decoder.layers.21.self_attn.out_proj.weight', 'model.decoder.layers.21.self_attn.out_proj.bias', 'model.decoder.layers.21.self_attn_layer_norm.weight', 'model.decoder.layers.21.self_attn_layer_norm.bias', 'model.decoder.layers.21.fc1.weight', 'model.decoder.layers.21.fc1.bias', 'model.decoder.layers.21.fc2.weight', 'model.decoder.layers.21.fc2.bias', 'model.decoder.layers.21.final_layer_norm.weight', 'model.decoder.layers.21.final_layer_norm.bias', 'model.decoder.layers.22.self_attn.k_proj.weight', 'model.decoder.layers.22.self_attn.k_proj.bias', 'model.decoder.layers.22.self_attn.v_proj.weight', 'model.decoder.layers.22.self_attn.v_proj.bias', 'model.decoder.layers.22.self_attn.q_proj.weight', 'model.decoder.layers.22.self_attn.q_proj.bias', 'model.decoder.layers.22.self_attn.out_proj.weight', 'model.decoder.layers.22.self_attn.out_proj.bias', 'model.decoder.layers.22.self_attn_layer_norm.weight', 'model.decoder.layers.22.self_attn_layer_norm.bias', 'model.decoder.layers.22.fc1.weight', 'model.decoder.layers.22.fc1.bias', 'model.decoder.layers.22.fc2.weight', 'model.decoder.layers.22.fc2.bias', 'model.decoder.layers.22.final_layer_norm.weight', 'model.decoder.layers.22.final_layer_norm.bias', 'model.decoder.layers.23.self_attn.k_proj.weight', 'model.decoder.layers.23.self_attn.k_proj.bias', 'model.decoder.layers.23.self_attn.v_proj.weight', 'model.decoder.layers.23.self_attn.v_proj.bias', 'model.decoder.layers.23.self_attn.q_proj.weight', 'model.decoder.layers.23.self_attn.q_proj.bias', 'model.decoder.layers.23.self_attn.out_proj.weight', 'model.decoder.layers.23.self_attn.out_proj.bias', 'model.decoder.layers.23.self_attn_layer_norm.weight', 'model.decoder.layers.23.self_attn_layer_norm.bias', 'model.decoder.layers.23.fc1.weight', 'model.decoder.layers.23.fc1.bias', 'model.decoder.layers.23.fc2.weight', 'model.decoder.layers.23.fc2.bias', 'model.decoder.layers.23.final_layer_norm.weight', 'model.decoder.layers.23.final_layer_norm.bias', 'model.decoder.layers.24.self_attn.k_proj.weight', 'model.decoder.layers.24.self_attn.k_proj.bias', 'model.decoder.layers.24.self_attn.v_proj.weight', 'model.decoder.layers.24.self_attn.v_proj.bias', 'model.decoder.layers.24.self_attn.q_proj.weight', 'model.decoder.layers.24.self_attn.q_proj.bias', 'model.decoder.layers.24.self_attn.out_proj.weight', 'model.decoder.layers.24.self_attn.out_proj.bias', 'model.decoder.layers.24.self_attn_layer_norm.weight', 'model.decoder.layers.24.self_attn_layer_norm.bias', 'model.decoder.layers.24.fc1.weight', 'model.decoder.layers.24.fc1.bias', 'model.decoder.layers.24.fc2.weight', 'model.decoder.layers.24.fc2.bias', 'model.decoder.layers.24.final_layer_norm.weight', 'model.decoder.layers.24.final_layer_norm.bias', 'model.decoder.layers.25.self_attn.k_proj.weight', 'model.decoder.layers.25.self_attn.k_proj.bias', 'model.decoder.layers.25.self_attn.v_proj.weight', 'model.decoder.layers.25.self_attn.v_proj.bias', 'model.decoder.layers.25.self_attn.q_proj.weight', 'model.decoder.layers.25.self_attn.q_proj.bias', 'model.decoder.layers.25.self_attn.out_proj.weight', 'model.decoder.layers.25.self_attn.out_proj.bias', 'model.decoder.layers.25.self_attn_layer_norm.weight', 'model.decoder.layers.25.self_attn_layer_norm.bias', 'model.decoder.layers.25.fc1.weight', 'model.decoder.layers.25.fc1.bias', 'model.decoder.layers.25.fc2.weight', 'model.decoder.layers.25.fc2.bias', 'model.decoder.layers.25.final_layer_norm.weight', 'model.decoder.layers.25.final_layer_norm.bias', 'model.decoder.layers.26.self_attn.k_proj.weight', 'model.decoder.layers.26.self_attn.k_proj.bias', 'model.decoder.layers.26.self_attn.v_proj.weight', 'model.decoder.layers.26.self_attn.v_proj.bias', 'model.decoder.layers.26.self_attn.q_proj.weight', 'model.decoder.layers.26.self_attn.q_proj.bias', 'model.decoder.layers.26.self_attn.out_proj.weight', 'model.decoder.layers.26.self_attn.out_proj.bias', 'model.decoder.layers.26.self_attn_layer_norm.weight', 'model.decoder.layers.26.self_attn_layer_norm.bias', 'model.decoder.layers.26.fc1.weight', 'model.decoder.layers.26.fc1.bias', 'model.decoder.layers.26.fc2.weight', 'model.decoder.layers.26.fc2.bias', 'model.decoder.layers.26.final_layer_norm.weight', 'model.decoder.layers.26.final_layer_norm.bias', 'model.decoder.layers.27.self_attn.k_proj.weight', 'model.decoder.layers.27.self_attn.k_proj.bias', 'model.decoder.layers.27.self_attn.v_proj.weight', 'model.decoder.layers.27.self_attn.v_proj.bias', 'model.decoder.layers.27.self_attn.q_proj.weight', 'model.decoder.layers.27.self_attn.q_proj.bias', 'model.decoder.layers.27.self_attn.out_proj.weight', 'model.decoder.layers.27.self_attn.out_proj.bias', 'model.decoder.layers.27.self_attn_layer_norm.weight', 'model.decoder.layers.27.self_attn_layer_norm.bias', 'model.decoder.layers.27.fc1.weight', 'model.decoder.layers.27.fc1.bias', 'model.decoder.layers.27.fc2.weight', 'model.decoder.layers.27.fc2.bias', 'model.decoder.layers.27.final_layer_norm.weight', 'model.decoder.layers.27.final_layer_norm.bias', 'model.decoder.layers.28.self_attn.k_proj.weight', 'model.decoder.layers.28.self_attn.k_proj.bias', 'model.decoder.layers.28.self_attn.v_proj.weight', 'model.decoder.layers.28.self_attn.v_proj.bias', 'model.decoder.layers.28.self_attn.q_proj.weight', 'model.decoder.layers.28.self_attn.q_proj.bias', 'model.decoder.layers.28.self_attn.out_proj.weight', 'model.decoder.layers.28.self_attn.out_proj.bias', 'model.decoder.layers.28.self_attn_layer_norm.weight', 'model.decoder.layers.28.self_attn_layer_norm.bias', 'model.decoder.layers.28.fc1.weight', 'model.decoder.layers.28.fc1.bias', 'model.decoder.layers.28.fc2.weight', 'model.decoder.layers.28.fc2.bias', 'model.decoder.layers.28.final_layer_norm.weight', 'model.decoder.layers.28.final_layer_norm.bias', 'model.decoder.layers.29.self_attn.k_proj.weight', 'model.decoder.layers.29.self_attn.k_proj.bias', 'model.decoder.layers.29.self_attn.v_proj.weight', 'model.decoder.layers.29.self_attn.v_proj.bias', 'model.decoder.layers.29.self_attn.q_proj.weight', 'model.decoder.layers.29.self_attn.q_proj.bias', 'model.decoder.layers.29.self_attn.out_proj.weight', 'model.decoder.layers.29.self_attn.out_proj.bias', 'model.decoder.layers.29.self_attn_layer_norm.weight', 'model.decoder.layers.29.self_attn_layer_norm.bias', 'model.decoder.layers.29.fc1.weight', 'model.decoder.layers.29.fc1.bias', 'model.decoder.layers.29.fc2.weight', 'model.decoder.layers.29.fc2.bias', 'model.decoder.layers.29.final_layer_norm.weight', 'model.decoder.layers.29.final_layer_norm.bias', 'model.decoder.layers.30.self_attn.k_proj.weight', 'model.decoder.layers.30.self_attn.k_proj.bias', 'model.decoder.layers.30.self_attn.v_proj.weight', 'model.decoder.layers.30.self_attn.v_proj.bias', 'model.decoder.layers.30.self_attn.q_proj.weight', 'model.decoder.layers.30.self_attn.q_proj.bias', 'model.decoder.layers.30.self_attn.out_proj.weight', 'model.decoder.layers.30.self_attn.out_proj.bias', 'model.decoder.layers.30.self_attn_layer_norm.weight', 'model.decoder.layers.30.self_attn_layer_norm.bias', 'model.decoder.layers.30.fc1.weight', 'model.decoder.layers.30.fc1.bias', 'model.decoder.layers.30.fc2.weight', 'model.decoder.layers.30.fc2.bias', 'model.decoder.layers.30.final_layer_norm.weight', 'model.decoder.layers.30.final_layer_norm.bias', 'model.decoder.layers.31.self_attn.k_proj.weight', 'model.decoder.layers.31.self_attn.k_proj.bias', 'model.decoder.layers.31.self_attn.v_proj.weight', 'model.decoder.layers.31.self_attn.v_proj.bias', 'model.decoder.layers.31.self_attn.q_proj.weight', 'model.decoder.layers.31.self_attn.q_proj.bias', 'model.decoder.layers.31.self_attn.out_proj.weight', 'model.decoder.layers.31.self_attn.out_proj.bias', 'model.decoder.layers.31.self_attn_layer_norm.weight', 'model.decoder.layers.31.self_attn_layer_norm.bias', 'model.decoder.layers.31.fc1.weight', 'model.decoder.layers.31.fc1.bias', 'model.decoder.layers.31.fc2.weight', 'model.decoder.layers.31.fc2.bias', 'model.decoder.layers.31.final_layer_norm.weight', 'model.decoder.layers.31.final_layer_norm.bias']
- This IS expected if you are initializing GPTNeoForCausalLM from a Flax model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a FlaxBertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoForCausalLM from a Flax model that you expect to be exactly identical (e.g. initializing a BertForSequenceClassification model from a FlaxBertForSequenceClassification model).
Some weights of GPTNeoForCausalLM were not initialized from the Flax model and are newly initialized: ['transformer.h.2.attn.attention.out_proj.weight', 'transformer.h.27.mlp.c_fc.weight', 'transformer.h.30.attn.attention.masked_bias', 'transformer.h.18.ln_2.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.2.ln_2.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.0.attn.attention.q_proj.weight', 'transformer.h.30.mlp.c_fc.weight', 'transformer.h.10.attn.attention.q_proj.weight', 'transformer.h.2.ln_2.weight', 'transformer.h.29.attn.attention.v_proj.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.4.attn.attention.out_proj.weight', 'transformer.h.31.mlp.c_fc.bias', 'transformer.h.14.attn.attention.bias', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.24.ln_2.bias', 'transformer.h.9.ln_1.bias', 'lm_head.weight', 'transformer.h.21.attn.attention.q_proj.weight', 'transformer.h.11.attn.attention.masked_bias', 'transformer.h.19.attn.attention.out_proj.weight', 'transformer.h.6.attn.attention.masked_bias', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.26.mlp.c_proj.weight', 'transformer.h.7.attn.attention.q_proj.weight', 'transformer.h.14.attn.attention.masked_bias', 'transformer.h.3.ln_1.bias', 'transformer.h.5.attn.attention.v_proj.weight', 'transformer.h.17.attn.attention.out_proj.bias', 'transformer.h.26.ln_2.bias', 'transformer.h.27.attn.attention.k_proj.weight', 'transformer.h.8.attn.attention.out_proj.bias', 'transformer.h.2.ln_1.bias', 'transformer.h.30.ln_2.weight', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.4.attn.attention.out_proj.bias', 'transformer.h.9.attn.attention.out_proj.weight', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.26.attn.attention.bias', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.18.ln_1.weight', 'transformer.h.12.attn.attention.k_proj.weight', 'transformer.h.25.ln_1.bias', 'transformer.h.16.attn.attention.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.12.ln_1.weight', 'transformer.h.8.ln_2.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.10.attn.attention.out_proj.bias', 'transformer.h.12.ln_2.bias', 'transformer.h.26.ln_1.weight', 'transformer.h.28.attn.attention.out_proj.bias', 'transformer.h.14.attn.attention.v_proj.weight', 'transformer.h.29.ln_1.weight', 'transformer.h.25.attn.attention.k_proj.weight', 'transformer.h.7.attn.attention.out_proj.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.9.attn.attention.v_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.7.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.24.ln_2.weight', 'transformer.h.15.attn.attention.masked_bias', 'transformer.h.8.attn.attention.v_proj.weight', 'transformer.h.15.attn.attention.q_proj.weight', 'transformer.h.6.attn.attention.v_proj.weight', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.22.ln_2.weight', 'transformer.h.22.attn.attention.bias', 'transformer.h.30.ln_1.weight', 'transformer.h.25.attn.attention.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.1.attn.attention.masked_bias', 'transformer.h.0.attn.attention.k_proj.weight', 'transformer.h.28.mlp.c_fc.weight', 'transformer.h.21.attn.attention.k_proj.weight', 'transformer.h.20.ln_1.bias', 'transformer.h.9.attn.attention.q_proj.weight', 'transformer.h.25.ln_2.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.31.mlp.c_fc.weight', 'transformer.h.23.attn.attention.k_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.12.attn.attention.out_proj.bias', 'transformer.h.28.attn.attention.masked_bias', 'transformer.h.7.attn.attention.masked_bias', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.30.attn.attention.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.26.ln_2.weight', 'transformer.h.26.mlp.c_fc.weight', 'transformer.h.11.ln_2.weight', 'transformer.h.24.attn.attention.bias', 'transformer.h.22.attn.attention.out_proj.weight', 'transformer.h.24.ln_1.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.30.attn.attention.out_proj.weight', 'transformer.h.10.attn.attention.v_proj.weight', 'transformer.h.31.attn.attention.out_proj.bias', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.5.attn.attention.masked_bias', 'transformer.h.23.attn.attention.out_proj.bias', 'transformer.h.21.ln_2.bias', 'transformer.h.27.attn.attention.out_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.25.mlp.c_fc.weight', 'transformer.h.6.attn.attention.out_proj.weight', 'transformer.h.18.attn.attention.out_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.4.attn.attention.masked_bias', 'transformer.h.8.attn.attention.masked_bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.24.mlp.c_fc.weight', 'transformer.h.30.attn.attention.k_proj.weight', 'transformer.h.23.attn.attention.out_proj.weight', 'transformer.h.6.ln_1.weight', 'transformer.h.0.ln_2.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.28.attn.attention.v_proj.weight', 'transformer.h.1.attn.attention.v_proj.weight', 'transformer.h.13.attn.attention.bias', 'transformer.h.23.ln_2.bias', 'transformer.h.18.ln_2.bias', 'transformer.h.21.attn.attention.v_proj.weight', 'transformer.wpe.weight', 'transformer.h.3.attn.attention.out_proj.bias', 'transformer.h.3.ln_2.bias', 'transformer.h.14.ln_1.weight', 'transformer.h.22.attn.attention.v_proj.weight', 'transformer.h.22.attn.attention.out_proj.bias', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.18.attn.attention.bias', 'transformer.h.26.attn.attention.masked_bias', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.13.attn.attention.out_proj.weight', 'transformer.h.13.attn.attention.out_proj.bias', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.19.attn.attention.masked_bias', 'transformer.h.30.ln_2.bias', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.29.attn.attention.out_proj.bias', 'transformer.h.0.ln_2.bias', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.24.attn.attention.k_proj.weight', 'transformer.h.5.attn.attention.out_proj.weight', 'transformer.h.27.attn.attention.v_proj.weight', 'transformer.h.12.attn.attention.masked_bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.27.ln_1.weight', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.21.ln_2.weight', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.9.ln_2.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.13.ln_1.bias', 'transformer.h.12.ln_2.weight', 'transformer.h.11.attn.attention.q_proj.weight', 'transformer.h.26.attn.attention.q_proj.weight', 'transformer.h.27.attn.attention.masked_bias', 'transformer.h.7.attn.attention.v_proj.weight', 'transformer.h.20.attn.attention.v_proj.weight', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.24.attn.attention.q_proj.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.3.attn.attention.q_proj.weight', 'transformer.h.4.attn.attention.k_proj.weight', 'transformer.h.22.ln_2.bias', 'transformer.h.22.attn.attention.k_proj.weight', 'transformer.h.11.attn.attention.k_proj.weight', 'transformer.h.29.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.26.attn.attention.out_proj.bias', 'transformer.h.17.attn.attention.masked_bias', 'transformer.h.28.mlp.c_fc.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.21.ln_1.weight', 'transformer.h.25.attn.attention.out_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.10.attn.attention.out_proj.weight', 'transformer.h.24.mlp.c_fc.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.28.ln_2.weight', 'transformer.h.19.attn.attention.k_proj.weight', 'transformer.h.25.attn.attention.out_proj.weight', 'transformer.h.28.ln_1.bias', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.5.attn.attention.k_proj.weight', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.0.attn.attention.v_proj.weight', 'transformer.h.2.attn.attention.v_proj.weight', 'transformer.h.27.mlp.c_proj.weight', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.25.attn.attention.q_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.23.attn.attention.q_proj.weight', 'transformer.h.7.attn.attention.bias', 'transformer.h.5.ln_1.bias', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.15.ln_2.bias', 'transformer.h.28.mlp.c_proj.weight', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.0.attn.attention.masked_bias', 'transformer.h.0.attn.attention.out_proj.weight', 'transformer.h.31.attn.attention.v_proj.weight', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.21.ln_1.bias', 'transformer.h.11.ln_1.bias', 'transformer.h.21.attn.attention.masked_bias', 'transformer.h.7.ln_2.bias', 'transformer.h.15.attn.attention.k_proj.weight', 'transformer.h.16.ln_1.weight', 'transformer.h.27.attn.attention.bias', 'transformer.h.25.mlp.c_proj.weight', 'transformer.h.28.attn.attention.k_proj.weight', 'transformer.h.16.ln_2.weight', 'transformer.h.13.attn.attention.k_proj.weight', 'transformer.h.17.attn.attention.bias', 'transformer.h.9.ln_2.bias', 'transformer.h.9.attn.attention.masked_bias', 'transformer.h.14.attn.attention.k_proj.weight', 'transformer.h.16.attn.attention.k_proj.weight', 'transformer.h.25.attn.attention.v_proj.weight', 'transformer.h.12.attn.attention.out_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.7.attn.attention.out_proj.weight', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.27.attn.attention.out_proj.bias', 'transformer.h.16.attn.attention.masked_bias', 'transformer.h.28.attn.attention.out_proj.weight', 'transformer.h.18.attn.attention.out_proj.weight', 'transformer.h.1.attn.attention.k_proj.weight', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.30.ln_1.bias', 'transformer.h.8.ln_1.bias', 'transformer.h.16.attn.attention.v_proj.weight', 'transformer.h.22.ln_1.bias', 'transformer.h.4.attn.attention.q_proj.weight', 'transformer.h.31.attn.attention.q_proj.weight', 'transformer.h.10.attn.attention.masked_bias', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.1.attn.attention.out_proj.bias', 'transformer.wte.weight', 'transformer.h.19.ln_2.bias', 'transformer.h.6.attn.attention.out_proj.bias', 'transformer.h.30.attn.attention.v_proj.weight', 'transformer.h.25.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.23.attn.attention.v_proj.weight', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.30.attn.attention.out_proj.bias', 'transformer.h.29.attn.attention.q_proj.weight', 'transformer.h.11.attn.attention.v_proj.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.22.ln_1.weight', 'transformer.h.14.attn.attention.out_proj.weight', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.20.ln_2.bias', 'transformer.h.16.ln_1.bias', 'transformer.h.21.attn.attention.out_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.18.ln_1.bias', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.6.ln_2.bias', 'transformer.h.16.ln_2.bias', 'transformer.h.28.ln_1.weight', 'transformer.h.24.ln_1.weight', 'transformer.h.15.attn.attention.out_proj.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.9.attn.attention.out_proj.bias', 'transformer.h.29.attn.attention.masked_bias', 'transformer.h.17.ln_1.bias', 'transformer.h.18.attn.attention.k_proj.weight', 'transformer.h.24.attn.attention.masked_bias', 'transformer.h.29.attn.attention.out_proj.weight', 'transformer.h.17.attn.attention.out_proj.weight', 'transformer.h.25.ln_2.weight', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.19.ln_2.weight', 'transformer.h.20.ln_1.weight', 'transformer.h.11.attn.attention.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.16.attn.attention.q_proj.weight', 'transformer.h.23.attn.attention.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.13.attn.attention.masked_bias', 'transformer.h.10.ln_1.bias', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.27.ln_2.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.19.ln_1.bias', 'transformer.h.27.ln_2.bias', 'transformer.h.29.attn.attention.k_proj.weight', 'transformer.h.14.attn.attention.q_proj.weight', 'transformer.h.24.attn.attention.out_proj.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.11.ln_1.weight', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.22.attn.attention.q_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.21.attn.attention.out_proj.weight', 'transformer.h.4.ln_1.weight', 'transformer.h.6.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.23.ln_1.weight', 'transformer.h.16.attn.attention.out_proj.weight', 'transformer.h.23.ln_1.bias', 'transformer.h.11.attn.attention.out_proj.weight', 'transformer.h.31.attn.attention.bias', 'transformer.h.12.attn.attention.q_proj.weight', 'transformer.h.15.attn.attention.out_proj.bias', 'transformer.h.17.ln_2.weight', 'transformer.h.18.attn.attention.masked_bias', 'transformer.h.13.ln_2.weight', 'transformer.h.3.attn.attention.k_proj.weight', 'transformer.h.9.ln_1.weight', 'transformer.h.17.attn.attention.k_proj.weight', 'transformer.h.10.attn.attention.bias', 'transformer.h.4.attn.attention.v_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.30.attn.attention.q_proj.weight', 'transformer.h.2.attn.attention.out_proj.bias', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.3.attn.attention.masked_bias', 'transformer.h.8.attn.attention.k_proj.weight', 'transformer.h.8.attn.attention.q_proj.weight', 'transformer.h.26.attn.attention.v_proj.weight', 'transformer.h.11.attn.attention.out_proj.bias', 'transformer.h.20.attn.attention.masked_bias', 'transformer.h.1.ln_2.bias', 'transformer.h.25.ln_1.weight', 'transformer.h.31.ln_2.bias', 'transformer.h.12.ln_1.bias', 'transformer.h.13.attn.attention.v_proj.weight', 'transformer.h.15.attn.attention.v_proj.weight', 'transformer.h.23.attn.attention.masked_bias', 'transformer.h.30.mlp.c_fc.bias', 'transformer.h.0.ln_1.weight', 'transformer.h.19.ln_1.weight', 'transformer.h.19.attn.attention.out_proj.bias', 'transformer.h.31.ln_2.weight', 'transformer.h.17.attn.attention.v_proj.weight', 'transformer.h.29.ln_2.bias', 'transformer.h.1.ln_1.bias', 'transformer.h.24.mlp.c_proj.weight', 'transformer.h.17.ln_1.weight', 'transformer.h.12.attn.attention.bias', 'transformer.h.0.attn.attention.out_proj.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.25.attn.attention.masked_bias', 'transformer.h.5.attn.attention.q_proj.weight', 'transformer.h.0.attn.attention.bias', 'transformer.ln_f.bias', 'transformer.h.15.ln_2.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.31.attn.attention.masked_bias', 'transformer.h.14.ln_1.bias', 'transformer.h.20.ln_2.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.9.attn.attention.k_proj.weight', 'transformer.h.4.attn.attention.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.26.ln_1.bias', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.22.attn.attention.masked_bias', 'transformer.h.15.ln_1.bias', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.8.attn.attention.bias', 'transformer.h.29.mlp.c_proj.weight', 'transformer.h.20.attn.attention.out_proj.bias', 'transformer.h.19.attn.attention.q_proj.weight', 'transformer.h.29.mlp.c_fc.weight', 'transformer.h.2.attn.attention.bias', 'transformer.h.24.attn.attention.v_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.30.mlp.c_proj.weight', 'transformer.h.8.attn.attention.out_proj.weight', 'transformer.h.27.ln_1.bias', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.19.attn.attention.bias', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.15.ln_1.weight', 'transformer.h.2.attn.attention.k_proj.weight', 'transformer.h.17.attn.attention.q_proj.weight', 'transformer.h.1.attn.attention.out_proj.weight', 'transformer.h.20.attn.attention.out_proj.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.27.attn.attention.q_proj.weight', 'transformer.h.28.attn.attention.bias', 'transformer.h.17.ln_2.bias', 'transformer.h.28.ln_2.bias', 'transformer.ln_f.weight', 'transformer.h.31.ln_1.weight', 'transformer.h.18.attn.attention.v_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.26.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.6.attn.attention.q_proj.weight', 'transformer.h.12.attn.attention.v_proj.weight', 'transformer.h.5.attn.attention.out_proj.bias', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.26.attn.attention.out_proj.weight', 'transformer.h.2.attn.attention.q_proj.weight', 'transformer.h.21.attn.attention.bias', 'transformer.h.8.ln_2.bias', 'transformer.h.20.attn.attention.q_proj.weight', 'transformer.h.13.attn.attention.q_proj.weight', 'transformer.h.10.ln_1.weight', 'transformer.h.5.attn.attention.bias', 'transformer.h.1.attn.attention.q_proj.weight', 'transformer.h.7.attn.attention.k_proj.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.28.attn.attention.q_proj.weight', 'transformer.h.31.mlp.c_proj.weight', 'transformer.h.14.ln_2.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.31.attn.attention.k_proj.weight', 'transformer.h.13.ln_1.weight', 'transformer.h.20.attn.attention.bias', 'transformer.h.13.ln_2.bias', 'transformer.h.3.attn.attention.out_proj.weight', 'transformer.h.29.attn.attention.bias', 'transformer.h.7.ln_1.bias', 'transformer.h.14.attn.attention.out_proj.bias', 'transformer.h.18.attn.attention.q_proj.weight', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.29.mlp.c_fc.bias', 'transformer.h.2.attn.attention.masked_bias', 'transformer.h.23.ln_2.weight', 'transformer.h.31.ln_1.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.26.attn.attention.k_proj.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.20.attn.attention.k_proj.weight', 'transformer.h.3.ln_1.weight', 'transformer.h.3.attn.attention.v_proj.weight', 'transformer.h.27.mlp.c_fc.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.31.attn.attention.out_proj.weight', 'transformer.h.10.attn.attention.k_proj.weight', 'transformer.h.6.attn.attention.k_proj.weight', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.29.ln_1.bias', 'transformer.h.19.attn.attention.v_proj.weight', 'transformer.h.16.attn.attention.out_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.14.ln_2.weight', 'transformer.h.24.attn.attention.out_proj.weight', 'transformer.h.9.mlp.c_fc.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2022-11-18 16:01:57,285][datasets.builder][WARNING] - Using custom data configuration ViktorThink--mountain_combined_813306-6918b9ea07482433
[2022-11-18 16:01:57,360][datasets.builder][WARNING] - Found cached dataset csv (/home/paperspace/.cache/huggingface/datasets/ViktorThink___csv/ViktorThink--mountain_combined_813306-6918b9ea07482433/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  5.88it/s]100%|██████████| 2/2 [00:00<00:00, 11.17it/s]
Running tokenizer on dataset:   0%|          | 0/814 [00:00<?, ?ba/s]Running tokenizer on dataset:   0%|          | 1/814 [00:02<34:22,  2.54s/ba]Running tokenizer on dataset:   0%|          | 2/814 [00:04<31:40,  2.34s/ba]Running tokenizer on dataset:   0%|          | 3/814 [00:06<30:37,  2.27s/ba]Running tokenizer on dataset:   0%|          | 4/814 [00:09<29:44,  2.20s/ba]Running tokenizer on dataset:   1%|          | 5/814 [00:11<29:02,  2.15s/ba]Running tokenizer on dataset:   1%|          | 6/814 [00:13<28:10,  2.09s/ba]Running tokenizer on dataset:   1%|          | 7/814 [00:15<27:55,  2.08s/ba]Running tokenizer on dataset:   1%|          | 8/814 [00:17<27:29,  2.05s/ba]Running tokenizer on dataset:   1%|          | 9/814 [00:19<26:59,  2.01s/ba]Running tokenizer on dataset:   1%|          | 10/814 [00:20<26:32,  1.98s/ba]Running tokenizer on dataset:   1%|▏         | 11/814 [00:22<26:32,  1.98s/ba]Running tokenizer on dataset:   1%|▏         | 12/814 [00:24<26:23,  1.97s/ba]Running tokenizer on dataset:   2%|▏         | 13/814 [00:26<26:14,  1.97s/ba]Running tokenizer on dataset:   2%|▏         | 14/814 [00:28<26:09,  1.96s/ba]Running tokenizer on dataset:   2%|▏         | 15/814 [00:30<25:57,  1.95s/ba]Running tokenizer on dataset:   2%|▏         | 16/814 [00:32<25:42,  1.93s/ba]Running tokenizer on dataset:   2%|▏         | 17/814 [00:34<25:27,  1.92s/ba]Running tokenizer on dataset:   2%|▏         | 18/814 [00:36<25:14,  1.90s/ba]Running tokenizer on dataset:   2%|▏         | 19/814 [00:38<25:07,  1.90s/ba]Running tokenizer on dataset:   2%|▏         | 20/814 [00:40<25:06,  1.90s/ba]Running tokenizer on dataset:   3%|▎         | 21/814 [00:42<25:05,  1.90s/ba]Running tokenizer on dataset:   3%|▎         | 22/814 [00:43<24:53,  1.89s/ba]Running tokenizer on dataset:   3%|▎         | 23/814 [00:45<24:44,  1.88s/ba]Running tokenizer on dataset:   3%|▎         | 24/814 [00:47<24:38,  1.87s/ba]Running tokenizer on dataset:   3%|▎         | 25/814 [00:49<24:39,  1.88s/ba]Running tokenizer on dataset:   3%|▎         | 26/814 [00:51<25:50,  1.97s/ba]Running tokenizer on dataset:   3%|▎         | 27/814 [00:54<27:34,  2.10s/ba]Running tokenizer on dataset:   3%|▎         | 28/814 [00:56<28:10,  2.15s/ba]Running tokenizer on dataset:   4%|▎         | 29/814 [00:58<27:08,  2.08s/ba]Running tokenizer on dataset:   4%|▎         | 30/814 [01:00<26:20,  2.02s/ba]Running tokenizer on dataset:   4%|▍         | 31/814 [01:01<25:32,  1.96s/ba]Running tokenizer on dataset:   4%|▍         | 32/814 [01:03<25:18,  1.94s/ba]Running tokenizer on dataset:   4%|▍         | 33/814 [01:05<25:02,  1.92s/ba]Running tokenizer on dataset:   4%|▍         | 34/814 [01:07<24:31,  1.89s/ba]Running tokenizer on dataset:   4%|▍         | 35/814 [01:09<24:10,  1.86s/ba]Running tokenizer on dataset:   4%|▍         | 36/814 [01:11<23:50,  1.84s/ba]Running tokenizer on dataset:   5%|▍         | 37/814 [01:12<23:37,  1.82s/ba]Running tokenizer on dataset:   5%|▍         | 38/814 [01:14<23:19,  1.80s/ba]Running tokenizer on dataset:   5%|▍         | 39/814 [01:16<23:34,  1.83s/ba]Running tokenizer on dataset:   5%|▍         | 40/814 [01:18<23:32,  1.83s/ba]Running tokenizer on dataset:   5%|▌         | 41/814 [01:20<23:39,  1.84s/ba]Running tokenizer on dataset:   5%|▌         | 42/814 [01:22<23:43,  1.84s/ba]Running tokenizer on dataset:   5%|▌         | 43/814 [01:23<23:33,  1.83s/ba]Running tokenizer on dataset:   5%|▌         | 44/814 [01:25<23:48,  1.86s/ba]Running tokenizer on dataset:   6%|▌         | 45/814 [01:27<23:41,  1.85s/ba]Running tokenizer on dataset:   6%|▌         | 46/814 [01:29<23:26,  1.83s/ba]Running tokenizer on dataset:   6%|▌         | 47/814 [01:31<23:19,  1.82s/ba]Running tokenizer on dataset:   6%|▌         | 48/814 [01:32<23:03,  1.81s/ba]Running tokenizer on dataset:   6%|▌         | 49/814 [01:34<23:04,  1.81s/ba]Running tokenizer on dataset:   6%|▌         | 50/814 [01:36<23:04,  1.81s/ba]Running tokenizer on dataset:   6%|▋         | 51/814 [01:38<22:53,  1.80s/ba]Running tokenizer on dataset:   6%|▋         | 52/814 [01:40<23:00,  1.81s/ba]Running tokenizer on dataset:   7%|▋         | 53/814 [01:42<22:53,  1.81s/ba]Running tokenizer on dataset:   7%|▋         | 54/814 [01:43<22:50,  1.80s/ba]Running tokenizer on dataset:   7%|▋         | 55/814 [01:45<23:05,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 56/814 [01:47<23:00,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 57/814 [01:49<23:12,  1.84s/ba]Running tokenizer on dataset:   7%|▋         | 58/814 [01:51<22:56,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 59/814 [01:52<22:47,  1.81s/ba]Running tokenizer on dataset:   7%|▋         | 60/814 [01:54<22:35,  1.80s/ba]Running tokenizer on dataset:   7%|▋         | 61/814 [01:56<22:35,  1.80s/ba]Running tokenizer on dataset:   8%|▊         | 62/814 [01:58<22:40,  1.81s/ba]Running tokenizer on dataset:   8%|▊         | 63/814 [02:00<22:36,  1.81s/ba]Running tokenizer on dataset:   8%|▊         | 64/814 [02:01<22:23,  1.79s/ba]Running tokenizer on dataset:   8%|▊         | 65/814 [02:03<22:33,  1.81s/ba]Running tokenizer on dataset:   8%|▊         | 66/814 [02:05<22:31,  1.81s/ba]Running tokenizer on dataset:   8%|▊         | 67/814 [02:07<22:15,  1.79s/ba]Running tokenizer on dataset:   8%|▊         | 68/814 [02:09<22:09,  1.78s/ba]Running tokenizer on dataset:   8%|▊         | 69/814 [02:10<22:00,  1.77s/ba]Running tokenizer on dataset:   9%|▊         | 70/814 [02:12<21:57,  1.77s/ba]Running tokenizer on dataset:   9%|▊         | 71/814 [02:14<21:49,  1.76s/ba]Running tokenizer on dataset:   9%|▉         | 72/814 [02:16<23:57,  1.94s/ba]Running tokenizer on dataset:   9%|▉         | 73/814 [02:18<23:16,  1.88s/ba]Running tokenizer on dataset:   9%|▉         | 74/814 [02:20<22:47,  1.85s/ba]Running tokenizer on dataset:   9%|▉         | 75/814 [02:21<22:27,  1.82s/ba]Running tokenizer on dataset:   9%|▉         | 76/814 [02:23<22:52,  1.86s/ba]Running tokenizer on dataset:   9%|▉         | 77/814 [02:25<22:28,  1.83s/ba]Running tokenizer on dataset:  10%|▉         | 78/814 [02:27<22:20,  1.82s/ba]Running tokenizer on dataset:  10%|▉         | 79/814 [02:29<22:02,  1.80s/ba]Running tokenizer on dataset:  10%|▉         | 79/814 [02:30<23:16,  1.90s/ba]
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/arrow_dataset.py:2982 in _map_single                         │
│                                                                              │
│   2979 │   │   │   │   │   │   │   range(*(slice(i, i + batch_size).indices( │
│   2980 │   │   │   │   │   │   )  # Something simpler?                       │
│   2981 │   │   │   │   │   │   try:                                          │
│ ❱ 2982 │   │   │   │   │   │   │   batch = apply_function_on_filtered_inputs │
│   2983 │   │   │   │   │   │   │   │   batch,                                │
│   2984 │   │   │   │   │   │   │   │   indices,                              │
│   2985 │   │   │   │   │   │   │   │   check_same_num_examples=len(input_dat │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │ apply_function_on_filtered_inputs = <function                            │ │
│ │                                     Dataset._map_single.<locals>.apply_… │ │
│ │                                     at 0x7f467d7d7820>                   │ │
│ │                             batch = {                                    │ │
│ │                                     │   'train': [                       │ │
│ │                                     │   │   'upon better and\\nnobler    │ │
│ │                                     views; and advise your elder         │ │
│ │                                     sisters, when they perceive'+1575,   │ │
│ │                                     │   │   "log time reviewing U.S.     │ │
│ │                                     Army training films from the Vietnam │ │
│ │                                     era. That's the mil"+1355,           │ │
│ │                                     │   │   'the speculative             │ │
│ │                                     philosophers. For example, the great │ │
│ │                                     mathematician Euler, who do'+1567,   │ │
│ │                                     │   │   'the bedchamber. A short     │ │
│ │                                     while later he confided in one of    │ │
│ │                                     his servants, secretly'+1331,        │ │
│ │                                     │   │   'the familiar bond valuation │ │
│ │                                     equation. Assuming semiannual coupon │ │
│ │                                     payments, the e'+1402,               │ │
│ │                                     │   │   '$X$ associated with         │ │
│ │                                     $U\\\\bar{U}$ and $\\\\bar{D}{D}$    │ │
│ │                                     since this would drop the            │ │
│ │                                     poss'+1860,                          │ │
│ │                                     │   │   'parking lot                 │ │
│ │                                     dings.\\"\\n\\n\\"When these little  │ │
│ │                                     charges blow, they\'ll leave M16     │ │
│ │                                     impa'+1470,                          │ │
│ │                                     │   │   'above from Helms, \\"The    │ │
│ │                                     Indians,\\" pp. 37–45. \\n | Bruhns, │ │
│ │                                     _Ancient South Americ'+1388,         │ │
│ │                                     │   │   'get some sleep, so I        │ │
│ │                                     removed the light bulb and stored it │ │
│ │                                     in the pocket of the C'+1301,        │ │
│ │                                     │   │   'set of ADE20K in Table      │ │
│ │                                     \\\\[tab:baseline\\\\]. All our      │ │
│ │                                     results except the last-row o'+1732, │ │
│ │                                     │   │   ... +990                     │ │
│ │                                     │   ],                               │ │
│ │                                     │   'validation': [                  │ │
│ │                                     │   │   "given we can't stop people  │ │
│ │                                     signing up again with a new          │ │
│ │                                     disposable email address)"+1984,     │ │
│ │                                     │   │   'I\'m just repeating\'       │ │
│ │                                     stuff.\\" \\"Did I hear you say      │ │
│ │                                     you\'re looking for an               │ │
│ │                                     apartmen'+1451,                      │ │
│ │                                     │   │   'a model for belief. In M.   │ │
│ │                                     Vargas \\u0026 G. Yaffe (Eds.),      │ │
│ │                                     Rational and social age'+1793,       │ │
│ │                                     │   │   'away.\\"\\n\\n\\"Somehow, I │ │
│ │                                     think she knows where he is,\\"      │ │
│ │                                     Virginia said, thoughtful'+1451,     │ │
│ │                                     │   │   'plane the limits of         │ │
│ │                                     validity of our model.\\n\\n![(Color │ │
│ │                                     online) [*Characterizatio'+2041,     │ │
│ │                                     │   │   "son for fighting\\nagainst  │ │
│ │                                     the king and the mother country. The │ │
│ │                                     old lady's face w"+1653,             │ │
│ │                                     │   │   'again.\\" \\"We\'re gonna   │ │
│ │                                     have to make the cut now.\\"         │ │
│ │                                     \\"Sharon?\\" \\"can you hear        │ │
│ │                                     me'+1554,                            │ │
│ │                                     │   │   'even have more commissions  │ │
│ │                                     such as yours.\\"\\n\\nThat was a    │ │
│ │                                     not-so-veiled referenc'+1491,        │ │
│ │                                     │   │   'stealth technology we were  │ │
│ │                                     never able to break.\\" \\"So how    │ │
│ │                                     come all of a sudden '+1534,         │ │
│ │                                     │   │   'create\\na small example    │ │
│ │                                     service.\\nI want to focus on two    │ │
│ │                                     major requirements.\\nPe'+1838,      │ │
│ │                                     │   │   ... +990                     │ │
│ │                                     │   ]                                │ │
│ │                                     }                                    │ │
│ │                        batch_size = 1000                                 │ │
│ │                           batched = True                                 │ │
│ │                        buf_writer = None                                 │ │
│ │                   cache_file_name = '/home/paperspace/.cache/huggingfac… │ │
│ │                        cache_only = False                                │ │
│ │                              desc = 'Running tokenizer on dataset'       │ │
│ │                  disable_nullable = False                                │ │
│ │                      disable_tqdm = False                                │ │
│ │                   drop_last_batch = False                                │ │
│ │                          features = None                                 │ │
│ │                         fn_kwargs = {}                                   │ │
│ │                          function = <function                            │ │
│ │                                     preprocess.<locals>.tokenize_fn at   │ │
│ │                                     0x7f45bc6e3e50>                      │ │
│ │                                 i = 79000                                │ │
│ │                           indices = [                                    │ │
│ │                                     │   79000,                           │ │
│ │                                     │   79001,                           │ │
│ │                                     │   79002,                           │ │
│ │                                     │   79003,                           │ │
│ │                                     │   79004,                           │ │
│ │                                     │   79005,                           │ │
│ │                                     │   79006,                           │ │
│ │                                     │   79007,                           │ │
│ │                                     │   79008,                           │ │
│ │                                     │   79009,                           │ │
│ │                                     │   ... +990                         │ │
│ │                                     ]                                    │ │
│ │            init_buffer_and_writer = <function                            │ │
│ │                                     Dataset._map_single.<locals>.init_b… │ │
│ │                                     at 0x7f45bc6c0550>                   │ │
│ │                     input_columns = None                                 │ │
│ │                     input_dataset = Dataset({                            │ │
│ │                                     │   features: ['train',              │ │
│ │                                     'validation'],                       │ │
│ │                                     │   num_rows: 813306                 │ │
│ │                                     })                                   │ │
│ │                    keep_in_memory = False                                │ │
│ │              load_from_cache_file = True                                 │ │
│ │                   new_fingerprint = '4608445b9d629a4f'                   │ │
│ │                          num_rows = 813306                               │ │
│ │          NumExamplesMismatchError = <class                               │ │
│ │                                     'datasets.arrow_dataset.Dataset._ma… │ │
│ │                            offset = 0                                    │ │
│ │                              pbar = <tqdm.asyncio.tqdm_asyncio object at │ │
│ │                                     0x7f467de30a30>                      │ │
│ │                         pbar_desc = 'Running tokenizer on dataset'       │ │
│ │                     pbar_iterable = <zip object at 0x7f467820e4c0>       │ │
│ │                        pbar_total = 814                                  │ │
│ │                         pbar_unit = 'ba'                                 │ │
│ │                              rank = None                                 │ │
│ │                    remove_columns = None                                 │ │
│ │                              self = Dataset({                            │ │
│ │                                     │   features: ['train',              │ │
│ │                                     'validation'],                       │ │
│ │                                     │   num_rows: 813306                 │ │
│ │                                     })                                   │ │
│ │                             stack = <contextlib.ExitStack object at      │ │
│ │                                     0x7f467de301f0>                      │ │
│ │                          tmp_file = <tempfile._TemporaryFileWrapper      │ │
│ │                                     object at 0x7f467de12220>            │ │
│ │                       update_data = True                                 │ │
│ │          validate_function_output = <function                            │ │
│ │                                     Dataset._map_single.<locals>.valida… │ │
│ │                                     at 0x7f45bc6e3dc0>                   │ │
│ │                      with_indices = False                                │ │
│ │                         with_rank = False                                │ │
│ │                            writer = <datasets.arrow_writer.ArrowWriter   │ │
│ │                                     object at 0x7f467ddb13d0>            │ │
│ │                 writer_batch_size = 1000                                 │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/arrow_dataset.py:2865 in apply_function_on_filtered_inputs   │
│                                                                              │
│   2862 │   │   │   │   additional_args += (effective_indices,)               │
│   2863 │   │   │   if with_rank:                                             │
│   2864 │   │   │   │   additional_args += (rank,)                            │
│ ❱ 2865 │   │   │   processed_inputs = function(*fn_args, *additional_args, * │
│   2866 │   │   │   if update_data is None:                                   │
│   2867 │   │   │   │   # Check if the function returns updated examples      │
│   2868 │   │   │   │   update_data = isinstance(processed_inputs, (Mapping,  │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │          additional_args = ()                                            │ │
│ │  check_same_num_examples = False                                         │ │
│ │        effective_indices = [                                             │ │
│ │                            │   79000,                                    │ │
│ │                            │   79001,                                    │ │
│ │                            │   79002,                                    │ │
│ │                            │   79003,                                    │ │
│ │                            │   79004,                                    │ │
│ │                            │   79005,                                    │ │
│ │                            │   79006,                                    │ │
│ │                            │   79007,                                    │ │
│ │                            │   79008,                                    │ │
│ │                            │   79009,                                    │ │
│ │                            │   ... +990                                  │ │
│ │                            ]                                             │ │
│ │                  fn_args = [                                             │ │
│ │                            │   {                                         │ │
│ │                            │   │   'train': [                            │ │
│ │                            │   │   │   'upon better and\\nnobler views;  │ │
│ │                            and advise your elder sisters, when they      │ │
│ │                            perceive'+1575,                               │ │
│ │                            │   │   │   "log time reviewing U.S. Army     │ │
│ │                            training films from the Vietnam era. That's   │ │
│ │                            the mil"+1355,                                │ │
│ │                            │   │   │   'the speculative philosophers.    │ │
│ │                            For example, the great mathematician Euler,   │ │
│ │                            who do'+1567,                                 │ │
│ │                            │   │   │   'the bedchamber. A short while    │ │
│ │                            later he confided in one of his servants,     │ │
│ │                            secretly'+1331,                               │ │
│ │                            │   │   │   'the familiar bond valuation      │ │
│ │                            equation. Assuming semiannual coupon          │ │
│ │                            payments, the e'+1402,                        │ │
│ │                            │   │   │   '$X$ associated with              │ │
│ │                            $U\\\\bar{U}$ and $\\\\bar{D}{D}$ since this  │ │
│ │                            would drop the poss'+1860,                    │ │
│ │                            │   │   │   'parking lot                      │ │
│ │                            dings.\\"\\n\\n\\"When these little charges   │ │
│ │                            blow, they\'ll leave M16 impa'+1470,          │ │
│ │                            │   │   │   'above from Helms, \\"The         │ │
│ │                            Indians,\\" pp. 37–45. \\n | Bruhns, _Ancient │ │
│ │                            South Americ'+1388,                           │ │
│ │                            │   │   │   'get some sleep, so I removed the │ │
│ │                            light bulb and stored it in the pocket of the │ │
│ │                            C'+1301,                                      │ │
│ │                            │   │   │   'set of ADE20K in Table           │ │
│ │                            \\\\[tab:baseline\\\\]. All our results       │ │
│ │                            except the last-row o'+1732,                  │ │
│ │                            │   │   │   ... +990                          │ │
│ │                            │   │   ],                                    │ │
│ │                            │   │   'validation': [                       │ │
│ │                            │   │   │   "given we can't stop people       │ │
│ │                            signing up again with a new disposable email  │ │
│ │                            address)"+1984,                               │ │
│ │                            │   │   │   'I\'m just repeating\' stuff.\\"  │ │
│ │                            \\"Did I hear you say you\'re looking for an  │ │
│ │                            apartmen'+1451,                               │ │
│ │                            │   │   │   'a model for belief. In M. Vargas │ │
│ │                            \\u0026 G. Yaffe (Eds.), Rational and social  │ │
│ │                            age'+1793,                                    │ │
│ │                            │   │   │   'away.\\"\\n\\n\\"Somehow, I      │ │
│ │                            think she knows where he is,\\" Virginia      │ │
│ │                            said, thoughtful'+1451,                       │ │
│ │                            │   │   │   'plane the limits of validity of  │ │
│ │                            our model.\\n\\n![(Color online)              │ │
│ │                            [*Characterizatio'+2041,                      │ │
│ │                            │   │   │   "son for fighting\\nagainst the   │ │
│ │                            king and the mother country. The old lady's   │ │
│ │                            face w"+1653,                                 │ │
│ │                            │   │   │   'again.\\" \\"We\'re gonna have   │ │
│ │                            to make the cut now.\\" \\"Sharon?\\" \\"can  │ │
│ │                            you hear me'+1554,                            │ │
│ │                            │   │   │   'even have more commissions such  │ │
│ │                            as yours.\\"\\n\\nThat was a not-so-veiled    │ │
│ │                            referenc'+1491,                               │ │
│ │                            │   │   │   'stealth technology we were never │ │
│ │                            able to break.\\" \\"So how come all of a     │ │
│ │                            sudden '+1534,                                │ │
│ │                            │   │   │   'create\\na small example         │ │
│ │                            service.\\nI want to focus on two major       │ │
│ │                            requirements.\\nPe'+1838,                     │ │
│ │                            │   │   │   ... +990                          │ │
│ │                            │   │   ]                                     │ │
│ │                            │   }                                         │ │
│ │                            ]                                             │ │
│ │                fn_kwargs = {}                                            │ │
│ │                 function = <function preprocess.<locals>.tokenize_fn at  │ │
│ │                            0x7f45bc6e3e50>                               │ │
│ │                  indices = [                                             │ │
│ │                            │   79000,                                    │ │
│ │                            │   79001,                                    │ │
│ │                            │   79002,                                    │ │
│ │                            │   79003,                                    │ │
│ │                            │   79004,                                    │ │
│ │                            │   79005,                                    │ │
│ │                            │   79006,                                    │ │
│ │                            │   79007,                                    │ │
│ │                            │   79008,                                    │ │
│ │                            │   79009,                                    │ │
│ │                            │   ... +990                                  │ │
│ │                            ]                                             │ │
│ │            input_columns = None                                          │ │
│ │                   inputs = {                                             │ │
│ │                            │   'train': [                                │ │
│ │                            │   │   'upon better and\\nnobler views; and  │ │
│ │                            advise your elder sisters, when they          │ │
│ │                            perceive'+1575,                               │ │
│ │                            │   │   "log time reviewing U.S. Army         │ │
│ │                            training films from the Vietnam era. That's   │ │
│ │                            the mil"+1355,                                │ │
│ │                            │   │   'the speculative philosophers. For    │ │
│ │                            example, the great mathematician Euler, who   │ │
│ │                            do'+1567,                                     │ │
│ │                            │   │   'the bedchamber. A short while later  │ │
│ │                            he confided in one of his servants,           │ │
│ │                            secretly'+1331,                               │ │
│ │                            │   │   'the familiar bond valuation          │ │
│ │                            equation. Assuming semiannual coupon          │ │
│ │                            payments, the e'+1402,                        │ │
│ │                            │   │   '$X$ associated with $U\\\\bar{U}$    │ │
│ │                            and $\\\\bar{D}{D}$ since this would drop the │ │
│ │                            poss'+1860,                                   │ │
│ │                            │   │   'parking lot dings.\\"\\n\\n\\"When   │ │
│ │                            these little charges blow, they\'ll leave M16 │ │
│ │                            impa'+1470,                                   │ │
│ │                            │   │   'above from Helms, \\"The Indians,\\" │ │
│ │                            pp. 37–45. \\n | Bruhns, _Ancient South       │ │
│ │                            Americ'+1388,                                 │ │
│ │                            │   │   'get some sleep, so I removed the     │ │
│ │                            light bulb and stored it in the pocket of the │ │
│ │                            C'+1301,                                      │ │
│ │                            │   │   'set of ADE20K in Table               │ │
│ │                            \\\\[tab:baseline\\\\]. All our results       │ │
│ │                            except the last-row o'+1732,                  │ │
│ │                            │   │   ... +990                              │ │
│ │                            │   ],                                        │ │
│ │                            │   'validation': [                           │ │
│ │                            │   │   "given we can't stop people signing   │ │
│ │                            up again with a new disposable email          │ │
│ │                            address)"+1984,                               │ │
│ │                            │   │   'I\'m just repeating\' stuff.\\"      │ │
│ │                            \\"Did I hear you say you\'re looking for an  │ │
│ │                            apartmen'+1451,                               │ │
│ │                            │   │   'a model for belief. In M. Vargas     │ │
│ │                            \\u0026 G. Yaffe (Eds.), Rational and social  │ │
│ │                            age'+1793,                                    │ │
│ │                            │   │   'away.\\"\\n\\n\\"Somehow, I think    │ │
│ │                            she knows where he is,\\" Virginia said,      │ │
│ │                            thoughtful'+1451,                             │ │
│ │                            │   │   'plane the limits of validity of our  │ │
│ │                            model.\\n\\n![(Color online)                  │ │
│ │                            [*Characterizatio'+2041,                      │ │
│ │                            │   │   "son for fighting\\nagainst the king  │ │
│ │                            and the mother country. The old lady's face   │ │
│ │                            w"+1653,                                      │ │
│ │                            │   │   'again.\\" \\"We\'re gonna have to    │ │
│ │                            make the cut now.\\" \\"Sharon?\\" \\"can you │ │
│ │                            hear me'+1554,                                │ │
│ │                            │   │   'even have more commissions such as   │ │
│ │                            yours.\\"\\n\\nThat was a not-so-veiled       │ │
│ │                            referenc'+1491,                               │ │
│ │                            │   │   'stealth technology we were never     │ │
│ │                            able to break.\\" \\"So how come all of a     │ │
│ │                            sudden '+1534,                                │ │
│ │                            │   │   'create\\na small example             │ │
│ │                            service.\\nI want to focus on two major       │ │
│ │                            requirements.\\nPe'+1838,                     │ │
│ │                            │   │   ... +990                              │ │
│ │                            │   ]                                         │ │
│ │                            }                                             │ │
│ │ NumExamplesMismatchError = <class                                        │ │
│ │                            'datasets.arrow_dataset.Dataset._map_single.… │ │
│ │                   offset = 0                                             │ │
│ │                     rank = None                                          │ │
│ │           remove_columns = None                                          │ │
│ │                     self = Dataset({                                     │ │
│ │                            │   features: ['train', 'validation'],        │ │
│ │                            │   num_rows: 813306                          │ │
│ │                            })                                            │ │
│ │              update_data = True                                          │ │
│ │ validate_function_output = <function                                     │ │
│ │                            Dataset._map_single.<locals>.validate_functi… │ │
│ │                            at 0x7f45bc6e3dc0>                            │ │
│ │             with_indices = False                                         │ │
│ │                with_rank = False                                         │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/arrow_dataset.py:2545 in decorated                           │
│                                                                              │
│   2542 │   │   │   │   │   Example(item, features=self.features) if not batc │
│   2543 │   │   │   │   )                                                     │
│   2544 │   │   │   │   # Use the LazyDict internally, while mapping the func │
│ ❱ 2545 │   │   │   │   result = f(decorated_item, *args, **kwargs)           │
│   2546 │   │   │   │   # Return a standard dict                              │
│   2547 │   │   │   │   return result.data if isinstance(result, LazyDict) el │
│   2548                                                                       │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │           args = ()                                                      │ │
│ │        batched = True                                                    │ │
│ │ decorated_item = {                                                       │ │
│ │                  │   'train': [                                          │ │
│ │                  │   │   'upon better and\\nnobler views; and advise     │ │
│ │                  your elder sisters, when they perceive'+1575,           │ │
│ │                  │   │   "log time reviewing U.S. Army training films    │ │
│ │                  from the Vietnam era. That's the mil"+1355,             │ │
│ │                  │   │   'the speculative philosophers. For example, the │ │
│ │                  great mathematician Euler, who do'+1567,                │ │
│ │                  │   │   'the bedchamber. A short while later he         │ │
│ │                  confided in one of his servants, secretly'+1331,        │ │
│ │                  │   │   'the familiar bond valuation equation. Assuming │ │
│ │                  semiannual coupon payments, the e'+1402,                │ │
│ │                  │   │   '$X$ associated with $U\\\\bar{U}$ and          │ │
│ │                  $\\\\bar{D}{D}$ since this would drop the poss'+1860,   │ │
│ │                  │   │   'parking lot dings.\\"\\n\\n\\"When these       │ │
│ │                  little charges blow, they\'ll leave M16 impa'+1470,     │ │
│ │                  │   │   'above from Helms, \\"The Indians,\\" pp.       │ │
│ │                  37–45. \\n | Bruhns, _Ancient South Americ'+1388,       │ │
│ │                  │   │   'get some sleep, so I removed the light bulb    │ │
│ │                  and stored it in the pocket of the C'+1301,             │ │
│ │                  │   │   'set of ADE20K in Table \\\\[tab:baseline\\\\]. │ │
│ │                  All our results except the last-row o'+1732,            │ │
│ │                  │   │   ... +990                                        │ │
│ │                  │   ],                                                  │ │
│ │                  │   'validation': [                                     │ │
│ │                  │   │   "given we can't stop people signing up again    │ │
│ │                  with a new disposable email address)"+1984,             │ │
│ │                  │   │   'I\'m just repeating\' stuff.\\" \\"Did I hear  │ │
│ │                  you say you\'re looking for an apartmen'+1451,          │ │
│ │                  │   │   'a model for belief. In M. Vargas \\u0026 G.    │ │
│ │                  Yaffe (Eds.), Rational and social age'+1793,            │ │
│ │                  │   │   'away.\\"\\n\\n\\"Somehow, I think she knows    │ │
│ │                  where he is,\\" Virginia said, thoughtful'+1451,        │ │
│ │                  │   │   'plane the limits of validity of our            │ │
│ │                  model.\\n\\n![(Color online) [*Characterizatio'+2041,   │ │
│ │                  │   │   "son for fighting\\nagainst the king and the    │ │
│ │                  mother country. The old lady's face w"+1653,            │ │
│ │                  │   │   'again.\\" \\"We\'re gonna have to make the cut │ │
│ │                  now.\\" \\"Sharon?\\" \\"can you hear me'+1554,         │ │
│ │                  │   │   'even have more commissions such as             │ │
│ │                  yours.\\"\\n\\nThat was a not-so-veiled referenc'+1491, │ │
│ │                  │   │   'stealth technology we were never able to       │ │
│ │                  break.\\" \\"So how come all of a sudden '+1534,        │ │
│ │                  │   │   'create\\na small example service.\\nI want to  │ │
│ │                  focus on two major requirements.\\nPe'+1838,            │ │
│ │                  │   │   ... +990                                        │ │
│ │                  │   ]                                                   │ │
│ │                  }                                                       │ │
│ │              f = <function preprocess.<locals>.tokenize_fn at            │ │
│ │                  0x7f4678211670>                                         │ │
│ │           item = {                                                       │ │
│ │                  │   'train': [                                          │ │
│ │                  │   │   'upon better and\\nnobler views; and advise     │ │
│ │                  your elder sisters, when they perceive'+1575,           │ │
│ │                  │   │   "log time reviewing U.S. Army training films    │ │
│ │                  from the Vietnam era. That's the mil"+1355,             │ │
│ │                  │   │   'the speculative philosophers. For example, the │ │
│ │                  great mathematician Euler, who do'+1567,                │ │
│ │                  │   │   'the bedchamber. A short while later he         │ │
│ │                  confided in one of his servants, secretly'+1331,        │ │
│ │                  │   │   'the familiar bond valuation equation. Assuming │ │
│ │                  semiannual coupon payments, the e'+1402,                │ │
│ │                  │   │   '$X$ associated with $U\\\\bar{U}$ and          │ │
│ │                  $\\\\bar{D}{D}$ since this would drop the poss'+1860,   │ │
│ │                  │   │   'parking lot dings.\\"\\n\\n\\"When these       │ │
│ │                  little charges blow, they\'ll leave M16 impa'+1470,     │ │
│ │                  │   │   'above from Helms, \\"The Indians,\\" pp.       │ │
│ │                  37–45. \\n | Bruhns, _Ancient South Americ'+1388,       │ │
│ │                  │   │   'get some sleep, so I removed the light bulb    │ │
│ │                  and stored it in the pocket of the C'+1301,             │ │
│ │                  │   │   'set of ADE20K in Table \\\\[tab:baseline\\\\]. │ │
│ │                  All our results except the last-row o'+1732,            │ │
│ │                  │   │   ... +990                                        │ │
│ │                  │   ],                                                  │ │
│ │                  │   'validation': [                                     │ │
│ │                  │   │   "given we can't stop people signing up again    │ │
│ │                  with a new disposable email address)"+1984,             │ │
│ │                  │   │   'I\'m just repeating\' stuff.\\" \\"Did I hear  │ │
│ │                  you say you\'re looking for an apartmen'+1451,          │ │
│ │                  │   │   'a model for belief. In M. Vargas \\u0026 G.    │ │
│ │                  Yaffe (Eds.), Rational and social age'+1793,            │ │
│ │                  │   │   'away.\\"\\n\\n\\"Somehow, I think she knows    │ │
│ │                  where he is,\\" Virginia said, thoughtful'+1451,        │ │
│ │                  │   │   'plane the limits of validity of our            │ │
│ │                  model.\\n\\n![(Color online) [*Characterizatio'+2041,   │ │
│ │                  │   │   "son for fighting\\nagainst the king and the    │ │
│ │                  mother country. The old lady's face w"+1653,            │ │
│ │                  │   │   'again.\\" \\"We\'re gonna have to make the cut │ │
│ │                  now.\\" \\"Sharon?\\" \\"can you hear me'+1554,         │ │
│ │                  │   │   'even have more commissions such as             │ │
│ │                  yours.\\"\\n\\nThat was a not-so-veiled referenc'+1491, │ │
│ │                  │   │   'stealth technology we were never able to       │ │
│ │                  break.\\" \\"So how come all of a sudden '+1534,        │ │
│ │                  │   │   'create\\na small example service.\\nI want to  │ │
│ │                  focus on two major requirements.\\nPe'+1838,            │ │
│ │                  │   │   ... +990                                        │ │
│ │                  │   ]                                                   │ │
│ │                  }                                                       │ │
│ │         kwargs = {}                                                      │ │
│ │           self = Dataset({                                               │ │
│ │                  │   features: ['train', 'validation'],                  │ │
│ │                  │   num_rows: 813306                                    │ │
│ │                  })                                                      │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/finetune_using_clm_wandb.p │
│ y:204 in tokenize_fn                                                         │
│                                                                              │
│   201 │   │   return result                                                  │
│   202 │                                                                      │
│   203 │   def tokenize_fn(examples):                                         │
│ ❱ 204 │   │   result = tokenizer(                                            │
│   205 │   │   │   examples[text_column_name],                                │
│   206 │   │   │   padding=pad,                                               │
│   207 │   │   │   truncation=True,                                           │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │              cfg = {'output_dir': 'tuned-model', 'bittensor':            │ │
│ │                    {'network': 'nobunaga'}, 'dataset': {'name':          │ │
│ │                    'ViktorThink/mountain_combined_813306',               │ │
│ │                    'config_name': None, 'num_batches': 5000,             │ │
│ │                    'block_size': 256, 'overwrite_cache': False,          │ │
│ │                    'keep_linebreaks': True, 'concatenate_raw': False,    │ │
│ │                    'load_tokenized_data': False}, 'model': {'name':      │ │
│ │                    'facebook/opt-2.7b', 'config_name': None},            │ │
│ │                    'tokenizer': {'name': None, 'use_fast': True,         │ │
│ │                    'preprocessing_num_workers': None, 'pad_token':       │ │
│ │                    '[PAD]'}, 'training': {'seed': 17,                    │ │
│ │                    'val_split_percent': 20, 'train_batch_size': 32,      │ │
│ │                    'eval_batch_size': 16, 'learning_rate': 3e-06,        │ │
│ │                    'weight_decay': 0.05, 'num_epochs': 4,                │ │
│ │                    'max_train_steps': None,                              │ │
│ │                    'gradient_accumulation_steps': 2, 'lr_scheduler':     │ │
│ │                    'constant', 'lr_warmup_steps': 5, 'eval_every': 250,  │ │
│ │                    'max_eval_steps': 500, 'checkpoint':                  │ │
│ │                    {'resume_from_checkpoint': 0, 'every_n_steps':        │ │
│ │                    None}}, 'tracking': {'enabled': True, 'report_to':    │ │
│ │                    'all'}, 'testing': {'enabled': False}}                │ │
│ │         examples = {                                                     │ │
│ │                    │   'train': [                                        │ │
│ │                    │   │   'upon better and\\nnobler views; and advise   │ │
│ │                    your elder sisters, when they perceive'+1575,         │ │
│ │                    │   │   "log time reviewing U.S. Army training films  │ │
│ │                    from the Vietnam era. That's the mil"+1355,           │ │
│ │                    │   │   'the speculative philosophers. For example,   │ │
│ │                    the great mathematician Euler, who do'+1567,          │ │
│ │                    │   │   'the bedchamber. A short while later he       │ │
│ │                    confided in one of his servants, secretly'+1331,      │ │
│ │                    │   │   'the familiar bond valuation equation.        │ │
│ │                    Assuming semiannual coupon payments, the e'+1402,     │ │
│ │                    │   │   '$X$ associated with $U\\\\bar{U}$ and        │ │
│ │                    $\\\\bar{D}{D}$ since this would drop the poss'+1860, │ │
│ │                    │   │   'parking lot dings.\\"\\n\\n\\"When these     │ │
│ │                    little charges blow, they\'ll leave M16 impa'+1470,   │ │
│ │                    │   │   'above from Helms, \\"The Indians,\\" pp.     │ │
│ │                    37–45. \\n | Bruhns, _Ancient South Americ'+1388,     │ │
│ │                    │   │   'get some sleep, so I removed the light bulb  │ │
│ │                    and stored it in the pocket of the C'+1301,           │ │
│ │                    │   │   'set of ADE20K in Table                       │ │
│ │                    \\\\[tab:baseline\\\\]. All our results except the    │ │
│ │                    last-row o'+1732,                                     │ │
│ │                    │   │   ... +990                                      │ │
│ │                    │   ],                                                │ │
│ │                    │   'validation': [                                   │ │
│ │                    │   │   "given we can't stop people signing up again  │ │
│ │                    with a new disposable email address)"+1984,           │ │
│ │                    │   │   'I\'m just repeating\' stuff.\\" \\"Did I     │ │
│ │                    hear you say you\'re looking for an apartmen'+1451,   │ │
│ │                    │   │   'a model for belief. In M. Vargas \\u0026 G.  │ │
│ │                    Yaffe (Eds.), Rational and social age'+1793,          │ │
│ │                    │   │   'away.\\"\\n\\n\\"Somehow, I think she knows  │ │
│ │                    where he is,\\" Virginia said, thoughtful'+1451,      │ │
│ │                    │   │   'plane the limits of validity of our          │ │
│ │                    model.\\n\\n![(Color online) [*Characterizatio'+2041, │ │
│ │                    │   │   "son for fighting\\nagainst the king and the  │ │
│ │                    mother country. The old lady's face w"+1653,          │ │
│ │                    │   │   'again.\\" \\"We\'re gonna have to make the   │ │
│ │                    cut now.\\" \\"Sharon?\\" \\"can you hear me'+1554,   │ │
│ │                    │   │   'even have more commissions such as           │ │
│ │                    yours.\\"\\n\\nThat was a not-so-veiled               │ │
│ │                    referenc'+1491,                                       │ │
│ │                    │   │   'stealth technology we were never able to     │ │
│ │                    break.\\" \\"So how come all of a sudden '+1534,      │ │
│ │                    │   │   'create\\na small example service.\\nI want   │ │
│ │                    to focus on two major requirements.\\nPe'+1838,       │ │
│ │                    │   │   ... +990                                      │ │
│ │                    │   ]                                                 │ │
│ │                    }                                                     │ │
│ │              pad = 'max_length'                                          │ │
│ │ text_column_name = 'train'                                               │ │
│ │        tokenizer = PreTrainedTokenizer(name_or_path='facebook/opt-2.7b', │ │
│ │                    vocab_size=50265,                                     │ │
│ │                    model_max_len=1000000000000000019884624838656,        │ │
│ │                    is_fast=False, padding_side='right',                  │ │
│ │                    truncation_side='right', special_tokens={'bos_token': │ │
│ │                    AddedToken("</s>", rstrip=False, lstrip=False,        │ │
│ │                    single_word=False, normalized=True), 'eos_token':     │ │
│ │                    AddedToken("</s>", rstrip=False, lstrip=False,        │ │
│ │                    single_word=False, normalized=True), 'unk_token':     │ │
│ │                    AddedToken("</s>", rstrip=False, lstrip=False,        │ │
│ │                    single_word=False, normalized=True), 'pad_token':     │ │
│ │                    AddedToken("<pad>", rstrip=False, lstrip=False,       │ │
│ │                    single_word=False, normalized=True)})                 │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/transformers/tokenization_utils_base.py:2488 in __call__              │
│                                                                              │
│   2485 │   │   │   # input mode in this case.                                │
│   2486 │   │   │   if not self._in_target_context_manager:                   │
│   2487 │   │   │   │   self._switch_to_input_mode()                          │
│ ❱ 2488 │   │   │   encodings = self._call_one(text=text, text_pair=text_pair │
│   2489 │   │   if text_target is not None:                                   │
│   2490 │   │   │   self._switch_to_target_mode()                             │
│   2491 │   │   │   target_encodings = self._call_one(text=text_target, text_ │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │         add_special_tokens = True                                        │ │
│ │                 all_kwargs = {                                           │ │
│ │                              │   'add_special_tokens': True,             │ │
│ │                              │   'padding': 'max_length',                │ │
│ │                              │   'truncation': True,                     │ │
│ │                              │   'max_length': 256,                      │ │
│ │                              │   'stride': 0,                            │ │
│ │                              │   'is_split_into_words': False,           │ │
│ │                              │   'pad_to_multiple_of': None,             │ │
│ │                              │   'return_tensors': None,                 │ │
│ │                              │   'return_token_type_ids': None,          │ │
│ │                              │   'return_attention_mask': None,          │ │
│ │                              │   ... +5                                  │ │
│ │                              }                                           │ │
│ │        is_split_into_words = False                                       │ │
│ │                     kwargs = {}                                          │ │
│ │                 max_length = 256                                         │ │
│ │         pad_to_multiple_of = None                                        │ │
│ │                    padding = 'max_length'                                │ │
│ │      return_attention_mask = None                                        │ │
│ │              return_length = False                                       │ │
│ │     return_offsets_mapping = False                                       │ │
│ │  return_overflowing_tokens = False                                       │ │
│ │ return_special_tokens_mask = False                                       │ │
│ │             return_tensors = None                                        │ │
│ │      return_token_type_ids = None                                        │ │
│ │                       self = PreTrainedTokenizer(name_or_path='facebook… │ │
│ │                              vocab_size=50265,                           │ │
│ │                              model_max_len=1000000000000000019884624838… │ │
│ │                              is_fast=False, padding_side='right',        │ │
│ │                              truncation_side='right',                    │ │
│ │                              special_tokens={'bos_token':                │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'eos_token':              │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'unk_token':              │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'pad_token':              │ │
│ │                              AddedToken("<pad>", rstrip=False,           │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True)})                          │ │
│ │                     stride = 0                                           │ │
│ │                       text = [                                           │ │
│ │                              │   'upon better and\\nnobler views; and    │ │
│ │                              advise your elder sisters, when they        │ │
│ │                              perceive'+1575,                             │ │
│ │                              │   "log time reviewing U.S. Army training  │ │
│ │                              films from the Vietnam era. That's the      │ │
│ │                              mil"+1355,                                  │ │
│ │                              │   'the speculative philosophers. For      │ │
│ │                              example, the great mathematician Euler, who │ │
│ │                              do'+1567,                                   │ │
│ │                              │   'the bedchamber. A short while later he │ │
│ │                              confided in one of his servants,            │ │
│ │                              secretly'+1331,                             │ │
│ │                              │   'the familiar bond valuation equation.  │ │
│ │                              Assuming semiannual coupon payments, the    │ │
│ │                              e'+1402,                                    │ │
│ │                              │   '$X$ associated with $U\\\\bar{U}$ and  │ │
│ │                              $\\\\bar{D}{D}$ since this would drop the   │ │
│ │                              poss'+1860,                                 │ │
│ │                              │   'parking lot dings.\\"\\n\\n\\"When     │ │
│ │                              these little charges blow, they\'ll leave   │ │
│ │                              M16 impa'+1470,                             │ │
│ │                              │   'above from Helms, \\"The Indians,\\"   │ │
│ │                              pp. 37–45. \\n | Bruhns, _Ancient South     │ │
│ │                              Americ'+1388,                               │ │
│ │                              │   'get some sleep, so I removed the light │ │
│ │                              bulb and stored it in the pocket of the     │ │
│ │                              C'+1301,                                    │ │
│ │                              │   'set of ADE20K in Table                 │ │
│ │                              \\\\[tab:baseline\\\\]. All our results     │ │
│ │                              except the last-row o'+1732,                │ │
│ │                              │   ... +990                                │ │
│ │                              ]                                           │ │
│ │                  text_pair = None                                        │ │
│ │           text_pair_target = None                                        │ │
│ │                text_target = None                                        │ │
│ │                 truncation = True                                        │ │
│ │                    verbose = True                                        │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/transformers/tokenization_utils_base.py:2574 in _call_one             │
│                                                                              │
│   2571 │   │   │   │   │   f" {len(text_pair)}."                             │
│   2572 │   │   │   │   )                                                     │
│   2573 │   │   │   batch_text_or_text_pairs = list(zip(text, text_pair)) if  │
│ ❱ 2574 │   │   │   return self.batch_encode_plus(                            │
│   2575 │   │   │   │   batch_text_or_text_pairs=batch_text_or_text_pairs,    │
│   2576 │   │   │   │   add_special_tokens=add_special_tokens,                │
│   2577 │   │   │   │   padding=padding,                                      │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │       _is_valid_text_input = <function                                   │ │
│ │                              PreTrainedTokenizerBase._call_one.<locals>… │ │
│ │                              at 0x7f459958b9d0>                          │ │
│ │         add_special_tokens = True                                        │ │
│ │   batch_text_or_text_pairs = [                                           │ │
│ │                              │   'upon better and\\nnobler views; and    │ │
│ │                              advise your elder sisters, when they        │ │
│ │                              perceive'+1575,                             │ │
│ │                              │   "log time reviewing U.S. Army training  │ │
│ │                              films from the Vietnam era. That's the      │ │
│ │                              mil"+1355,                                  │ │
│ │                              │   'the speculative philosophers. For      │ │
│ │                              example, the great mathematician Euler, who │ │
│ │                              do'+1567,                                   │ │
│ │                              │   'the bedchamber. A short while later he │ │
│ │                              confided in one of his servants,            │ │
│ │                              secretly'+1331,                             │ │
│ │                              │   'the familiar bond valuation equation.  │ │
│ │                              Assuming semiannual coupon payments, the    │ │
│ │                              e'+1402,                                    │ │
│ │                              │   '$X$ associated with $U\\\\bar{U}$ and  │ │
│ │                              $\\\\bar{D}{D}$ since this would drop the   │ │
│ │                              poss'+1860,                                 │ │
│ │                              │   'parking lot dings.\\"\\n\\n\\"When     │ │
│ │                              these little charges blow, they\'ll leave   │ │
│ │                              M16 impa'+1470,                             │ │
│ │                              │   'above from Helms, \\"The Indians,\\"   │ │
│ │                              pp. 37–45. \\n | Bruhns, _Ancient South     │ │
│ │                              Americ'+1388,                               │ │
│ │                              │   'get some sleep, so I removed the light │ │
│ │                              bulb and stored it in the pocket of the     │ │
│ │                              C'+1301,                                    │ │
│ │                              │   'set of ADE20K in Table                 │ │
│ │                              \\\\[tab:baseline\\\\]. All our results     │ │
│ │                              except the last-row o'+1732,                │ │
│ │                              │   ... +990                                │ │
│ │                              ]                                           │ │
│ │                 is_batched = True                                        │ │
│ │        is_split_into_words = False                                       │ │
│ │                     kwargs = {}                                          │ │
│ │                 max_length = 256                                         │ │
│ │         pad_to_multiple_of = None                                        │ │
│ │                    padding = 'max_length'                                │ │
│ │      return_attention_mask = None                                        │ │
│ │              return_length = False                                       │ │
│ │     return_offsets_mapping = False                                       │ │
│ │  return_overflowing_tokens = False                                       │ │
│ │ return_special_tokens_mask = False                                       │ │
│ │             return_tensors = None                                        │ │
│ │      return_token_type_ids = None                                        │ │
│ │                       self = PreTrainedTokenizer(name_or_path='facebook… │ │
│ │                              vocab_size=50265,                           │ │
│ │                              model_max_len=1000000000000000019884624838… │ │
│ │                              is_fast=False, padding_side='right',        │ │
│ │                              truncation_side='right',                    │ │
│ │                              special_tokens={'bos_token':                │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'eos_token':              │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'unk_token':              │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'pad_token':              │ │
│ │                              AddedToken("<pad>", rstrip=False,           │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True)})                          │ │
│ │                     stride = 0                                           │ │
│ │                       text = [                                           │ │
│ │                              │   'upon better and\\nnobler views; and    │ │
│ │                              advise your elder sisters, when they        │ │
│ │                              perceive'+1575,                             │ │
│ │                              │   "log time reviewing U.S. Army training  │ │
│ │                              films from the Vietnam era. That's the      │ │
│ │                              mil"+1355,                                  │ │
│ │                              │   'the speculative philosophers. For      │ │
│ │                              example, the great mathematician Euler, who │ │
│ │                              do'+1567,                                   │ │
│ │                              │   'the bedchamber. A short while later he │ │
│ │                              confided in one of his servants,            │ │
│ │                              secretly'+1331,                             │ │
│ │                              │   'the familiar bond valuation equation.  │ │
│ │                              Assuming semiannual coupon payments, the    │ │
│ │                              e'+1402,                                    │ │
│ │                              │   '$X$ associated with $U\\\\bar{U}$ and  │ │
│ │                              $\\\\bar{D}{D}$ since this would drop the   │ │
│ │                              poss'+1860,                                 │ │
│ │                              │   'parking lot dings.\\"\\n\\n\\"When     │ │
│ │                              these little charges blow, they\'ll leave   │ │
│ │                              M16 impa'+1470,                             │ │
│ │                              │   'above from Helms, \\"The Indians,\\"   │ │
│ │                              pp. 37–45. \\n | Bruhns, _Ancient South     │ │
│ │                              Americ'+1388,                               │ │
│ │                              │   'get some sleep, so I removed the light │ │
│ │                              bulb and stored it in the pocket of the     │ │
│ │                              C'+1301,                                    │ │
│ │                              │   'set of ADE20K in Table                 │ │
│ │                              \\\\[tab:baseline\\\\]. All our results     │ │
│ │                              except the last-row o'+1732,                │ │
│ │                              │   ... +990                                │ │
│ │                              ]                                           │ │
│ │                  text_pair = None                                        │ │
│ │                 truncation = True                                        │ │
│ │                    verbose = True                                        │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/transformers/tokenization_utils_base.py:2765 in batch_encode_plus     │
│                                                                              │
│   2762 │   │   │   **kwargs,                                                 │
│   2763 │   │   )                                                             │
│   2764 │   │                                                                 │
│ ❱ 2765 │   │   return self._batch_encode_plus(                               │
│   2766 │   │   │   batch_text_or_text_pairs=batch_text_or_text_pairs,        │
│   2767 │   │   │   add_special_tokens=add_special_tokens,                    │
│   2768 │   │   │   padding_strategy=padding_strategy,                        │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │         add_special_tokens = True                                        │ │
│ │   batch_text_or_text_pairs = [                                           │ │
│ │                              │   'upon better and\\nnobler views; and    │ │
│ │                              advise your elder sisters, when they        │ │
│ │                              perceive'+1575,                             │ │
│ │                              │   "log time reviewing U.S. Army training  │ │
│ │                              films from the Vietnam era. That's the      │ │
│ │                              mil"+1355,                                  │ │
│ │                              │   'the speculative philosophers. For      │ │
│ │                              example, the great mathematician Euler, who │ │
│ │                              do'+1567,                                   │ │
│ │                              │   'the bedchamber. A short while later he │ │
│ │                              confided in one of his servants,            │ │
│ │                              secretly'+1331,                             │ │
│ │                              │   'the familiar bond valuation equation.  │ │
│ │                              Assuming semiannual coupon payments, the    │ │
│ │                              e'+1402,                                    │ │
│ │                              │   '$X$ associated with $U\\\\bar{U}$ and  │ │
│ │                              $\\\\bar{D}{D}$ since this would drop the   │ │
│ │                              poss'+1860,                                 │ │
│ │                              │   'parking lot dings.\\"\\n\\n\\"When     │ │
│ │                              these little charges blow, they\'ll leave   │ │
│ │                              M16 impa'+1470,                             │ │
│ │                              │   'above from Helms, \\"The Indians,\\"   │ │
│ │                              pp. 37–45. \\n | Bruhns, _Ancient South     │ │
│ │                              Americ'+1388,                               │ │
│ │                              │   'get some sleep, so I removed the light │ │
│ │                              bulb and stored it in the pocket of the     │ │
│ │                              C'+1301,                                    │ │
│ │                              │   'set of ADE20K in Table                 │ │
│ │                              \\\\[tab:baseline\\\\]. All our results     │ │
│ │                              except the last-row o'+1732,                │ │
│ │                              │   ... +990                                │ │
│ │                              ]                                           │ │
│ │        is_split_into_words = False                                       │ │
│ │                     kwargs = {}                                          │ │
│ │                 max_length = 256                                         │ │
│ │         pad_to_multiple_of = None                                        │ │
│ │                    padding = 'max_length'                                │ │
│ │           padding_strategy = <PaddingStrategy.MAX_LENGTH: 'max_length'>  │ │
│ │      return_attention_mask = None                                        │ │
│ │              return_length = False                                       │ │
│ │     return_offsets_mapping = False                                       │ │
│ │  return_overflowing_tokens = False                                       │ │
│ │ return_special_tokens_mask = False                                       │ │
│ │             return_tensors = None                                        │ │
│ │      return_token_type_ids = None                                        │ │
│ │                       self = PreTrainedTokenizer(name_or_path='facebook… │ │
│ │                              vocab_size=50265,                           │ │
│ │                              model_max_len=1000000000000000019884624838… │ │
│ │                              is_fast=False, padding_side='right',        │ │
│ │                              truncation_side='right',                    │ │
│ │                              special_tokens={'bos_token':                │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'eos_token':              │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'unk_token':              │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'pad_token':              │ │
│ │                              AddedToken("<pad>", rstrip=False,           │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True)})                          │ │
│ │                     stride = 0                                           │ │
│ │                 truncation = True                                        │ │
│ │        truncation_strategy = <TruncationStrategy.LONGEST_FIRST:          │ │
│ │                              'longest_first'>                            │ │
│ │                    verbose = True                                        │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/transformers/tokenization_utils.py:733 in _batch_encode_plus          │
│                                                                              │
│   730 │   │   │   else:                                                      │
│   731 │   │   │   │   ids, pair_ids = ids_or_pair_ids                        │
│   732 │   │   │                                                              │
│ ❱ 733 │   │   │   first_ids = get_input_ids(ids)                             │
│   734 │   │   │   second_ids = get_input_ids(pair_ids) if pair_ids is not No │
│   735 │   │   │   input_ids.append((first_ids, second_ids))                  │
│   736                                                                        │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │         add_special_tokens = True                                        │ │
│ │   batch_text_or_text_pairs = [                                           │ │
│ │                              │   'upon better and\\nnobler views; and    │ │
│ │                              advise your elder sisters, when they        │ │
│ │                              perceive'+1575,                             │ │
│ │                              │   "log time reviewing U.S. Army training  │ │
│ │                              films from the Vietnam era. That's the      │ │
│ │                              mil"+1355,                                  │ │
│ │                              │   'the speculative philosophers. For      │ │
│ │                              example, the great mathematician Euler, who │ │
│ │                              do'+1567,                                   │ │
│ │                              │   'the bedchamber. A short while later he │ │
│ │                              confided in one of his servants,            │ │
│ │                              secretly'+1331,                             │ │
│ │                              │   'the familiar bond valuation equation.  │ │
│ │                              Assuming semiannual coupon payments, the    │ │
│ │                              e'+1402,                                    │ │
│ │                              │   '$X$ associated with $U\\\\bar{U}$ and  │ │
│ │                              $\\\\bar{D}{D}$ since this would drop the   │ │
│ │                              poss'+1860,                                 │ │
│ │                              │   'parking lot dings.\\"\\n\\n\\"When     │ │
│ │                              these little charges blow, they\'ll leave   │ │
│ │                              M16 impa'+1470,                             │ │
│ │                              │   'above from Helms, \\"The Indians,\\"   │ │
│ │                              pp. 37–45. \\n | Bruhns, _Ancient South     │ │
│ │                              Americ'+1388,                               │ │
│ │                              │   'get some sleep, so I removed the light │ │
│ │                              bulb and stored it in the pocket of the     │ │
│ │                              C'+1301,                                    │ │
│ │                              │   'set of ADE20K in Table                 │ │
│ │                              \\\\[tab:baseline\\\\]. All our results     │ │
│ │                              except the last-row o'+1732,                │ │
│ │                              │   ... +990                                │ │
│ │                              ]                                           │ │
│ │                  first_ids = [                                           │ │
│ │                              │   9226,                                   │ │
│ │                              │   169,                                    │ │
│ │                              │   13,                                     │ │
│ │                              │   47,                                     │ │
│ │                              │   4,                                      │ │
│ │                              │   3676,                                   │ │
│ │                              │   3809,                                   │ │
│ │                              │   7575,                                   │ │
│ │                              │   5,                                      │ │
│ │                              │   507,                                    │ │
│ │                              │   ... +326                                │ │
│ │                              ]                                           │ │
│ │              get_input_ids = <function                                   │ │
│ │                              PreTrainedTokenizer._batch_encode_plus.<lo… │ │
│ │                              at 0x7f459958bb80>                          │ │
│ │                        ids = 'Calibur. The anklebiter ignored him and    │ │
│ │                              kept furiously digging. Calibur took a      │ │
│ │                              k'+1342                                     │ │
│ │            ids_or_pair_ids = 'Calibur. The anklebiter ignored him and    │ │
│ │                              kept furiously digging. Calibur took a      │ │
│ │                              k'+1342                                     │ │
│ │                  input_ids = [                                           │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   32630,                          │ │
│ │                              │   │   │   357,                            │ │
│ │                              │   │   │   8,                              │ │
│ │                              │   │   │   37457,                          │ │
│ │                              │   │   │   15688,                          │ │
│ │                              │   │   │   2413,                           │ │
│ │                              │   │   │   1371,                           │ │
│ │                              │   │   │   2728,                           │ │
│ │                              │   │   │   131,                            │ │
│ │                              │   │   │   8,                              │ │
│ │                              │   │   │   ... +412                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   12376,                          │ │
│ │                              │   │   │   86,                             │ │
│ │                              │   │   │   9311,                           │ │
│ │                              │   │   │   121,                            │ │
│ │                              │   │   │   4,                              │ │
│ │                              │   │   │   104,                            │ │
│ │                              │   │   │   4,                              │ │
│ │                              │   │   │   2938,                           │ │
│ │                              │   │   │   1058,                           │ │
│ │                              │   │   │   3541,                           │ │
│ │                              │   │   │   ... +327                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   627,                            │ │
│ │                              │   │   │   21779,                          │ │
│ │                              │   │   │   44267,                          │ │
│ │                              │   │   │   4,                              │ │
│ │                              │   │   │   286,                            │ │
│ │                              │   │   │   1246,                           │ │
│ │                              │   │   │   6,                              │ │
│ │                              │   │   │   5,                              │ │
│ │                              │   │   │   372,                            │ │
│ │                              │   │   │   43027,                          │ │
│ │                              │   │   │   ... +315                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   627,                            │ │
│ │                              │   │   │   3267,                           │ │
│ │                              │   │   │   611,                            │ │
│ │                              │   │   │   19383,                          │ │
│ │                              │   │   │   4,                              │ │
│ │                              │   │   │   83,                             │ │
│ │                              │   │   │   765,                            │ │
│ │                              │   │   │   150,                            │ │
│ │                              │   │   │   423,                            │ │
│ │                              │   │   │   37,                             │ │
│ │                              │   │   │   ... +326                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   627,                            │ │
│ │                              │   │   │   2950,                           │ │
│ │                              │   │   │   2175,                           │ │
│ │                              │   │   │   7440,                           │ │
│ │                              │   │   │   19587,                          │ │
│ │                              │   │   │   4,                              │ │
│ │                              │   │   │   29175,                          │ │
│ │                              │   │   │   9031,                           │ │
│ │                              │   │   │   30265,                          │ │
│ │                              │   │   │   5564,                           │ │
│ │                              │   │   │   ... +324                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   1629,                           │ │
│ │                              │   │   │   1000,                           │ │
│ │                              │   │   │   1629,                           │ │
│ │                              │   │   │   3059,                           │ │
│ │                              │   │   │   19,                             │ │
│ │                              │   │   │   68,                             │ │
│ │                              │   │   │   791,                            │ │
│ │                              │   │   │   48669,                          │ │
│ │                              │   │   │   4901,                           │ │
│ │                              │   │   │   45152,                          │ │
│ │                              │   │   │   ... +516                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   15129,                          │ │
│ │                              │   │   │   154,                            │ │
│ │                              │   │   │   319,                            │ │
│ │                              │   │   │   385,                            │ │
│ │                              │   │   │   1033,                           │ │
│ │                              │   │   │   4,                              │ │
│ │                              │   │   │   48110,                          │ │
│ │                              │   │   │   37457,                          │ │
│ │                              │   │   │   282,                            │ │
│ │                              │   │   │   37457,                          │ │
│ │                              │   │   │   ... +423                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   23444,                          │ │
│ │                              │   │   │   31,                             │ │
│ │                              │   │   │   6851,                           │ │
│ │                              │   │   │   4339,                           │ │
│ │                              │   │   │   6,                              │ │
│ │                              │   │   │   48298,                          │ │
│ │                              │   │   │   133,                            │ │
│ │                              │   │   │   6739,                           │ │
│ │                              │   │   │   6,                              │ │
│ │                              │   │   │   48110,                          │ │
│ │                              │   │   │   ... +469                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   6460,                           │ │
│ │                              │   │   │   103,                            │ │
│ │                              │   │   │   3581,                           │ │
│ │                              │   │   │   6,                              │ │
│ │                              │   │   │   98,                             │ │
│ │                              │   │   │   38,                             │ │
│ │                              │   │   │   2928,                           │ │
│ │                              │   │   │   5,                              │ │
│ │                              │   │   │   1109,                           │ │
│ │                              │   │   │   32384,                          │ │
│ │                              │   │   │   ... +302                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   (                                       │ │
│ │                              │   │   [                                   │ │
│ │                              │   │   │   8738,                           │ │
│ │                              │   │   │   9,                              │ │
│ │                              │   │   │   4516,                           │ │
│ │                              │   │   │   717,                            │ │
│ │                              │   │   │   844,                            │ │
│ │                              │   │   │   530,                            │ │
│ │                              │   │   │   11,                             │ │
│ │                              │   │   │   9513,                           │ │
│ │                              │   │   │   49309,                          │ │
│ │                              │   │   │   10975,                          │ │
│ │                              │   │   │   ... +449                        │ │
│ │                              │   │   ],                                  │ │
│ │                              │   │   None                                │ │
│ │                              │   ),                                      │ │
│ │                              │   ... +480                                │ │
│ │                              ]                                           │ │
│ │        is_split_into_words = False                                       │ │
│ │                     kwargs = {}                                          │ │
│ │                 max_length = 256                                         │ │
│ │         pad_to_multiple_of = None                                        │ │
│ │           padding_strategy = <PaddingStrategy.MAX_LENGTH: 'max_length'>  │ │
│ │                   pair_ids = None                                        │ │
│ │      return_attention_mask = None                                        │ │
│ │              return_length = False                                       │ │
│ │     return_offsets_mapping = False                                       │ │
│ │  return_overflowing_tokens = False                                       │ │
│ │ return_special_tokens_mask = False                                       │ │
│ │             return_tensors = None                                        │ │
│ │      return_token_type_ids = None                                        │ │
│ │                 second_ids = None                                        │ │
│ │                       self = PreTrainedTokenizer(name_or_path='facebook… │ │
│ │                              vocab_size=50265,                           │ │
│ │                              model_max_len=1000000000000000019884624838… │ │
│ │                              is_fast=False, padding_side='right',        │ │
│ │                              truncation_side='right',                    │ │
│ │                              special_tokens={'bos_token':                │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'eos_token':              │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'unk_token':              │ │
│ │                              AddedToken("</s>", rstrip=False,            │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True), 'pad_token':              │ │
│ │                              AddedToken("<pad>", rstrip=False,           │ │
│ │                              lstrip=False, single_word=False,            │ │
│ │                              normalized=True)})                          │ │
│ │                     stride = 0                                           │ │
│ │        truncation_strategy = <TruncationStrategy.LONGEST_FIRST:          │ │
│ │                              'longest_first'>                            │ │
│ │                    verbose = True                                        │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/transformers/tokenization_utils.py:701 in get_input_ids               │
│                                                                              │
│   698 │   │   def get_input_ids(text):                                       │
│   699 │   │   │   if isinstance(text, str):                                  │
│   700 │   │   │   │   tokens = self.tokenize(text, **kwargs)                 │
│ ❱ 701 │   │   │   │   return self.convert_tokens_to_ids(tokens)              │
│   702 │   │   │   elif isinstance(text, (list, tuple)) and len(text) > 0 and │
│   703 │   │   │   │   if is_split_into_words:                                │
│   704 │   │   │   │   │   tokens = list(                                     │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │ is_split_into_words = False                                              │ │
│ │              kwargs = {}                                                 │ │
│ │                self = PreTrainedTokenizer(name_or_path='facebook/opt-2.… │ │
│ │                       vocab_size=50265,                                  │ │
│ │                       model_max_len=1000000000000000019884624838656,     │ │
│ │                       is_fast=False, padding_side='right',               │ │
│ │                       truncation_side='right',                           │ │
│ │                       special_tokens={'bos_token': AddedToken("</s>",    │ │
│ │                       rstrip=False, lstrip=False, single_word=False,     │ │
│ │                       normalized=True), 'eos_token': AddedToken("</s>",  │ │
│ │                       rstrip=False, lstrip=False, single_word=False,     │ │
│ │                       normalized=True), 'unk_token': AddedToken("</s>",  │ │
│ │                       rstrip=False, lstrip=False, single_word=False,     │ │
│ │                       normalized=True), 'pad_token': AddedToken("<pad>", │ │
│ │                       rstrip=False, lstrip=False, single_word=False,     │ │
│ │                       normalized=True)})                                 │ │
│ │                text = 'Calibur. The anklebiter ignored him and kept      │ │
│ │                       furiously digging. Calibur took a k'+1342          │ │
│ │              tokens = [                                                  │ │
│ │                       │   'Cal',                                         │ │
│ │                       │   'ibur',                                        │ │
│ │                       │   '.',                                           │ │
│ │                       │   'ĠThe',                                        │ │
│ │                       │   'Ġankle',                                      │ │
│ │                       │   'bit',                                         │ │
│ │                       │   'er',                                          │ │
│ │                       │   'Ġignored',                                    │ │
│ │                       │   'Ġhim',                                        │ │
│ │                       │   'Ġand',                                        │ │
│ │                       │   ... +331                                       │ │
│ │                       ]                                                  │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/transformers/tokenization_utils.py:579 in convert_tokens_to_ids       │
│                                                                              │
│   576 │   │                                                                  │
│   577 │   │   ids = []                                                       │
│   578 │   │   for token in tokens:                                           │
│ ❱ 579 │   │   │   ids.append(self._convert_token_to_id_with_added_voc(token) │
│   580 │   │   return ids                                                     │
│   581 │                                                                      │
│   582 │   def _convert_token_to_id_with_added_voc(self, token):              │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │    ids = [15117, 28959, 4, 20, 7451, 5881, 254, 8266, 123, 8, ... +6]    │ │
│ │   self = PreTrainedTokenizer(name_or_path='facebook/opt-2.7b',           │ │
│ │          vocab_size=50265,                                               │ │
│ │          model_max_len=1000000000000000019884624838656, is_fast=False,   │ │
│ │          padding_side='right', truncation_side='right',                  │ │
│ │          special_tokens={'bos_token': AddedToken("</s>", rstrip=False,   │ │
│ │          lstrip=False, single_word=False, normalized=True), 'eos_token': │ │
│ │          AddedToken("</s>", rstrip=False, lstrip=False,                  │ │
│ │          single_word=False, normalized=True), 'unk_token':               │ │
│ │          AddedToken("</s>", rstrip=False, lstrip=False,                  │ │
│ │          single_word=False, normalized=True), 'pad_token':               │ │
│ │          AddedToken("<pad>", rstrip=False, lstrip=False,                 │ │
│ │          single_word=False, normalized=True)})                           │ │
│ │  token = 'Ġtook'                                                         │ │
│ │ tokens = [                                                               │ │
│ │          │   'Cal',                                                      │ │
│ │          │   'ibur',                                                     │ │
│ │          │   '.',                                                        │ │
│ │          │   'ĠThe',                                                     │ │
│ │          │   'Ġankle',                                                   │ │
│ │          │   'bit',                                                      │ │
│ │          │   'er',                                                       │ │
│ │          │   'Ġignored',                                                 │ │
│ │          │   'Ġhim',                                                     │ │
│ │          │   'Ġand',                                                     │ │
│ │          │   ... +331                                                    │ │
│ │          ]                                                               │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/transformers/tokenization_utils.py:588 in                             │
│ _convert_token_to_id_with_added_voc                                          │
│                                                                              │
│   585 │   │                                                                  │
│   586 │   │   if token in self.added_tokens_encoder:                         │
│   587 │   │   │   return self.added_tokens_encoder[token]                    │
│ ❱ 588 │   │   return self._convert_token_to_id(token)                        │
│   589 │                                                                      │
│   590 │   def _convert_token_to_id(self, token):                             │
│   591 │   │   raise NotImplementedError                                      │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │  self = PreTrainedTokenizer(name_or_path='facebook/opt-2.7b',            │ │
│ │         vocab_size=50265, model_max_len=1000000000000000019884624838656, │ │
│ │         is_fast=False, padding_side='right', truncation_side='right',    │ │
│ │         special_tokens={'bos_token': AddedToken("</s>", rstrip=False,    │ │
│ │         lstrip=False, single_word=False, normalized=True), 'eos_token':  │ │
│ │         AddedToken("</s>", rstrip=False, lstrip=False,                   │ │
│ │         single_word=False, normalized=True), 'unk_token':                │ │
│ │         AddedToken("</s>", rstrip=False, lstrip=False,                   │ │
│ │         single_word=False, normalized=True), 'pad_token':                │ │
│ │         AddedToken("<pad>", rstrip=False, lstrip=False,                  │ │
│ │         single_word=False, normalized=True)})                            │ │
│ │ token = 'Ġtook'                                                          │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/transformers/models/gpt2/tokenization_gpt2.py:308 in                  │
│ _convert_token_to_id                                                         │
│                                                                              │
│   305 │                                                                      │
│   306 │   def _convert_token_to_id(self, token):                             │
│   307 │   │   """Converts a token (str) in an id using the vocab."""         │
│ ❱ 308 │   │   return self.encoder.get(token, self.encoder.get(self.unk_token │
│   309 │                                                                      │
│   310 │   def _convert_id_to_token(self, index):                             │
│   311 │   │   """Converts an index (integer) in a token (str) using the voca │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │  self = PreTrainedTokenizer(name_or_path='facebook/opt-2.7b',            │ │
│ │         vocab_size=50265, model_max_len=1000000000000000019884624838656, │ │
│ │         is_fast=False, padding_side='right', truncation_side='right',    │ │
│ │         special_tokens={'bos_token': AddedToken("</s>", rstrip=False,    │ │
│ │         lstrip=False, single_word=False, normalized=True), 'eos_token':  │ │
│ │         AddedToken("</s>", rstrip=False, lstrip=False,                   │ │
│ │         single_word=False, normalized=True), 'unk_token':                │ │
│ │         AddedToken("</s>", rstrip=False, lstrip=False,                   │ │
│ │         single_word=False, normalized=True), 'pad_token':                │ │
│ │         AddedToken("<pad>", rstrip=False, lstrip=False,                  │ │
│ │         single_word=False, normalized=True)})                            │ │
│ │ token = 'Ġtook'                                                          │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/transformers/tokenization_utils_base.py:988 in unk_token              │
│                                                                              │
│    985 │   │   │   return None                                               │
│    986 │   │   return str(self._eos_token)                                   │
│    987 │                                                                     │
│ ❱  988 │   @property                                                         │
│    989 │   def unk_token(self) -> str:                                       │
│    990 │   │   """                                                           │
│    991 │   │   `str`: Unknown token. Log an error if used while not having b │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │ self = PreTrainedTokenizer(name_or_path='facebook/opt-2.7b',             │ │
│ │        vocab_size=50265, model_max_len=1000000000000000019884624838656,  │ │
│ │        is_fast=False, padding_side='right', truncation_side='right',     │ │
│ │        special_tokens={'bos_token': AddedToken("</s>", rstrip=False,     │ │
│ │        lstrip=False, single_word=False, normalized=True), 'eos_token':   │ │
│ │        AddedToken("</s>", rstrip=False, lstrip=False, single_word=False, │ │
│ │        normalized=True), 'unk_token': AddedToken("</s>", rstrip=False,   │ │
│ │        lstrip=False, single_word=False, normalized=True), 'pad_token':   │ │
│ │        AddedToken("<pad>", rstrip=False, lstrip=False,                   │ │
│ │        single_word=False, normalized=True)})                             │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
╰──────────────────────────────────────────────────────────────────────────────╯
KeyboardInterrupt

During handling of the above exception, another exception occurred:

╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /home/paperspace/Documents/Repos/clm_model_tuning/finetune_using_clm_wandb.p │
│ y:599 in <module>                                                            │
│                                                                              │
│   596                                                                        │
│   597                                                                        │
│   598 if __name__ == "__main__":                                             │
│ ❱ 599 │   main()                                                             │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │             __annotations__ = {}                                         │ │
│ │                __builtins__ = <module 'builtins' (built-in)>             │ │
│ │                  __cached__ = None                                       │ │
│ │                     __doc__ = '\nFine-tuning the library models for      │ │
│ │                               causal language modeling (GPT, GPT-2,      │ │
│ │                               CTRL, '+208                                │ │
│ │                    __file__ = 'finetune_using_clm_wandb.py'              │ │
│ │                  __loader__ = <_frozen_importlib_external.SourceFileLoa… │ │
│ │                               object at 0x7f46d89524c0>                  │ │
│ │                    __name__ = '__main__'                                 │ │
│ │                 __package__ = None                                       │ │
│ │                    __spec__ = None                                       │ │
│ │                 Accelerator = <class                                     │ │
│ │                               'accelerate.accelerator.Accelerator'>      │ │
│ │                  AutoConfig = <class                                     │ │
│ │                               'transformers.models.auto.configuration_a… │ │
│ │                   AutoModel = <class                                     │ │
│ │                               'transformers.models.auto.modeling_auto.A… │ │
│ │        AutoModelForCausalLM = <class                                     │ │
│ │                               'transformers.models.auto.modeling_auto.A… │ │
│ │               AutoTokenizer = <class                                     │ │
│ │                               'transformers.models.auto.tokenization_au… │ │
│ │                   bittensor = <module 'bittensor' from                   │ │
│ │                               '/home/paperspace/Documents/Repos/clm_mod… │ │
│ │                       chain = <class 'itertools.chain'>                  │ │
│ │ check_cfg_and_load_defaults = <function check_cfg_and_load_defaults at   │ │
│ │                               0x7f46d890e310>                            │ │
│ │          create_accelerator = <function create_accelerator at            │ │
│ │                               0x7f4680748f70>                            │ │
│ │            create_optimizer = <function create_optimizer at              │ │
│ │                               0x7f46807530d0>                            │ │
│ │                  DataLoader = <class                                     │ │
│ │                               'torch.utils.data.dataloader.DataLoader'>  │ │
│ │                     Dataset = <class 'datasets.arrow_dataset.Dataset'>   │ │
│ │                 DatasetDict = <class                                     │ │
│ │                               'datasets.dataset_dict.DatasetDict'>       │ │
│ │                    datasets = <module 'datasets' from                    │ │
│ │                               '/home/paperspace/Documents/Repos/clm_mod… │ │
│ │       default_data_collator = <function default_data_collator at         │ │
│ │                               0x7f469c066700>                            │ │
│ │                  DictConfig = <class 'omegaconf.dictconfig.DictConfig'>  │ │
│ │             DistributedType = <enum 'DistributedType'>                   │ │
│ │                  get_logger = <function get_logger at 0x7f46ab7781f0>    │ │
│ │               get_scheduler = <function get_scheduler at 0x7f469c0328b0> │ │
│ │             HF_access_token = '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'    │ │
│ │                       hydra = <module 'hydra' from                       │ │
│ │                               '/home/paperspace/Documents/Repos/clm_mod… │ │
│ │                load_dataset = <function load_dataset at 0x7f46bed87790>  │ │
│ │    load_model_and_tokenizer = <function load_model_and_tokenizer at      │ │
│ │                               0x7f4680753040>                            │ │
│ │         load_preloaded_data = <function load_preloaded_data at           │ │
│ │                               0x7f4680753280>                            │ │
│ │           load_raw_datasets = <function load_raw_datasets at             │ │
│ │                               0x7f4680748ee0>                            │ │
│ │                     logging = <module 'logging' from                     │ │
│ │                               '/usr/lib/python3.8/logging/__init__.py'>  │ │
│ │                        main = <function main at 0x7f4680753430>          │ │
│ │                        math = <module 'math' (built-in)>                 │ │
│ │                          np = <module 'numpy' from                       │ │
│ │                               '/home/paperspace/Documents/Repos/clm_mod… │ │
│ │                   OmegaConf = <class 'omegaconf.omegaconf.OmegaConf'>    │ │
│ │                          os = <module 'os' from                          │ │
│ │                               '/usr/lib/python3.8/os.py'>                │ │
│ │                      pickle = <module 'pickle' from                      │ │
│ │                               '/usr/lib/python3.8/pickle.py'>            │ │
│ │                  preprocess = <function preprocess at 0x7f4680753160>    │ │
│ │                      random = <module 'random' from                      │ │
│ │                               '/usr/lib/python3.8/random.py'>            │ │
│ │     save_tokenized_datasets = <function save_tokenized_datasets at       │ │
│ │                               0x7f46807531f0>                            │ │
│ │                    set_seed = <function set_seed at 0x7f46a897cd30>      │ │
│ │                  test_model = False                                      │ │
│ │                        time = <module 'time' (built-in)>                 │ │
│ │                       torch = <module 'torch' from                       │ │
│ │                               '/home/paperspace/Documents/Repos/clm_mod… │ │
│ │                        tqdm = <class 'tqdm.asyncio.tqdm_asyncio'>        │ │
│ │                transformers = <module 'transformers' from                │ │
│ │                               '/home/paperspace/Documents/Repos/clm_mod… │ │
│ │                       wandb = <module 'wandb' from                       │ │
│ │                               '/home/paperspace/Documents/Repos/clm_mod… │ │
│ │                   WANDB_KEY = 'c0d007cc7a7f9e6db9b2d3d4d37f17f1b0202276' │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/hydra/main.py:90 in decorated_main                                    │
│                                                                              │
│    87 │   │   │   │   else:                                                  │
│    88 │   │   │   │   │   # no return value from run_hydra() as it may somet │
│    89 │   │   │   │   │   # multiple times (--multirun)                      │
│ ❱  90 │   │   │   │   │   _run_hydra(                                        │
│    91 │   │   │   │   │   │   args=args,                                     │
│    92 │   │   │   │   │   │   args_parser=args_parser,                       │
│    93 │   │   │   │   │   │   task_function=task_function,                   │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │            args = Namespace(cfg=None, config_dir=None, config_name=None, │ │
│ │                   config_path=None, experimental_rerun=None, help=False, │ │
│ │                   hydra_help=False, info=None, multirun=False,           │ │
│ │                   overrides=['model.name=facebook/opt-2.7b',             │ │
│ │                   'training.eval_every=250',                             │ │
│ │                   'training.train_batch_size=32',                        │ │
│ │                   'training.weight_decay=0.05',                          │ │
│ │                   'training.eval_batch_size=16',                         │ │
│ │                   'training.learning_rate=0.000003',                     │ │
│ │                   'training.val_split_percent=20',                       │ │
│ │                   'training.num_epochs=4', 'training.lr_warmup_steps=5', │ │
│ │                   'training.gradient_accumulation_steps=2',              │ │
│ │                   'dataset.name=ViktorThink/mountain_combined_813306',   │ │
│ │                   'dataset.num_batches=5000', 'hydra.mode=RUN'],         │ │
│ │                   package=None, resolve=False, run=True,                 │ │
│ │                   shell_completion=False)                                │ │
│ │     args_parser = ArgumentParser(prog='finetune_using_clm_wandb.py',     │ │
│ │                   usage=None, description='Hydra',                       │ │
│ │                   formatter_class=<class 'argparse.HelpFormatter'>,      │ │
│ │                   conflict_handler='error', add_help=False)              │ │
│ │ cfg_passthrough = None                                                   │ │
│ │     config_name = 'config'                                               │ │
│ │     config_path = 'conf'                                                 │ │
│ │   task_function = <function main at 0x7f46807533a0>                      │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/hydra/_internal/utils.py:389 in _run_hydra                            │
│                                                                              │
│   386 │   │                                                                  │
│   387 │   │   if args.run or args.multirun:                                  │
│   388 │   │   │   run_mode = hydra.get_mode(config_name=config_name, overrid │
│ ❱ 389 │   │   │   _run_app(                                                  │
│   390 │   │   │   │   run=args.run,                                          │
│   391 │   │   │   │   multirun=args.multirun,                                │
│   392 │   │   │   │   mode=run_mode,                                         │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │       add_conf_dir = <function _run_hydra.<locals>.add_conf_dir at       │ │
│ │                      0x7f46807534c0>                                     │ │
│ │               args = Namespace(cfg=None, config_dir=None,                │ │
│ │                      config_name=None, config_path=None,                 │ │
│ │                      experimental_rerun=None, help=False,                │ │
│ │                      hydra_help=False, info=None, multirun=False,        │ │
│ │                      overrides=['model.name=facebook/opt-2.7b',          │ │
│ │                      'training.eval_every=250',                          │ │
│ │                      'training.train_batch_size=32',                     │ │
│ │                      'training.weight_decay=0.05',                       │ │
│ │                      'training.eval_batch_size=16',                      │ │
│ │                      'training.learning_rate=0.000003',                  │ │
│ │                      'training.val_split_percent=20',                    │ │
│ │                      'training.num_epochs=4',                            │ │
│ │                      'training.lr_warmup_steps=5',                       │ │
│ │                      'training.gradient_accumulation_steps=2',           │ │
│ │                      'dataset.name=ViktorThink/mountain_combined_813306… │ │
│ │                      'dataset.num_batches=5000', 'hydra.mode=RUN'],      │ │
│ │                      package=None, resolve=False, run=True,              │ │
│ │                      shell_completion=False)                             │ │
│ │        args_parser = ArgumentParser(prog='finetune_using_clm_wandb.py',  │ │
│ │                      usage=None, description='Hydra',                    │ │
│ │                      formatter_class=<class 'argparse.HelpFormatter'>,   │ │
│ │                      conflict_handler='error', add_help=False)           │ │
│ │ caller_stack_depth = 2                                                   │ │
│ │       calling_file = 'finetune_using_clm_wandb.py'                       │ │
│ │     calling_module = None                                                │ │
│ │        config_name = 'config'                                            │ │
│ │        config_path = 'conf'                                              │ │
│ │        GlobalHydra = <class 'hydra.core.global_hydra.GlobalHydra'>       │ │
│ │       has_show_cfg = False                                               │ │
│ │              Hydra = <class 'hydra._internal.hydra.Hydra'>               │ │
│ │              hydra = <hydra._internal.hydra.Hydra object at              │ │
│ │                      0x7f4680759fa0>                                     │ │
│ │       num_commands = 0                                                   │ │
│ │          overrides = [                                                   │ │
│ │                      │   'model.name=facebook/opt-2.7b',                 │ │
│ │                      │   'training.eval_every=250',                      │ │
│ │                      │   'training.train_batch_size=32',                 │ │
│ │                      │   'training.weight_decay=0.05',                   │ │
│ │                      │   'training.eval_batch_size=16',                  │ │
│ │                      │   'training.learning_rate=0.000003',              │ │
│ │                      │   'training.val_split_percent=20',                │ │
│ │                      │   'training.num_epochs=4',                        │ │
│ │                      │   'training.lr_warmup_steps=5',                   │ │
│ │                      │   'training.gradient_accumulation_steps=2',       │ │
│ │                      │   ... +3                                          │ │
│ │                      ]                                                   │ │
│ │           run_mode = None                                                │ │
│ │        search_path = <hydra._internal.config_search_path_impl.ConfigSea… │ │
│ │                      object at 0x7f4680755bb0>                           │ │
│ │      task_function = <function main at 0x7f46807533a0>                   │ │
│ │          task_name = 'finetune_using_clm_wandb'                          │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/hydra/_internal/utils.py:452 in _run_app                              │
│                                                                              │
│   449 │   │   │   overrides.extend(["hydra.mode=MULTIRUN"])                  │
│   450 │                                                                      │
│   451 │   if mode == RunMode.RUN:                                            │
│ ❱ 452 │   │   run_and_report(                                                │
│   453 │   │   │   lambda: hydra.run(                                         │
│   454 │   │   │   │   config_name=config_name,                               │
│   455 │   │   │   │   task_function=task_function,                           │
│                                                                              │
│ ╭──────────────────────────────── locals ────────────────────────────────╮   │
│ │   config_name = 'config'                                               │   │
│ │         hydra = <hydra._internal.hydra.Hydra object at 0x7f4680759fa0> │   │
│ │          mode = <RunMode.RUN: 1>                                       │   │
│ │      multirun = False                                                  │   │
│ │     overrides = [                                                      │   │
│ │                 │   'model.name=facebook/opt-2.7b',                    │   │
│ │                 │   'training.eval_every=250',                         │   │
│ │                 │   'training.train_batch_size=32',                    │   │
│ │                 │   'training.weight_decay=0.05',                      │   │
│ │                 │   'training.eval_batch_size=16',                     │   │
│ │                 │   'training.learning_rate=0.000003',                 │   │
│ │                 │   'training.val_split_percent=20',                   │   │
│ │                 │   'training.num_epochs=4',                           │   │
│ │                 │   'training.lr_warmup_steps=5',                      │   │
│ │                 │   'training.gradient_accumulation_steps=2',          │   │
│ │                 │   ... +3                                             │   │
│ │                 ]                                                      │   │
│ │           run = True                                                   │   │
│ │ task_function = <function main at 0x7f46807533a0>                      │   │
│ ╰────────────────────────────────────────────────────────────────────────╯   │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/hydra/_internal/utils.py:213 in run_and_report                        │
│                                                                              │
│   210                                                                        │
│   211 def run_and_report(func: Any) -> Any:                                  │
│   212 │   try:                                                               │
│ ❱ 213 │   │   return func()                                                  │
│   214 │   except Exception as ex:                                            │
│   215 │   │   if _is_env_set("HYDRA_FULL_ERROR") or is_under_debugger():     │
│   216 │   │   │   raise ex                                                   │
│                                                                              │
│ ╭──────────────────────────── locals ────────────────────────────╮           │
│ │ func = <function _run_app.<locals>.<lambda> at 0x7f46806b2790> │           │
│ ╰────────────────────────────────────────────────────────────────╯           │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/hydra/_internal/utils.py:453 in <lambda>                              │
│                                                                              │
│   450 │                                                                      │
│   451 │   if mode == RunMode.RUN:                                            │
│   452 │   │   run_and_report(                                                │
│ ❱ 453 │   │   │   lambda: hydra.run(                                         │
│   454 │   │   │   │   config_name=config_name,                               │
│   455 │   │   │   │   task_function=task_function,                           │
│   456 │   │   │   │   overrides=overrides,                                   │
│                                                                              │
│ ╭──────────────────────────────── locals ────────────────────────────────╮   │
│ │   config_name = 'config'                                               │   │
│ │         hydra = <hydra._internal.hydra.Hydra object at 0x7f4680759fa0> │   │
│ │     overrides = [                                                      │   │
│ │                 │   'model.name=facebook/opt-2.7b',                    │   │
│ │                 │   'training.eval_every=250',                         │   │
│ │                 │   'training.train_batch_size=32',                    │   │
│ │                 │   'training.weight_decay=0.05',                      │   │
│ │                 │   'training.eval_batch_size=16',                     │   │
│ │                 │   'training.learning_rate=0.000003',                 │   │
│ │                 │   'training.val_split_percent=20',                   │   │
│ │                 │   'training.num_epochs=4',                           │   │
│ │                 │   'training.lr_warmup_steps=5',                      │   │
│ │                 │   'training.gradient_accumulation_steps=2',          │   │
│ │                 │   ... +3                                             │   │
│ │                 ]                                                      │   │
│ │ task_function = <function main at 0x7f46807533a0>                      │   │
│ ╰────────────────────────────────────────────────────────────────────────╯   │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/hydra/_internal/hydra.py:119 in run                                   │
│                                                                              │
│   116 │   │   callbacks = Callbacks(cfg)                                     │
│   117 │   │   callbacks.on_run_start(config=cfg, config_name=config_name)    │
│   118 │   │                                                                  │
│ ❱ 119 │   │   ret = run_job(                                                 │
│   120 │   │   │   hydra_context=HydraContext(                                │
│   121 │   │   │   │   config_loader=self.config_loader, callbacks=callbacks  │
│   122 │   │   │   ),                                                         │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │              callbacks = <hydra._internal.callbacks.Callbacks object at  │ │
│ │                          0x7f46b4846d90>                                 │ │
│ │                    cfg = {'hydra': {'run': {'dir':                       │ │
│ │                          'outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}'},     │ │
│ │                          'sweep': {'dir':                                │ │
│ │                          'multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}',     │ │
│ │                          'subdir': '${hydra.job.num}'}, 'launcher':      │ │
│ │                          {'_target_':                                    │ │
│ │                          'hydra._internal.core_plugins.basic_launcher.B… │ │
│ │                          'sweeper': {'_target_':                         │ │
│ │                          'hydra._internal.core_plugins.basic_sweeper.Ba… │ │
│ │                          'max_batch_size': None, 'params': None},        │ │
│ │                          'help': {'app_name': '${hydra.job.name}',       │ │
│ │                          'header': '${hydra.help.app_name} is powered by │ │
│ │                          Hydra.\n', 'footer': 'Powered by Hydra          │ │
│ │                          (https://hydra.cc)\nUse --hydra-help to view    │ │
│ │                          Hydra specific help\n', 'template':             │ │
│ │                          '${hydra.help.header}\n== Configuration groups  │ │
│ │                          ==\nCompose your configuration from those       │ │
│ │                          groups                                          │ │
│ │                          (group=option)\n\n$APP_CONFIG_GROUPS\n\n==      │ │
│ │                          Config ==\nOverride anything in the config      │ │
│ │                          (foo.bar=value)\n\n$CONFIG\n\n${hydra.help.foo… │ │
│ │                          'hydra_help': {'template': "Hydra               │ │
│ │                          (${hydra.runtime.version})\nSee                 │ │
│ │                          https://hydra.cc for more info.\n\n== Flags     │ │
│ │                          ==\n$FLAGS_HELP\n\n== Configuration groups      │ │
│ │                          ==\nCompose your configuration from those       │ │
│ │                          groups (For example, append                     │ │
│ │                          hydra/job_logging=disabled to command           │ │
│ │                          line)\n\n$HYDRA_CONFIG_GROUPS\n\nUse '--cfg     │ │
│ │                          hydra' to Show the Hydra config.\n",            │ │
│ │                          'hydra_help': '???'}, 'hydra_logging':          │ │
│ │                          {'version': 1, 'formatters': {'simple':         │ │
│ │                          {'format': '[%(asctime)s][HYDRA]                │ │
│ │                          %(message)s'}}, 'handlers': {'console':         │ │
│ │                          {'class': 'logging.StreamHandler', 'formatter': │ │
│ │                          'simple', 'stream': 'ext://sys.stdout'}},       │ │
│ │                          'root': {'level': 'INFO', 'handlers':           │ │
│ │                          ['console']}, 'loggers': {'logging_example':    │ │
│ │                          {'level': 'DEBUG'}},                            │ │
│ │                          'disable_existing_loggers': False},             │ │
│ │                          'job_logging': {'version': 1, 'formatters':     │ │
│ │                          {'simple': {'format':                           │ │
│ │                          '[%(asctime)s][%(name)s][%(levelname)s] -       │ │
│ │                          %(message)s'}}, 'handlers': {'console':         │ │
│ │                          {'class': 'logging.StreamHandler', 'formatter': │ │
│ │                          'simple', 'stream': 'ext://sys.stdout'},        │ │
│ │                          'file': {'class': 'logging.FileHandler',        │ │
│ │                          'formatter': 'simple', 'filename':              │ │
│ │                          '${hydra.runtime.output_dir}/${hydra.job.name}… │ │
│ │                          'root': {'level': 'INFO', 'handlers':           │ │
│ │                          ['console', 'file']},                           │ │
│ │                          'disable_existing_loggers': False}, 'env': {},  │ │
│ │                          'mode': <RunMode.RUN: 1>, 'searchpath': [],     │ │
│ │                          'callbacks': {}, 'output_subdir': '.hydra',     │ │
│ │                          'overrides': {'hydra': ['hydra.mode=RUN'],      │ │
│ │                          'task': ['model.name=facebook/opt-2.7b',        │ │
│ │                          'training.eval_every=250',                      │ │
│ │                          'training.train_batch_size=32',                 │ │
│ │                          'training.weight_decay=0.05',                   │ │
│ │                          'training.eval_batch_size=16',                  │ │
│ │                          'training.learning_rate=0.000003',              │ │
│ │                          'training.val_split_percent=20',                │ │
│ │                          'training.num_epochs=4',                        │ │
│ │                          'training.lr_warmup_steps=5',                   │ │
│ │                          'training.gradient_accumulation_steps=2',       │ │
│ │                          'dataset.name=ViktorThink/mountain_combined_81… │ │
│ │                          'dataset.num_batches=5000']}, 'job': {'name':   │ │
│ │                          'finetune_using_clm_wandb', 'chdir': None,      │ │
│ │                          'override_dirname':                             │ │
│ │                          'dataset.name=ViktorThink/mountain_combined_81… │ │
│ │                          'id': '???', 'num': '???', 'config_name':       │ │
│ │                          'config', 'env_set': {}, 'env_copy': [],        │ │
│ │                          'config': {'override_dirname': {'kv_sep': '=',  │ │
│ │                          'item_sep': ',', 'exclude_keys': []}}},         │ │
│ │                          'runtime': {'version': '1.2.0', 'version_base': │ │
│ │                          '1.2', 'cwd':                                   │ │
│ │                          '/home/paperspace/Documents/Repos/clm_model_tu… │ │
│ │                          'config_sources': [{'path': 'hydra.conf',       │ │
│ │                          'schema': 'pkg', 'provider': 'hydra'}, {'path': │ │
│ │                          '/home/paperspace/Documents/Repos/clm_model_tu… │ │
│ │                          'schema': 'file', 'provider': 'main'}, {'path': │ │
│ │                          '', 'schema': 'structured', 'provider':         │ │
│ │                          'schema'}], 'output_dir':                       │ │
│ │                          '/home/paperspace/Documents/Repos/clm_model_tu… │ │
│ │                          'choices': {'hydra/env': 'default',             │ │
│ │                          'hydra/callbacks': None, 'hydra/job_logging':   │ │
│ │                          'default', 'hydra/hydra_logging': 'default',    │ │
│ │                          'hydra/hydra_help': 'default', 'hydra/help':    │ │
│ │                          'default', 'hydra/sweeper': 'basic',            │ │
│ │                          'hydra/launcher': 'basic', 'hydra/output':      │ │
│ │                          'default'}}, 'verbose': False}, 'output_dir':   │ │
│ │                          'tuned-model', 'bittensor': {'network':         │ │
│ │                          'nobunaga'}, 'dataset': {'name':                │ │
│ │                          'ViktorThink/mountain_combined_813306',         │ │
│ │                          'config_name': None, 'num_batches': 5000,       │ │
│ │                          'block_size': None, 'overwrite_cache': False,   │ │
│ │                          'keep_linebreaks': True, 'concatenate_raw':     │ │
│ │                          False, 'load_tokenized_data': False}, 'model':  │ │
│ │                          {'name': 'facebook/opt-2.7b', 'config_name':    │ │
│ │                          None}, 'tokenizer': {'name': None, 'use_fast':  │ │
│ │                          True, 'preprocessing_num_workers': None,        │ │
│ │                          'pad_token': '[PAD]'}, 'training': {'seed': 17, │ │
│ │                          'val_split_percent': 20, 'train_batch_size':    │ │
│ │                          32, 'eval_batch_size': 16, 'learning_rate':     │ │
│ │                          3e-06, 'weight_decay': 0.05, 'num_epochs': 4,   │ │
│ │                          'max_train_steps': None,                        │ │
│ │                          'gradient_accumulation_steps': 2,               │ │
│ │                          'lr_scheduler': 'constant', 'lr_warmup_steps':  │ │
│ │                          5, 'eval_every': 250, 'max_eval_steps': 500,    │ │
│ │                          'checkpoint': {'resume_from_checkpoint': 0,     │ │
│ │                          'every_n_steps': None}}, 'tracking':            │ │
│ │                          {'enabled': True, 'report_to': 'all'},          │ │
│ │                          'testing': {'enabled': False}}                  │ │
│ │            config_name = 'config'                                        │ │
│ │              overrides = [                                               │ │
│ │                          │   'model.name=facebook/opt-2.7b',             │ │
│ │                          │   'training.eval_every=250',                  │ │
│ │                          │   'training.train_batch_size=32',             │ │
│ │                          │   'training.weight_decay=0.05',               │ │
│ │                          │   'training.eval_batch_size=16',              │ │
│ │                          │   'training.learning_rate=0.000003',          │ │
│ │                          │   'training.val_split_percent=20',            │ │
│ │                          │   'training.num_epochs=4',                    │ │
│ │                          │   'training.lr_warmup_steps=5',               │ │
│ │                          │   'training.gradient_accumulation_steps=2',   │ │
│ │                          │   ... +3                                      │ │
│ │                          ]                                               │ │
│ │                   self = <hydra._internal.hydra.Hydra object at          │ │
│ │                          0x7f4680759fa0>                                 │ │
│ │          task_function = <function main at 0x7f46807533a0>               │ │
│ │ with_log_configuration = True                                            │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/hydra/core/utils.py:186 in run_job                                    │
│                                                                              │
│   183 │   │   with env_override(hydra_cfg.hydra.job.env_set):                │
│   184 │   │   │   callbacks.on_job_start(config=config)                      │
│   185 │   │   │   try:                                                       │
│ ❱ 186 │   │   │   │   ret.return_value = task_function(task_cfg)             │
│   187 │   │   │   │   ret.status = JobStatus.COMPLETED                       │
│   188 │   │   │   except Exception as e:                                     │
│   189 │   │   │   │   ret.return_value = e                                   │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │            _chdir = False                                                │ │
│ │         callbacks = <hydra._internal.callbacks.Callbacks object at       │ │
│ │                     0x7f46b4846d90>                                      │ │
│ │            config = {'hydra': {'run': {'dir':                            │ │
│ │                     'outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}'}, 'sweep': │ │
│ │                     {'dir': 'multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}',  │ │
│ │                     'subdir': '${hydra.job.num}'}, 'launcher':           │ │
│ │                     {'_target_':                                         │ │
│ │                     'hydra._internal.core_plugins.basic_launcher.BasicL… │ │
│ │                     'sweeper': {'_target_':                              │ │
│ │                     'hydra._internal.core_plugins.basic_sweeper.BasicSw… │ │
│ │                     'max_batch_size': None, 'params': None}, 'help':     │ │
│ │                     {'app_name': '${hydra.job.name}', 'header':          │ │
│ │                     '${hydra.help.app_name} is powered by Hydra.\n',     │ │
│ │                     'footer': 'Powered by Hydra (https://hydra.cc)\nUse  │ │
│ │                     --hydra-help to view Hydra specific help\n',         │ │
│ │                     'template': '${hydra.help.header}\n== Configuration  │ │
│ │                     groups ==\nCompose your configuration from those     │ │
│ │                     groups (group=option)\n\n$APP_CONFIG_GROUPS\n\n==    │ │
│ │                     Config ==\nOverride anything in the config           │ │
│ │                     (foo.bar=value)\n\n$CONFIG\n\n${hydra.help.footer}\… │ │
│ │                     'hydra_help': {'template': "Hydra                    │ │
│ │                     (${hydra.runtime.version})\nSee https://hydra.cc for │ │
│ │                     more info.\n\n== Flags ==\n$FLAGS_HELP\n\n==         │ │
│ │                     Configuration groups ==\nCompose your configuration  │ │
│ │                     from those groups (For example, append               │ │
│ │                     hydra/job_logging=disabled to command                │ │
│ │                     line)\n\n$HYDRA_CONFIG_GROUPS\n\nUse '--cfg hydra'   │ │
│ │                     to Show the Hydra config.\n", 'hydra_help': '???'},  │ │
│ │                     'hydra_logging': {'version': 1, 'formatters':        │ │
│ │                     {'simple': {'format': '[%(asctime)s][HYDRA]          │ │
│ │                     %(message)s'}}, 'handlers': {'console': {'class':    │ │
│ │                     'logging.StreamHandler', 'formatter': 'simple',      │ │
│ │                     'stream': 'ext://sys.stdout'}}, 'root': {'level':    │ │
│ │                     'INFO', 'handlers': ['console']}, 'loggers':         │ │
│ │                     {'logging_example': {'level': 'DEBUG'}},             │ │
│ │                     'disable_existing_loggers': False}, 'job_logging':   │ │
│ │                     {'version': 1, 'formatters': {'simple': {'format':   │ │
│ │                     '[%(asctime)s][%(name)s][%(levelname)s] -            │ │
│ │                     %(message)s'}}, 'handlers': {'console': {'class':    │ │
│ │                     'logging.StreamHandler', 'formatter': 'simple',      │ │
│ │                     'stream': 'ext://sys.stdout'}, 'file': {'class':     │ │
│ │                     'logging.FileHandler', 'formatter': 'simple',        │ │
│ │                     'filename':                                          │ │
│ │                     '${hydra.runtime.output_dir}/${hydra.job.name}.log'… │ │
│ │                     'root': {'level': 'INFO', 'handlers': ['console',    │ │
│ │                     'file']}, 'disable_existing_loggers': False}, 'env': │ │
│ │                     {}, 'mode': <RunMode.RUN: 1>, 'searchpath': [],      │ │
│ │                     'callbacks': {}, 'output_subdir': '.hydra',          │ │
│ │                     'overrides': {'hydra': ['hydra.mode=RUN'], 'task':   │ │
│ │                     ['model.name=facebook/opt-2.7b',                     │ │
│ │                     'training.eval_every=250',                           │ │
│ │                     'training.train_batch_size=32',                      │ │
│ │                     'training.weight_decay=0.05',                        │ │
│ │                     'training.eval_batch_size=16',                       │ │
│ │                     'training.learning_rate=0.000003',                   │ │
│ │                     'training.val_split_percent=20',                     │ │
│ │                     'training.num_epochs=4',                             │ │
│ │                     'training.lr_warmup_steps=5',                        │ │
│ │                     'training.gradient_accumulation_steps=2',            │ │
│ │                     'dataset.name=ViktorThink/mountain_combined_813306', │ │
│ │                     'dataset.num_batches=5000']}, 'job': {'name':        │ │
│ │                     'finetune_using_clm_wandb', 'chdir': None,           │ │
│ │                     'override_dirname':                                  │ │
│ │                     'dataset.name=ViktorThink/mountain_combined_813306,… │ │
│ │                     'id': '???', 'num': '???', 'config_name': 'config',  │ │
│ │                     'env_set': {}, 'env_copy': [], 'config':             │ │
│ │                     {'override_dirname': {'kv_sep': '=', 'item_sep':     │ │
│ │                     ',', 'exclude_keys': []}}}, 'runtime': {'version':   │ │
│ │                     '1.2.0', 'version_base': '1.2', 'cwd':               │ │
│ │                     '/home/paperspace/Documents/Repos/clm_model_tuning', │ │
│ │                     'config_sources': [{'path': 'hydra.conf', 'schema':  │ │
│ │                     'pkg', 'provider': 'hydra'}, {'path':                │ │
│ │                     '/home/paperspace/Documents/Repos/clm_model_tuning/… │ │
│ │                     'schema': 'file', 'provider': 'main'}, {'path': '',  │ │
│ │                     'schema': 'structured', 'provider': 'schema'}],      │ │
│ │                     'output_dir':                                        │ │
│ │                     '/home/paperspace/Documents/Repos/clm_model_tuning/… │ │
│ │                     'choices': {'hydra/env': 'default',                  │ │
│ │                     'hydra/callbacks': None, 'hydra/job_logging':        │ │
│ │                     'default', 'hydra/hydra_logging': 'default',         │ │
│ │                     'hydra/hydra_help': 'default', 'hydra/help':         │ │
│ │                     'default', 'hydra/sweeper': 'basic',                 │ │
│ │                     'hydra/launcher': 'basic', 'hydra/output':           │ │
│ │                     'default'}}, 'verbose': False}, 'output_dir':        │ │
│ │                     'tuned-model', 'bittensor': {'network': 'nobunaga'}, │ │
│ │                     'dataset': {'name':                                  │ │
│ │                     'ViktorThink/mountain_combined_813306',              │ │
│ │                     'config_name': None, 'num_batches': 5000,            │ │
│ │                     'block_size': None, 'overwrite_cache': False,        │ │
│ │                     'keep_linebreaks': True, 'concatenate_raw': False,   │ │
│ │                     'load_tokenized_data': False}, 'model': {'name':     │ │
│ │                     'facebook/opt-2.7b', 'config_name': None},           │ │
│ │                     'tokenizer': {'name': None, 'use_fast': True,        │ │
│ │                     'preprocessing_num_workers': None, 'pad_token':      │ │
│ │                     '[PAD]'}, 'training': {'seed': 17,                   │ │
│ │                     'val_split_percent': 20, 'train_batch_size': 32,     │ │
│ │                     'eval_batch_size': 16, 'learning_rate': 3e-06,       │ │
│ │                     'weight_decay': 0.05, 'num_epochs': 4,               │ │
│ │                     'max_train_steps': None,                             │ │
│ │                     'gradient_accumulation_steps': 2, 'lr_scheduler':    │ │
│ │                     'constant', 'lr_warmup_steps': 5, 'eval_every': 250, │ │
│ │                     'max_eval_steps': 500, 'checkpoint':                 │ │
│ │                     {'resume_from_checkpoint': 0, 'every_n_steps':       │ │
│ │                     None}}, 'tracking': {'enabled': True, 'report_to':   │ │
│ │                     'all'}, 'testing': {'enabled': False}}               │ │
│ │ configure_logging = True                                                 │ │
│ │         hydra_cfg = {'hydra': {'run': {'dir':                            │ │
│ │                     'outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}'}, 'sweep': │ │
│ │                     {'dir': 'multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}',  │ │
│ │                     'subdir': '${hydra.job.num}'}, 'launcher':           │ │
│ │                     {'_target_':                                         │ │
│ │                     'hydra._internal.core_plugins.basic_launcher.BasicL… │ │
│ │                     'sweeper': {'_target_':                              │ │
│ │                     'hydra._internal.core_plugins.basic_sweeper.BasicSw… │ │
│ │                     'max_batch_size': None, 'params': None}, 'help':     │ │
│ │                     {'app_name': '${hydra.job.name}', 'header':          │ │
│ │                     '${hydra.help.app_name} is powered by Hydra.\n',     │ │
│ │                     'footer': 'Powered by Hydra (https://hydra.cc)\nUse  │ │
│ │                     --hydra-help to view Hydra specific help\n',         │ │
│ │                     'template': '${hydra.help.header}\n== Configuration  │ │
│ │                     groups ==\nCompose your configuration from those     │ │
│ │                     groups (group=option)\n\n$APP_CONFIG_GROUPS\n\n==    │ │
│ │                     Config ==\nOverride anything in the config           │ │
│ │                     (foo.bar=value)\n\n$CONFIG\n\n${hydra.help.footer}\… │ │
│ │                     'hydra_help': {'template': "Hydra                    │ │
│ │                     (${hydra.runtime.version})\nSee https://hydra.cc for │ │
│ │                     more info.\n\n== Flags ==\n$FLAGS_HELP\n\n==         │ │
│ │                     Configuration groups ==\nCompose your configuration  │ │
│ │                     from those groups (For example, append               │ │
│ │                     hydra/job_logging=disabled to command                │ │
│ │                     line)\n\n$HYDRA_CONFIG_GROUPS\n\nUse '--cfg hydra'   │ │
│ │                     to Show the Hydra config.\n", 'hydra_help': '???'},  │ │
│ │                     'hydra_logging': {'version': 1, 'formatters':        │ │
│ │                     {'simple': {'format': '[%(asctime)s][HYDRA]          │ │
│ │                     %(message)s'}}, 'handlers': {'console': {'class':    │ │
│ │                     'logging.StreamHandler', 'formatter': 'simple',      │ │
│ │                     'stream': 'ext://sys.stdout'}}, 'root': {'level':    │ │
│ │                     'INFO', 'handlers': ['console']}, 'loggers':         │ │
│ │                     {'logging_example': {'level': 'DEBUG'}},             │ │
│ │                     'disable_existing_loggers': False}, 'job_logging':   │ │
│ │                     {'version': 1, 'formatters': {'simple': {'format':   │ │
│ │                     '[%(asctime)s][%(name)s][%(levelname)s] -            │ │
│ │                     %(message)s'}}, 'handlers': {'console': {'class':    │ │
│ │                     'logging.StreamHandler', 'formatter': 'simple',      │ │
│ │                     'stream': 'ext://sys.stdout'}, 'file': {'class':     │ │
│ │                     'logging.FileHandler', 'formatter': 'simple',        │ │
│ │                     'filename':                                          │ │
│ │                     '${hydra.runtime.output_dir}/${hydra.job.name}.log'… │ │
│ │                     'root': {'level': 'INFO', 'handlers': ['console',    │ │
│ │                     'file']}, 'disable_existing_loggers': False}, 'env': │ │
│ │                     {}, 'mode': <RunMode.RUN: 1>, 'searchpath': [],      │ │
│ │                     'callbacks': {}, 'output_subdir': '.hydra',          │ │
│ │                     'overrides': {'hydra': ['hydra.mode=RUN'], 'task':   │ │
│ │                     ['model.name=facebook/opt-2.7b',                     │ │
│ │                     'training.eval_every=250',                           │ │
│ │                     'training.train_batch_size=32',                      │ │
│ │                     'training.weight_decay=0.05',                        │ │
│ │                     'training.eval_batch_size=16',                       │ │
│ │                     'training.learning_rate=0.000003',                   │ │
│ │                     'training.val_split_percent=20',                     │ │
│ │                     'training.num_epochs=4',                             │ │
│ │                     'training.lr_warmup_steps=5',                        │ │
│ │                     'training.gradient_accumulation_steps=2',            │ │
│ │                     'dataset.name=ViktorThink/mountain_combined_813306', │ │
│ │                     'dataset.num_batches=5000']}, 'job': {'name':        │ │
│ │                     'finetune_using_clm_wandb', 'chdir': None,           │ │
│ │                     'override_dirname':                                  │ │
│ │                     'dataset.name=ViktorThink/mountain_combined_813306,… │ │
│ │                     'id': '???', 'num': '???', 'config_name': 'config',  │ │
│ │                     'env_set': {}, 'env_copy': [], 'config':             │ │
│ │                     {'override_dirname': {'kv_sep': '=', 'item_sep':     │ │
│ │                     ',', 'exclude_keys': []}}}, 'runtime': {'version':   │ │
│ │                     '1.2.0', 'version_base': '1.2', 'cwd':               │ │
│ │                     '/home/paperspace/Documents/Repos/clm_model_tuning', │ │
│ │                     'config_sources': [{'path': 'hydra.conf', 'schema':  │ │
│ │                     'pkg', 'provider': 'hydra'}, {'path':                │ │
│ │                     '/home/paperspace/Documents/Repos/clm_model_tuning/… │ │
│ │                     'schema': 'file', 'provider': 'main'}, {'path': '',  │ │
│ │                     'schema': 'structured', 'provider': 'schema'}],      │ │
│ │                     'output_dir':                                        │ │
│ │                     '/home/paperspace/Documents/Repos/clm_model_tuning/… │ │
│ │                     'choices': {'hydra/env': 'default',                  │ │
│ │                     'hydra/callbacks': None, 'hydra/job_logging':        │ │
│ │                     'default', 'hydra/hydra_logging': 'default',         │ │
│ │                     'hydra/hydra_help': 'default', 'hydra/help':         │ │
│ │                     'default', 'hydra/sweeper': 'basic',                 │ │
│ │                     'hydra/launcher': 'basic', 'hydra/output':           │ │
│ │                     'default'}}, 'verbose': False}}                      │ │
│ │     hydra_context = HydraContext(                                        │ │
│ │                     │                                                    │ │
│ │                     config_loader=<hydra._internal.config_loader_impl.C… │ │
│ │                     object at 0x7f4680755f70>,                           │ │
│ │                     │   callbacks=<hydra._internal.callbacks.Callbacks   │ │
│ │                     object at 0x7f46b4846d90>                            │ │
│ │                     )                                                    │ │
│ │      hydra_output = PosixPath('/home/paperspace/Documents/Repos/clm_mod… │ │
│ │       job_dir_key = 'hydra.run.dir'                                      │ │
│ │    job_subdir_key = None                                                 │ │
│ │           old_cwd = '/home/paperspace/Documents/Repos/clm_model_tuning'  │ │
│ │    orig_hydra_cfg = None                                                 │ │
│ │        output_dir = 'outputs/2022-11-18/16-01-29'                        │ │
│ │         overrides = [                                                    │ │
│ │                     │   'model.name=facebook/opt-2.7b',                  │ │
│ │                     │   'training.eval_every=250',                       │ │
│ │                     │   'training.train_batch_size=32',                  │ │
│ │                     │   'training.weight_decay=0.05',                    │ │
│ │                     │   'training.eval_batch_size=16',                   │ │
│ │                     │   'training.learning_rate=0.000003',               │ │
│ │                     │   'training.val_split_percent=20',                 │ │
│ │                     │   'training.num_epochs=4',                         │ │
│ │                     │   'training.lr_warmup_steps=5',                    │ │
│ │                     │   'training.gradient_accumulation_steps=2',        │ │
│ │                     │   ... +2                                           │ │
│ │                     ]                                                    │ │
│ │               ret = JobReturn(                                           │ │
│ │                     │   overrides=[                                      │ │
│ │                     │   │   'model.name=facebook/opt-2.7b',              │ │
│ │                     │   │   'training.eval_every=250',                   │ │
│ │                     │   │   'training.train_batch_size=32',              │ │
│ │                     │   │   'training.weight_decay=0.05',                │ │
│ │                     │   │   'training.eval_batch_size=16',               │ │
│ │                     │   │   'training.learning_rate=0.000003',           │ │
│ │                     │   │   'training.val_split_percent=20',             │ │
│ │                     │   │   'training.num_epochs=4',                     │ │
│ │                     │   │   'training.lr_warmup_steps=5',                │ │
│ │                     │   │   'training.gradient_accumulation_steps=2',    │ │
│ │                     │   │   ... +2                                       │ │
│ │                     │   ],                                               │ │
│ │                     │   cfg={'output_dir': 'tuned-model', 'bittensor':   │ │
│ │                     {'network': 'nobunaga'}, 'dataset': {'name':         │ │
│ │                     'ViktorThink/mountain_combined_813306',              │ │
│ │                     'config_name': None, 'num_batches': 5000,            │ │
│ │                     'block_size': 256, 'overwrite_cache': False,         │ │
│ │                     'keep_linebreaks': True, 'concatenate_raw': False,   │ │
│ │                     'load_tokenized_data': False}, 'model': {'name':     │ │
│ │                     'facebook/opt-2.7b', 'config_name': None},           │ │
│ │                     'tokenizer': {'name': None, 'use_fast': True,        │ │
│ │                     'preprocessing_num_workers': None, 'pad_token':      │ │
│ │                     '[PAD]'}, 'training': {'seed': 17,                   │ │
│ │                     'val_split_percent': 20, 'train_batch_size': 32,     │ │
│ │                     'eval_batch_size': 16, 'learning_rate': 3e-06,       │ │
│ │                     'weight_decay': 0.05, 'num_epochs': 4,               │ │
│ │                     'max_train_steps': None,                             │ │
│ │                     'gradient_accumulation_steps': 2, 'lr_scheduler':    │ │
│ │                     'constant', 'lr_warmup_steps': 5, 'eval_every': 250, │ │
│ │                     'max_eval_steps': 500, 'checkpoint':                 │ │
│ │                     {'resume_from_checkpoint': 0, 'every_n_steps':       │ │
│ │                     None}}, 'tracking': {'enabled': True, 'report_to':   │ │
│ │                     'all'}, 'testing': {'enabled': False}},              │ │
│ │                     │   hydra_cfg={'hydra': {'run': {'dir':              │ │
│ │                     'outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}'}, 'sweep': │ │
│ │                     {'dir': 'multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}',  │ │
│ │                     'subdir': '${hydra.job.num}'}, 'launcher':           │ │
│ │                     {'_target_':                                         │ │
│ │                     'hydra._internal.core_plugins.basic_launcher.BasicL… │ │
│ │                     'sweeper': {'_target_':                              │ │
│ │                     'hydra._internal.core_plugins.basic_sweeper.BasicSw… │ │
│ │                     'max_batch_size': None, 'params': None}, 'help':     │ │
│ │                     {'app_name': '${hydra.job.name}', 'header':          │ │
│ │                     '${hydra.help.app_name} is powered by Hydra.\n',     │ │
│ │                     'footer': 'Powered by Hydra (https://hydra.cc)\nUse  │ │
│ │                     --hydra-help to view Hydra specific help\n',         │ │
│ │                     'template': '${hydra.help.header}\n== Configuration  │ │
│ │                     groups ==\nCompose your configuration from those     │ │
│ │                     groups (group=option)\n\n$APP_CONFIG_GROUPS\n\n==    │ │
│ │                     Config ==\nOverride anything in the config           │ │
│ │                     (foo.bar=value)\n\n$CONFIG\n\n${hydra.help.footer}\… │ │
│ │                     'hydra_help': {'template': "Hydra                    │ │
│ │                     (${hydra.runtime.version})\nSee https://hydra.cc for │ │
│ │                     more info.\n\n== Flags ==\n$FLAGS_HELP\n\n==         │ │
│ │                     Configuration groups ==\nCompose your configuration  │ │
│ │                     from those groups (For example, append               │ │
│ │                     hydra/job_logging=disabled to command                │ │
│ │                     line)\n\n$HYDRA_CONFIG_GROUPS\n\nUse '--cfg hydra'   │ │
│ │                     to Show the Hydra config.\n", 'hydra_help': '???'},  │ │
│ │                     'hydra_logging': {'version': 1, 'formatters':        │ │
│ │                     {'simple': {'format': '[%(asctime)s][HYDRA]          │ │
│ │                     %(message)s'}}, 'handlers': {'console': {'class':    │ │
│ │                     'logging.StreamHandler', 'formatter': 'simple',      │ │
│ │                     'stream': 'ext://sys.stdout'}}, 'root': {'level':    │ │
│ │                     'INFO', 'handlers': ['console']}, 'loggers':         │ │
│ │                     {'logging_example': {'level': 'DEBUG'}},             │ │
│ │                     'disable_existing_loggers': False}, 'job_logging':   │ │
│ │                     {'version': 1, 'formatters': {'simple': {'format':   │ │
│ │                     '[%(asctime)s][%(name)s][%(levelname)s] -            │ │
│ │                     %(message)s'}}, 'handlers': {'console': {'class':    │ │
│ │                     'logging.StreamHandler', 'formatter': 'simple',      │ │
│ │                     'stream': 'ext://sys.stdout'}, 'file': {'class':     │ │
│ │                     'logging.FileHandler', 'formatter': 'simple',        │ │
│ │                     'filename':                                          │ │
│ │                     '${hydra.runtime.output_dir}/${hydra.job.name}.log'… │ │
│ │                     'root': {'level': 'INFO', 'handlers': ['console',    │ │
│ │                     'file']}, 'disable_existing_loggers': False}, 'env': │ │
│ │                     {}, 'mode': <RunMode.RUN: 1>, 'searchpath': [],      │ │
│ │                     'callbacks': {}, 'output_subdir': '.hydra',          │ │
│ │                     'overrides': {'hydra': ['hydra.mode=RUN'], 'task':   │ │
│ │                     ['model.name=facebook/opt-2.7b',                     │ │
│ │                     'training.eval_every=250',                           │ │
│ │                     'training.train_batch_size=32',                      │ │
│ │                     'training.weight_decay=0.05',                        │ │
│ │                     'training.eval_batch_size=16',                       │ │
│ │                     'training.learning_rate=0.000003',                   │ │
│ │                     'training.val_split_percent=20',                     │ │
│ │                     'training.num_epochs=4',                             │ │
│ │                     'training.lr_warmup_steps=5',                        │ │
│ │                     'training.gradient_accumulation_steps=2',            │ │
│ │                     'dataset.name=ViktorThink/mountain_combined_813306', │ │
│ │                     'dataset.num_batches=5000']}, 'job': {'name':        │ │
│ │                     'finetune_using_clm_wandb', 'chdir': None,           │ │
│ │                     'override_dirname':                                  │ │
│ │                     'dataset.name=ViktorThink/mountain_combined_813306,… │ │
│ │                     'id': '???', 'num': '???', 'config_name': 'config',  │ │
│ │                     'env_set': {}, 'env_copy': [], 'config':             │ │
│ │                     {'override_dirname': {'kv_sep': '=', 'item_sep':     │ │
│ │                     ',', 'exclude_keys': []}}}, 'runtime': {'version':   │ │
│ │                     '1.2.0', 'version_base': '1.2', 'cwd':               │ │
│ │                     '/home/paperspace/Documents/Repos/clm_model_tuning', │ │
│ │                     'config_sources': [{'path': 'hydra.conf', 'schema':  │ │
│ │                     'pkg', 'provider': 'hydra'}, {'path':                │ │
│ │                     '/home/paperspace/Documents/Repos/clm_model_tuning/… │ │
│ │                     'schema': 'file', 'provider': 'main'}, {'path': '',  │ │
│ │                     'schema': 'structured', 'provider': 'schema'}],      │ │
│ │                     'output_dir':                                        │ │
│ │                     '/home/paperspace/Documents/Repos/clm_model_tuning/… │ │
│ │                     'choices': {'hydra/env': 'default',                  │ │
│ │                     'hydra/callbacks': None, 'hydra/job_logging':        │ │
│ │                     'default', 'hydra/hydra_logging': 'default',         │ │
│ │                     'hydra/hydra_help': 'default', 'hydra/help':         │ │
│ │                     'default', 'hydra/sweeper': 'basic',                 │ │
│ │                     'hydra/launcher': 'basic', 'hydra/output':           │ │
│ │                     'default'}}, 'verbose': False}},                     │ │
│ │                     │                                                    │ │
│ │                     working_dir='/home/paperspace/Documents/Repos/clm_m… │ │
│ │                     │   task_name=None,                                  │ │
│ │                     │   status=<JobStatus.UNKNOWN: 0>,                   │ │
│ │                     │   _return_value=None                               │ │
│ │                     )                                                    │ │
│ │          task_cfg = {'output_dir': 'tuned-model', 'bittensor':           │ │
│ │                     {'network': 'nobunaga'}, 'dataset': {'name':         │ │
│ │                     'ViktorThink/mountain_combined_813306',              │ │
│ │                     'config_name': None, 'num_batches': 5000,            │ │
│ │                     'block_size': 256, 'overwrite_cache': False,         │ │
│ │                     'keep_linebreaks': True, 'concatenate_raw': False,   │ │
│ │                     'load_tokenized_data': False}, 'model': {'name':     │ │
│ │                     'facebook/opt-2.7b', 'config_name': None},           │ │
│ │                     'tokenizer': {'name': None, 'use_fast': True,        │ │
│ │                     'preprocessing_num_workers': None, 'pad_token':      │ │
│ │                     '[PAD]'}, 'training': {'seed': 17,                   │ │
│ │                     'val_split_percent': 20, 'train_batch_size': 32,     │ │
│ │                     'eval_batch_size': 16, 'learning_rate': 3e-06,       │ │
│ │                     'weight_decay': 0.05, 'num_epochs': 4,               │ │
│ │                     'max_train_steps': None,                             │ │
│ │                     'gradient_accumulation_steps': 2, 'lr_scheduler':    │ │
│ │                     'constant', 'lr_warmup_steps': 5, 'eval_every': 250, │ │
│ │                     'max_eval_steps': 500, 'checkpoint':                 │ │
│ │                     {'resume_from_checkpoint': 0, 'every_n_steps':       │ │
│ │                     None}}, 'tracking': {'enabled': True, 'report_to':   │ │
│ │                     'all'}, 'testing': {'enabled': False}}               │ │
│ │     task_function = <function main at 0x7f46807533a0>                    │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/finetune_using_clm_wandb.p │
│ y:304 in main                                                                │
│                                                                              │
│   301 │   else:                                                              │
│   302 │   │   raw_datasets = load_raw_datasets(cfg)                          │
│   303 │   │                                                                  │
│ ❱ 304 │   │   tokenized_datasets = preprocess(cfg, accelerator, tokenizer, r │
│   305 │   │   if "train" not in tokenized_datasets.column_names:             │
│   306 │   │   │   tokenized_datasets = tokenized_datasets.train_test_split(  │
│   307 │   │   │   │   test_size=cfg.training.val_split_percent / 100,        │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │  accelerator = <accelerate.accelerator.Accelerator object at             │ │
│ │                0x7f4680635700>                                           │ │
│ │          cfg = {'output_dir': 'tuned-model', 'bittensor': {'network':    │ │
│ │                'nobunaga'}, 'dataset': {'name':                          │ │
│ │                'ViktorThink/mountain_combined_813306', 'config_name':    │ │
│ │                None, 'num_batches': 5000, 'block_size': 256,             │ │
│ │                'overwrite_cache': False, 'keep_linebreaks': True,        │ │
│ │                'concatenate_raw': False, 'load_tokenized_data': False},  │ │
│ │                'model': {'name': 'facebook/opt-2.7b', 'config_name':     │ │
│ │                None}, 'tokenizer': {'name': None, 'use_fast': True,      │ │
│ │                'preprocessing_num_workers': None, 'pad_token': '[PAD]'}, │ │
│ │                'training': {'seed': 17, 'val_split_percent': 20,         │ │
│ │                'train_batch_size': 32, 'eval_batch_size': 16,            │ │
│ │                'learning_rate': 3e-06, 'weight_decay': 0.05,             │ │
│ │                'num_epochs': 4, 'max_train_steps': None,                 │ │
│ │                'gradient_accumulation_steps': 2, 'lr_scheduler':         │ │
│ │                'constant', 'lr_warmup_steps': 5, 'eval_every': 250,      │ │
│ │                'max_eval_steps': 500, 'checkpoint':                      │ │
│ │                {'resume_from_checkpoint': 0, 'every_n_steps': None}},    │ │
│ │                'tracking': {'enabled': True, 'report_to': 'all'},        │ │
│ │                'testing': {'enabled': False}}                            │ │
│ │       logger = <MultiProcessAdapter __main__ (INFO)>                     │ │
│ │ lr_scheduler = <torch.optim.lr_scheduler.LambdaLR object at              │ │
│ │                0x7f467aebf640>                                           │ │
│ │        model = GPTNeoForCausalLM(                                        │ │
│ │                  (transformer): GPTNeoModel(                             │ │
│ │                │   (wte): Embedding(50265, 2560)                         │ │
│ │                │   (wpe): Embedding(2048, 2560)                          │ │
│ │                │   (drop): Dropout(p=0.0, inplace=False)                 │ │
│ │                │   (h): ModuleList(                                      │ │
│ │                │     (0): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (1): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (2): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (3): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (4): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (5): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (6): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (7): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (8): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (9): GPTNeoBlock(                                   │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (10): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (11): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (12): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (13): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (14): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (15): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (16): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (17): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (18): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (19): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (20): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (21): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (22): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (23): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (24): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (25): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (26): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (27): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (28): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (29): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (30): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │     (31): GPTNeoBlock(                                  │ │
│ │                │   │   (ln_1): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (attn): GPTNeoAttention(                          │ │
│ │                │   │     (attention): GPTNeoSelfAttention(               │ │
│ │                │   │   │   (attn_dropout): Dropout(p=0.0, inplace=False) │ │
│ │                │   │   │   (resid_dropout): Dropout(p=0.0,               │ │
│ │                inplace=False)                                            │ │
│ │                │   │   │   (k_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (v_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (q_proj): Linear(in_features=2560,            │ │
│ │                out_features=2560, bias=False)                            │ │
│ │                │   │   │   (out_proj): Linear(in_features=2560,          │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     )                                               │ │
│ │                │   │   )                                                 │ │
│ │                │   │   (ln_2): LayerNorm((2560,), eps=1e-05,             │ │
│ │                elementwise_affine=True)                                  │ │
│ │                │   │   (mlp): GPTNeoMLP(                                 │ │
│ │                │   │     (c_fc): Linear(in_features=2560,                │ │
│ │                out_features=10240, bias=True)                            │ │
│ │                │   │     (c_proj): Linear(in_features=10240,             │ │
│ │                out_features=2560, bias=True)                             │ │
│ │                │   │     (act): NewGELUActivation()                      │ │
│ │                │   │     (dropout): Dropout(p=0.0, inplace=False)        │ │
│ │                │   │   )                                                 │ │
│ │                │     )                                                   │ │
│ │                │   )                                                     │ │
│ │                │   (ln_f): LayerNorm((2560,), eps=1e-05,                 │ │
│ │                elementwise_affine=True)                                  │ │
│ │                  )                                                       │ │
│ │                  (lm_head): Linear(in_features=2560, out_features=50265, │ │
│ │                bias=False)                                               │ │
│ │                )                                                         │ │
│ │    optimizer = AdamW (                                                   │ │
│ │                Parameter Group 0                                         │ │
│ │                │   amsgrad: False                                        │ │
│ │                │   betas: (0.9, 0.999)                                   │ │
│ │                │   capturable: False                                     │ │
│ │                │   eps: 1e-08                                            │ │
│ │                │   foreach: None                                         │ │
│ │                │   initial_lr: 3e-06                                     │ │
│ │                │   lr: 3e-06                                             │ │
│ │                │   maximize: False                                       │ │
│ │                │   weight_decay: 0.05                                    │ │
│ │                                                                          │ │
│ │                Parameter Group 1                                         │ │
│ │                │   amsgrad: False                                        │ │
│ │                │   betas: (0.9, 0.999)                                   │ │
│ │                │   capturable: False                                     │ │
│ │                │   eps: 1e-08                                            │ │
│ │                │   foreach: None                                         │ │
│ │                │   initial_lr: 3e-06                                     │ │
│ │                │   lr: 3e-06                                             │ │
│ │                │   maximize: False                                       │ │
│ │                │   weight_decay: 0.0                                     │ │
│ │                )                                                         │ │
│ │ raw_datasets = DatasetDict({                                             │ │
│ │                │   train: Dataset({                                      │ │
│ │                │   │   features: ['train', 'validation'],                │ │
│ │                │   │   num_rows: 813306                                  │ │
│ │                │   })                                                    │ │
│ │                │   validation: Dataset({                                 │ │
│ │                │   │   features: ['train', 'validation'],                │ │
│ │                │   │   num_rows: 813306                                  │ │
│ │                │   })                                                    │ │
│ │                })                                                        │ │
│ │    tokenizer = PreTrainedTokenizer(name_or_path='facebook/opt-2.7b',     │ │
│ │                vocab_size=50265,                                         │ │
│ │                model_max_len=1000000000000000019884624838656,            │ │
│ │                is_fast=False, padding_side='right',                      │ │
│ │                truncation_side='right', special_tokens={'bos_token':     │ │
│ │                AddedToken("</s>", rstrip=False, lstrip=False,            │ │
│ │                single_word=False, normalized=True), 'eos_token':         │ │
│ │                AddedToken("</s>", rstrip=False, lstrip=False,            │ │
│ │                single_word=False, normalized=True), 'unk_token':         │ │
│ │                AddedToken("</s>", rstrip=False, lstrip=False,            │ │
│ │                single_word=False, normalized=True), 'pad_token':         │ │
│ │                AddedToken("<pad>", rstrip=False, lstrip=False,           │ │
│ │                single_word=False, normalized=True)})                     │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/finetune_using_clm_wandb.p │
│ y:215 in preprocess                                                          │
│                                                                              │
│   212 │                                                                      │
│   213 │   with accelerator.main_process_first():                             │
│   214 │   │                                                                  │
│ ❱ 215 │   │   tokenized_datasets = raw_datasets.map(                         │
│   216 │   │   │   tokenize_fn,                                               │
│   217 │   │   │   batched=True,                                              │
│   218 │   │   │   num_proc=cfg.tokenizer.preprocessing_num_workers,          │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │      accelerator = <accelerate.accelerator.Accelerator object at         │ │
│ │                    0x7f4680635700>                                       │ │
│ │              cfg = {'output_dir': 'tuned-model', 'bittensor':            │ │
│ │                    {'network': 'nobunaga'}, 'dataset': {'name':          │ │
│ │                    'ViktorThink/mountain_combined_813306',               │ │
│ │                    'config_name': None, 'num_batches': 5000,             │ │
│ │                    'block_size': 256, 'overwrite_cache': False,          │ │
│ │                    'keep_linebreaks': True, 'concatenate_raw': False,    │ │
│ │                    'load_tokenized_data': False}, 'model': {'name':      │ │
│ │                    'facebook/opt-2.7b', 'config_name': None},            │ │
│ │                    'tokenizer': {'name': None, 'use_fast': True,         │ │
│ │                    'preprocessing_num_workers': None, 'pad_token':       │ │
│ │                    '[PAD]'}, 'training': {'seed': 17,                    │ │
│ │                    'val_split_percent': 20, 'train_batch_size': 32,      │ │
│ │                    'eval_batch_size': 16, 'learning_rate': 3e-06,        │ │
│ │                    'weight_decay': 0.05, 'num_epochs': 4,                │ │
│ │                    'max_train_steps': None,                              │ │
│ │                    'gradient_accumulation_steps': 2, 'lr_scheduler':     │ │
│ │                    'constant', 'lr_warmup_steps': 5, 'eval_every': 250,  │ │
│ │                    'max_eval_steps': 500, 'checkpoint':                  │ │
│ │                    {'resume_from_checkpoint': 0, 'every_n_steps':        │ │
│ │                    None}}, 'tracking': {'enabled': True, 'report_to':    │ │
│ │                    'all'}, 'testing': {'enabled': False}}                │ │
│ │     column_names = {                                                     │ │
│ │                    │   'train': ['train', 'validation'],                 │ │
│ │                    │   'validation': ['train', 'validation']             │ │
│ │                    }                                                     │ │
│ │      group_texts = <function preprocess.<locals>.group_texts at          │ │
│ │                    0x7f46782113a0>                                       │ │
│ │              pad = 'max_length'                                          │ │
│ │     raw_datasets = DatasetDict({                                         │ │
│ │                    │   train: Dataset({                                  │ │
│ │                    │   │   features: ['train', 'validation'],            │ │
│ │                    │   │   num_rows: 813306                              │ │
│ │                    │   })                                                │ │
│ │                    │   validation: Dataset({                             │ │
│ │                    │   │   features: ['train', 'validation'],            │ │
│ │                    │   │   num_rows: 813306                              │ │
│ │                    │   })                                                │ │
│ │                    })                                                    │ │
│ │ text_column_name = 'train'                                               │ │
│ │      tokenize_fn = <function preprocess.<locals>.tokenize_fn at          │ │
│ │                    0x7f4678211670>                                       │ │
│ │        tokenizer = PreTrainedTokenizer(name_or_path='facebook/opt-2.7b', │ │
│ │                    vocab_size=50265,                                     │ │
│ │                    model_max_len=1000000000000000019884624838656,        │ │
│ │                    is_fast=False, padding_side='right',                  │ │
│ │                    truncation_side='right', special_tokens={'bos_token': │ │
│ │                    AddedToken("</s>", rstrip=False, lstrip=False,        │ │
│ │                    single_word=False, normalized=True), 'eos_token':     │ │
│ │                    AddedToken("</s>", rstrip=False, lstrip=False,        │ │
│ │                    single_word=False, normalized=True), 'unk_token':     │ │
│ │                    AddedToken("</s>", rstrip=False, lstrip=False,        │ │
│ │                    single_word=False, normalized=True), 'pad_token':     │ │
│ │                    AddedToken("<pad>", rstrip=False, lstrip=False,       │ │
│ │                    single_word=False, normalized=True)})                 │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/dataset_dict.py:777 in map                                   │
│                                                                              │
│    774 │   │   if cache_file_names is None:                                  │
│    775 │   │   │   cache_file_names = {k: None for k in self}                │
│    776 │   │   return DatasetDict(                                           │
│ ❱  777 │   │   │   {                                                         │
│    778 │   │   │   │   k: dataset.map(                                       │
│    779 │   │   │   │   │   function=function,                                │
│    780 │   │   │   │   │   with_indices=with_indices,                        │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │           batch_size = 1000                                              │ │
│ │              batched = True                                              │ │
│ │     cache_file_names = {'train': None, 'validation': None}               │ │
│ │                 desc = 'Running tokenizer on dataset'                    │ │
│ │     disable_nullable = False                                             │ │
│ │      drop_last_batch = False                                             │ │
│ │             features = None                                              │ │
│ │            fn_kwargs = None                                              │ │
│ │             function = <function preprocess.<locals>.tokenize_fn at      │ │
│ │                        0x7f4678211670>                                   │ │
│ │        input_columns = None                                              │ │
│ │       keep_in_memory = False                                             │ │
│ │ load_from_cache_file = True                                              │ │
│ │             num_proc = None                                              │ │
│ │       remove_columns = None                                              │ │
│ │                 self = DatasetDict({                                     │ │
│ │                        │   train: Dataset({                              │ │
│ │                        │   │   features: ['train', 'validation'],        │ │
│ │                        │   │   num_rows: 813306                          │ │
│ │                        │   })                                            │ │
│ │                        │   validation: Dataset({                         │ │
│ │                        │   │   features: ['train', 'validation'],        │ │
│ │                        │   │   num_rows: 813306                          │ │
│ │                        │   })                                            │ │
│ │                        })                                                │ │
│ │         with_indices = False                                             │ │
│ │            with_rank = False                                             │ │
│ │    writer_batch_size = 1000                                              │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/dataset_dict.py:778 in <dictcomp>                            │
│                                                                              │
│    775 │   │   │   cache_file_names = {k: None for k in self}                │
│    776 │   │   return DatasetDict(                                           │
│    777 │   │   │   {                                                         │
│ ❱  778 │   │   │   │   k: dataset.map(                                       │
│    779 │   │   │   │   │   function=function,                                │
│    780 │   │   │   │   │   with_indices=with_indices,                        │
│    781 │   │   │   │   │   with_rank=with_rank,                              │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │                   .0 = <dict_itemiterator object at 0x7f467ad4ad60>      │ │
│ │           batch_size = 1000                                              │ │
│ │              batched = True                                              │ │
│ │     cache_file_names = {'train': None, 'validation': None}               │ │
│ │              dataset = Dataset({                                         │ │
│ │                        │   features: ['train', 'validation'],            │ │
│ │                        │   num_rows: 813306                              │ │
│ │                        })                                                │ │
│ │                 desc = 'Running tokenizer on dataset'                    │ │
│ │     disable_nullable = False                                             │ │
│ │      drop_last_batch = False                                             │ │
│ │             features = None                                              │ │
│ │            fn_kwargs = None                                              │ │
│ │             function = <function preprocess.<locals>.tokenize_fn at      │ │
│ │                        0x7f4678211670>                                   │ │
│ │        input_columns = None                                              │ │
│ │                    k = 'train'                                           │ │
│ │       keep_in_memory = False                                             │ │
│ │ load_from_cache_file = True                                              │ │
│ │             num_proc = None                                              │ │
│ │       remove_columns = None                                              │ │
│ │         with_indices = False                                             │ │
│ │            with_rank = False                                             │ │
│ │    writer_batch_size = 1000                                              │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/arrow_dataset.py:2585 in map                                 │
│                                                                              │
│   2582 │   │   disable_tqdm = not logging.is_progress_bar_enabled()          │
│   2583 │   │                                                                 │
│   2584 │   │   if num_proc is None or num_proc == 1:                         │
│ ❱ 2585 │   │   │   return self._map_single(                                  │
│   2586 │   │   │   │   function=function,                                    │
│   2587 │   │   │   │   with_indices=with_indices,                            │
│   2588 │   │   │   │   with_rank=with_rank,                                  │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │           batch_size = 1000                                              │ │
│ │              batched = True                                              │ │
│ │      cache_file_name = None                                              │ │
│ │             decorate = <function Dataset.map.<locals>.decorate at        │ │
│ │                        0x7f45bc6e3ee0>                                   │ │
│ │                 desc = 'Running tokenizer on dataset'                    │ │
│ │     disable_nullable = False                                             │ │
│ │         disable_tqdm = False                                             │ │
│ │      drop_last_batch = False                                             │ │
│ │             features = None                                              │ │
│ │            fn_kwargs = {}                                                │ │
│ │             function = <function preprocess.<locals>.tokenize_fn at      │ │
│ │                        0x7f45bc6e3e50>                                   │ │
│ │        input_columns = None                                              │ │
│ │       keep_in_memory = False                                             │ │
│ │ load_from_cache_file = True                                              │ │
│ │      new_fingerprint = None                                              │ │
│ │             num_proc = None                                              │ │
│ │       remove_columns = None                                              │ │
│ │                 self = Dataset({                                         │ │
│ │                        │   features: ['train', 'validation'],            │ │
│ │                        │   num_rows: 813306                              │ │
│ │                        })                                                │ │
│ │      suffix_template = '_{rank:05d}_of_{num_proc:05d}'                   │ │
│ │         with_indices = False                                             │ │
│ │            with_rank = False                                             │ │
│ │    writer_batch_size = 1000                                              │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/arrow_dataset.py:585 in wrapper                              │
│                                                                              │
│    582 │   │   else:                                                         │
│    583 │   │   │   self: "Dataset" = kwargs.pop("self")                      │
│    584 │   │   # apply actual function                                       │
│ ❱  585 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
│    586 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
│    587 │   │   for dataset in datasets:                                      │
│    588 │   │   │   # Remove task templates if a column mapping of the templa │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │   args = ()                                                              │ │
│ │   func = <function Dataset._map_single at 0x7f46c90be8b0>                │ │
│ │ kwargs = {                                                               │ │
│ │          │   'function': <function preprocess.<locals>.tokenize_fn at    │ │
│ │          0x7f45bc6e3e50>,                                                │ │
│ │          │   'with_indices': False,                                      │ │
│ │          │   'with_rank': False,                                         │ │
│ │          │   'input_columns': None,                                      │ │
│ │          │   'batched': True,                                            │ │
│ │          │   'batch_size': 1000,                                         │ │
│ │          │   'drop_last_batch': False,                                   │ │
│ │          │   'remove_columns': None,                                     │ │
│ │          │   'keep_in_memory': False,                                    │ │
│ │          │   'load_from_cache_file': True,                               │ │
│ │          │   ... +8                                                      │ │
│ │          }                                                               │ │
│ │   self = Dataset({                                                       │ │
│ │          │   features: ['train', 'validation'],                          │ │
│ │          │   num_rows: 813306                                            │ │
│ │          })                                                              │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/arrow_dataset.py:552 in wrapper                              │
│                                                                              │
│    549 │   │   │   "output_all_columns": self._output_all_columns,           │
│    550 │   │   }                                                             │
│    551 │   │   # apply actual function                                       │
│ ❱  552 │   │   out: Union["Dataset", "DatasetDict"] = func(self, *args, **kw │
│    553 │   │   datasets: List["Dataset"] = list(out.values()) if isinstance( │
│    554 │   │   # re-apply format to the output                               │
│    555 │   │   for dataset in datasets:                                      │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │                args = ()                                                 │ │
│ │                func = <function Dataset._map_single at 0x7f46c90be9d0>   │ │
│ │              kwargs = {                                                  │ │
│ │                       │   'function': <function                          │ │
│ │                       preprocess.<locals>.tokenize_fn at                 │ │
│ │                       0x7f45bc6e3e50>,                                   │ │
│ │                       │   'with_indices': False,                         │ │
│ │                       │   'with_rank': False,                            │ │
│ │                       │   'input_columns': None,                         │ │
│ │                       │   'batched': True,                               │ │
│ │                       │   'batch_size': 1000,                            │ │
│ │                       │   'drop_last_batch': False,                      │ │
│ │                       │   'remove_columns': None,                        │ │
│ │                       │   'keep_in_memory': False,                       │ │
│ │                       │   'load_from_cache_file': True,                  │ │
│ │                       │   ... +8                                         │ │
│ │                       }                                                  │ │
│ │                self = Dataset({                                          │ │
│ │                       │   features: ['train', 'validation'],             │ │
│ │                       │   num_rows: 813306                               │ │
│ │                       })                                                 │ │
│ │         self_format = {                                                  │ │
│ │                       │   'type': None,                                  │ │
│ │                       │   'format_kwargs': {},                           │ │
│ │                       │   'columns': None,                               │ │
│ │                       │   'output_all_columns': False                    │ │
│ │                       }                                                  │ │
│ │ unformatted_columns = {'validation', 'train'}                            │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/fingerprint.py:480 in wrapper                                │
│                                                                              │
│   477 │   │   │                                                              │
│   478 │   │   │   # Call actual function                                     │
│   479 │   │   │                                                              │
│ ❱ 480 │   │   │   out = func(self, *args, **kwargs)                          │
│   481 │   │   │                                                              │
│   482 │   │   │   # Update fingerprint of in-place transforms + update in-pl │
│   483                                                                        │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │                   args = ()                                              │ │
│ │          default_value = False                                           │ │
│ │         default_values = {                                               │ │
│ │                          │   'function': None,                           │ │
│ │                          │   'with_indices': False,                      │ │
│ │                          │   'with_rank': False,                         │ │
│ │                          │   'input_columns': None,                      │ │
│ │                          │   'batched': False,                           │ │
│ │                          │   'batch_size': 1000,                         │ │
│ │                          │   'drop_last_batch': False,                   │ │
│ │                          │   'remove_columns': None,                     │ │
│ │                          │   'keep_in_memory': False,                    │ │
│ │                          │   'load_from_cache_file': None,               │ │
│ │                          │   ... +11                                     │ │
│ │                          }                                               │ │
│ │        default_varname = 'cache_only'                                    │ │
│ │       fingerprint_name = 'new_fingerprint'                               │ │
│ │      fingerprint_names = ['new_fingerprint']                             │ │
│ │                   func = <function Dataset._map_single at                │ │
│ │                          0x7f46c90be940>                                 │ │
│ │          ignore_kwargs = [                                               │ │
│ │                          │   'load_from_cache_file',                     │ │
│ │                          │   'cache_file_name',                          │ │
│ │                          │   'disable_tqdm',                             │ │
│ │                          │   'desc',                                     │ │
│ │                          │   'cache_only'                                │ │
│ │                          ]                                               │ │
│ │                inplace = False                                           │ │
│ │                 kwargs = {                                               │ │
│ │                          │   'function': <function                       │ │
│ │                          preprocess.<locals>.tokenize_fn at              │ │
│ │                          0x7f45bc6e3e50>,                                │ │
│ │                          │   'with_indices': False,                      │ │
│ │                          │   'with_rank': False,                         │ │
│ │                          │   'input_columns': None,                      │ │
│ │                          │   'batched': True,                            │ │
│ │                          │   'batch_size': 1000,                         │ │
│ │                          │   'drop_last_batch': False,                   │ │
│ │                          │   'remove_columns': None,                     │ │
│ │                          │   'keep_in_memory': False,                    │ │
│ │                          │   'load_from_cache_file': True,               │ │
│ │                          │   ... +8                                      │ │
│ │                          }                                               │ │
│ │ kwargs_for_fingerprint = {                                               │ │
│ │                          │   'function': <function                       │ │
│ │                          preprocess.<locals>.tokenize_fn at              │ │
│ │                          0x7f45bc6e3e50>,                                │ │
│ │                          │   'batched': True,                            │ │
│ │                          │   'fn_kwargs': {},                            │ │
│ │                          │   'fingerprint_name': 'new_fingerprint'       │ │
│ │                          }                                               │ │
│ │                 params = [                                               │ │
│ │                          │   'function',                                 │ │
│ │                          │   'with_indices',                             │ │
│ │                          │   'with_rank',                                │ │
│ │                          │   'input_columns',                            │ │
│ │                          │   'batched',                                  │ │
│ │                          │   'batch_size',                               │ │
│ │                          │   'drop_last_batch',                          │ │
│ │                          │   'remove_columns',                           │ │
│ │                          │   'keep_in_memory',                           │ │
│ │                          │   'load_from_cache_file',                     │ │
│ │                          │   ... +11                                     │ │
│ │                          ]                                               │ │
│ │    randomized_function = False                                           │ │
│ │                   self = Dataset({                                       │ │
│ │                          │   features: ['train', 'validation'],          │ │
│ │                          │   num_rows: 813306                            │ │
│ │                          })                                              │ │
│ │              transform = 'datasets.arrow_dataset.Dataset._map_single'    │ │
│ │             use_kwargs = None                                            │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
│                                                                              │
│ /home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-pa │
│ ckages/datasets/arrow_dataset.py:3009 in _map_single                         │
│                                                                              │
│   3006 │   │   │   │   │   if tmp_file is not None:                          │
│   3007 │   │   │   │   │   │   tmp_file.close()                              │
│   3008 │   │   │   │   │   │   if os.path.exists(tmp_file.name):             │
│ ❱ 3009 │   │   │   │   │   │   │   os.remove(tmp_file.name)                  │
│   3010 │   │   │   │   raise                                                 │
│   3011 │   │                                                                 │
│   3012 │   │   if update_data and tmp_file is not None:                      │
│                                                                              │
│ ╭───────────────────────────────── locals ─────────────────────────────────╮ │
│ │ apply_function_on_filtered_inputs = <function                            │ │
│ │                                     Dataset._map_single.<locals>.apply_… │ │
│ │                                     at 0x7f467d7d7820>                   │ │
│ │                             batch = {                                    │ │
│ │                                     │   'train': [                       │ │
│ │                                     │   │   'upon better and\\nnobler    │ │
│ │                                     views; and advise your elder         │ │
│ │                                     sisters, when they perceive'+1575,   │ │
│ │                                     │   │   "log time reviewing U.S.     │ │
│ │                                     Army training films from the Vietnam │ │
│ │                                     era. That's the mil"+1355,           │ │
│ │                                     │   │   'the speculative             │ │
│ │                                     philosophers. For example, the great │ │
│ │                                     mathematician Euler, who do'+1567,   │ │
│ │                                     │   │   'the bedchamber. A short     │ │
│ │                                     while later he confided in one of    │ │
│ │                                     his servants, secretly'+1331,        │ │
│ │                                     │   │   'the familiar bond valuation │ │
│ │                                     equation. Assuming semiannual coupon │ │
│ │                                     payments, the e'+1402,               │ │
│ │                                     │   │   '$X$ associated with         │ │
│ │                                     $U\\\\bar{U}$ and $\\\\bar{D}{D}$    │ │
│ │                                     since this would drop the            │ │
│ │                                     poss'+1860,                          │ │
│ │                                     │   │   'parking lot                 │ │
│ │                                     dings.\\"\\n\\n\\"When these little  │ │
│ │                                     charges blow, they\'ll leave M16     │ │
│ │                                     impa'+1470,                          │ │
│ │                                     │   │   'above from Helms, \\"The    │ │
│ │                                     Indians,\\" pp. 37–45. \\n | Bruhns, │ │
│ │                                     _Ancient South Americ'+1388,         │ │
│ │                                     │   │   'get some sleep, so I        │ │
│ │                                     removed the light bulb and stored it │ │
│ │                                     in the pocket of the C'+1301,        │ │
│ │                                     │   │   'set of ADE20K in Table      │ │
│ │                                     \\\\[tab:baseline\\\\]. All our      │ │
│ │                                     results except the last-row o'+1732, │ │
│ │                                     │   │   ... +990                     │ │
│ │                                     │   ],                               │ │
│ │                                     │   'validation': [                  │ │
│ │                                     │   │   "given we can't stop people  │ │
│ │                                     signing up again with a new          │ │
│ │                                     disposable email address)"+1984,     │ │
│ │                                     │   │   'I\'m just repeating\'       │ │
│ │                                     stuff.\\" \\"Did I hear you say      │ │
│ │                                     you\'re looking for an               │ │
│ │                                     apartmen'+1451,                      │ │
│ │                                     │   │   'a model for belief. In M.   │ │
│ │                                     Vargas \\u0026 G. Yaffe (Eds.),      │ │
│ │                                     Rational and social age'+1793,       │ │
│ │                                     │   │   'away.\\"\\n\\n\\"Somehow, I │ │
│ │                                     think she knows where he is,\\"      │ │
│ │                                     Virginia said, thoughtful'+1451,     │ │
│ │                                     │   │   'plane the limits of         │ │
│ │                                     validity of our model.\\n\\n![(Color │ │
│ │                                     online) [*Characterizatio'+2041,     │ │
│ │                                     │   │   "son for fighting\\nagainst  │ │
│ │                                     the king and the mother country. The │ │
│ │                                     old lady's face w"+1653,             │ │
│ │                                     │   │   'again.\\" \\"We\'re gonna   │ │
│ │                                     have to make the cut now.\\"         │ │
│ │                                     \\"Sharon?\\" \\"can you hear        │ │
│ │                                     me'+1554,                            │ │
│ │                                     │   │   'even have more commissions  │ │
│ │                                     such as yours.\\"\\n\\nThat was a    │ │
│ │                                     not-so-veiled referenc'+1491,        │ │
│ │                                     │   │   'stealth technology we were  │ │
│ │                                     never able to break.\\" \\"So how    │ │
│ │                                     come all of a sudden '+1534,         │ │
│ │                                     │   │   'create\\na small example    │ │
│ │                                     service.\\nI want to focus on two    │ │
│ │                                     major requirements.\\nPe'+1838,      │ │
│ │                                     │   │   ... +990                     │ │
│ │                                     │   ]                                │ │
│ │                                     }                                    │ │
│ │                        batch_size = 1000                                 │ │
│ │                           batched = True                                 │ │
│ │                        buf_writer = None                                 │ │
│ │                   cache_file_name = '/home/paperspace/.cache/huggingfac… │ │
│ │                        cache_only = False                                │ │
│ │                              desc = 'Running tokenizer on dataset'       │ │
│ │                  disable_nullable = False                                │ │
│ │                      disable_tqdm = False                                │ │
│ │                   drop_last_batch = False                                │ │
│ │                          features = None                                 │ │
│ │                         fn_kwargs = {}                                   │ │
│ │                          function = <function                            │ │
│ │                                     preprocess.<locals>.tokenize_fn at   │ │
│ │                                     0x7f45bc6e3e50>                      │ │
│ │                                 i = 79000                                │ │
│ │                           indices = [                                    │ │
│ │                                     │   79000,                           │ │
│ │                                     │   79001,                           │ │
│ │                                     │   79002,                           │ │
│ │                                     │   79003,                           │ │
│ │                                     │   79004,                           │ │
│ │                                     │   79005,                           │ │
│ │                                     │   79006,                           │ │
│ │                                     │   79007,                           │ │
│ │                                     │   79008,                           │ │
│ │                                     │   79009,                           │ │
│ │                                     │   ... +990                         │ │
│ │                                     ]                                    │ │
│ │            init_buffer_and_writer = <function                            │ │
│ │                                     Dataset._map_single.<locals>.init_b… │ │
│ │                                     at 0x7f45bc6c0550>                   │ │
│ │                     input_columns = None                                 │ │
│ │                     input_dataset = Dataset({                            │ │
│ │                                     │   features: ['train',              │ │
│ │                                     'validation'],                       │ │
│ │                                     │   num_rows: 813306                 │ │
│ │                                     })                                   │ │
│ │                    keep_in_memory = False                                │ │
│ │              load_from_cache_file = True                                 │ │
│ │                   new_fingerprint = '4608445b9d629a4f'                   │ │
│ │                          num_rows = 813306                               │ │
│ │          NumExamplesMismatchError = <class                               │ │
│ │                                     'datasets.arrow_dataset.Dataset._ma… │ │
│ │                            offset = 0                                    │ │
│ │                              pbar = <tqdm.asyncio.tqdm_asyncio object at │ │
│ │                                     0x7f467de30a30>                      │ │
│ │                         pbar_desc = 'Running tokenizer on dataset'       │ │
│ │                     pbar_iterable = <zip object at 0x7f467820e4c0>       │ │
│ │                        pbar_total = 814                                  │ │
│ │                         pbar_unit = 'ba'                                 │ │
│ │                              rank = None                                 │ │
│ │                    remove_columns = None                                 │ │
│ │                              self = Dataset({                            │ │
│ │                                     │   features: ['train',              │ │
│ │                                     'validation'],                       │ │
│ │                                     │   num_rows: 813306                 │ │
│ │                                     })                                   │ │
│ │                             stack = <contextlib.ExitStack object at      │ │
│ │                                     0x7f467de301f0>                      │ │
│ │                          tmp_file = <tempfile._TemporaryFileWrapper      │ │
│ │                                     object at 0x7f467de12220>            │ │
│ │                       update_data = True                                 │ │
│ │          validate_function_output = <function                            │ │
│ │                                     Dataset._map_single.<locals>.valida… │ │
│ │                                     at 0x7f45bc6e3dc0>                   │ │
│ │                      with_indices = False                                │ │
│ │                         with_rank = False                                │ │
│ │                            writer = <datasets.arrow_writer.ArrowWriter   │ │
│ │                                     object at 0x7f467ddb13d0>            │ │
│ │                 writer_batch_size = 1000                                 │ │
│ ╰──────────────────────────────────────────────────────────────────────────╯ │
╰──────────────────────────────────────────────────────────────────────────────╯
KeyboardInterrupt
[2022-11-18 16:04:42,455][__main__][INFO] - Setting random seed to 17
[2022-11-18 16:04:42,456][__main__][INFO] - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

[2022-11-18 16:04:42,459][__main__][INFO] - output_dir: tuned-model
bittensor:
  network: nobunaga
dataset:
  name: ViktorThink/mountain_combined_813306
  config_name: null
  num_batches: 10
  block_size: 256
  overwrite_cache: false
  keep_linebreaks: true
  concatenate_raw: false
  load_tokenized_data: false
model:
  name: facebook/opt-2.7b
  config_name: null
tokenizer:
  name: null
  use_fast: true
  preprocessing_num_workers: null
  pad_token: '[PAD]'
training:
  seed: 17
  val_split_percent: 20
  train_batch_size: 32
  eval_batch_size: 16
  learning_rate: 3.0e-06
  weight_decay: 0.05
  num_epochs: 4
  max_train_steps: null
  gradient_accumulation_steps: 2
  lr_scheduler: constant
  lr_warmup_steps: 5
  eval_every: 250
  max_eval_steps: 500
  checkpoint:
    resume_from_checkpoint: 0
    every_n_steps: null
tracking:
  enabled: true
  report_to: all
testing:
  enabled: false

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/config.json
Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-2.7B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      16
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 20,
  "num_layers": 32,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading file vocab.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/vocab.json
loading file merges.txt from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/merges.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/tokenizer_config.json
loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading weights file flax_model.msgpack from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Loading Flax weights from /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Some weights of the Flax model were not used when initializing the PyTorch model GPTNeoForCausalLM: ['model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.final_layer_norm.weight', 'model.decoder.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.12.self_attn.k_proj.weight', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.weight', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.weight', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.12.self_attn_layer_norm.weight', 'model.decoder.layers.12.self_attn_layer_norm.bias', 'model.decoder.layers.12.fc1.weight', 'model.decoder.layers.12.fc1.bias', 'model.decoder.layers.12.fc2.weight', 'model.decoder.layers.12.fc2.bias', 'model.decoder.layers.12.final_layer_norm.weight', 'model.decoder.layers.12.final_layer_norm.bias', 'model.decoder.layers.13.self_attn.k_proj.weight', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.weight', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.weight', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.weight', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn_layer_norm.weight', 'model.decoder.layers.13.self_attn_layer_norm.bias', 'model.decoder.layers.13.fc1.weight', 'model.decoder.layers.13.fc1.bias', 'model.decoder.layers.13.fc2.weight', 'model.decoder.layers.13.fc2.bias', 'model.decoder.layers.13.final_layer_norm.weight', 'model.decoder.layers.13.final_layer_norm.bias', 'model.decoder.layers.14.self_attn.k_proj.weight', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.weight', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.weight', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.weight', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.self_attn_layer_norm.weight', 'model.decoder.layers.14.self_attn_layer_norm.bias', 'model.decoder.layers.14.fc1.weight', 'model.decoder.layers.14.fc1.bias', 'model.decoder.layers.14.fc2.weight', 'model.decoder.layers.14.fc2.bias', 'model.decoder.layers.14.final_layer_norm.weight', 'model.decoder.layers.14.final_layer_norm.bias', 'model.decoder.layers.15.self_attn.k_proj.weight', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.weight', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.weight', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.weight', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn_layer_norm.weight', 'model.decoder.layers.15.self_attn_layer_norm.bias', 'model.decoder.layers.15.fc1.weight', 'model.decoder.layers.15.fc1.bias', 'model.decoder.layers.15.fc2.weight', 'model.decoder.layers.15.fc2.bias', 'model.decoder.layers.15.final_layer_norm.weight', 'model.decoder.layers.15.final_layer_norm.bias', 'model.decoder.layers.16.self_attn.k_proj.weight', 'model.decoder.layers.16.self_attn.k_proj.bias', 'model.decoder.layers.16.self_attn.v_proj.weight', 'model.decoder.layers.16.self_attn.v_proj.bias', 'model.decoder.layers.16.self_attn.q_proj.weight', 'model.decoder.layers.16.self_attn.q_proj.bias', 'model.decoder.layers.16.self_attn.out_proj.weight', 'model.decoder.layers.16.self_attn.out_proj.bias', 'model.decoder.layers.16.self_attn_layer_norm.weight', 'model.decoder.layers.16.self_attn_layer_norm.bias', 'model.decoder.layers.16.fc1.weight', 'model.decoder.layers.16.fc1.bias', 'model.decoder.layers.16.fc2.weight', 'model.decoder.layers.16.fc2.bias', 'model.decoder.layers.16.final_layer_norm.weight', 'model.decoder.layers.16.final_layer_norm.bias', 'model.decoder.layers.17.self_attn.k_proj.weight', 'model.decoder.layers.17.self_attn.k_proj.bias', 'model.decoder.layers.17.self_attn.v_proj.weight', 'model.decoder.layers.17.self_attn.v_proj.bias', 'model.decoder.layers.17.self_attn.q_proj.weight', 'model.decoder.layers.17.self_attn.q_proj.bias', 'model.decoder.layers.17.self_attn.out_proj.weight', 'model.decoder.layers.17.self_attn.out_proj.bias', 'model.decoder.layers.17.self_attn_layer_norm.weight', 'model.decoder.layers.17.self_attn_layer_norm.bias', 'model.decoder.layers.17.fc1.weight', 'model.decoder.layers.17.fc1.bias', 'model.decoder.layers.17.fc2.weight', 'model.decoder.layers.17.fc2.bias', 'model.decoder.layers.17.final_layer_norm.weight', 'model.decoder.layers.17.final_layer_norm.bias', 'model.decoder.layers.18.self_attn.k_proj.weight', 'model.decoder.layers.18.self_attn.k_proj.bias', 'model.decoder.layers.18.self_attn.v_proj.weight', 'model.decoder.layers.18.self_attn.v_proj.bias', 'model.decoder.layers.18.self_attn.q_proj.weight', 'model.decoder.layers.18.self_attn.q_proj.bias', 'model.decoder.layers.18.self_attn.out_proj.weight', 'model.decoder.layers.18.self_attn.out_proj.bias', 'model.decoder.layers.18.self_attn_layer_norm.weight', 'model.decoder.layers.18.self_attn_layer_norm.bias', 'model.decoder.layers.18.fc1.weight', 'model.decoder.layers.18.fc1.bias', 'model.decoder.layers.18.fc2.weight', 'model.decoder.layers.18.fc2.bias', 'model.decoder.layers.18.final_layer_norm.weight', 'model.decoder.layers.18.final_layer_norm.bias', 'model.decoder.layers.19.self_attn.k_proj.weight', 'model.decoder.layers.19.self_attn.k_proj.bias', 'model.decoder.layers.19.self_attn.v_proj.weight', 'model.decoder.layers.19.self_attn.v_proj.bias', 'model.decoder.layers.19.self_attn.q_proj.weight', 'model.decoder.layers.19.self_attn.q_proj.bias', 'model.decoder.layers.19.self_attn.out_proj.weight', 'model.decoder.layers.19.self_attn.out_proj.bias', 'model.decoder.layers.19.self_attn_layer_norm.weight', 'model.decoder.layers.19.self_attn_layer_norm.bias', 'model.decoder.layers.19.fc1.weight', 'model.decoder.layers.19.fc1.bias', 'model.decoder.layers.19.fc2.weight', 'model.decoder.layers.19.fc2.bias', 'model.decoder.layers.19.final_layer_norm.weight', 'model.decoder.layers.19.final_layer_norm.bias', 'model.decoder.layers.20.self_attn.k_proj.weight', 'model.decoder.layers.20.self_attn.k_proj.bias', 'model.decoder.layers.20.self_attn.v_proj.weight', 'model.decoder.layers.20.self_attn.v_proj.bias', 'model.decoder.layers.20.self_attn.q_proj.weight', 'model.decoder.layers.20.self_attn.q_proj.bias', 'model.decoder.layers.20.self_attn.out_proj.weight', 'model.decoder.layers.20.self_attn.out_proj.bias', 'model.decoder.layers.20.self_attn_layer_norm.weight', 'model.decoder.layers.20.self_attn_layer_norm.bias', 'model.decoder.layers.20.fc1.weight', 'model.decoder.layers.20.fc1.bias', 'model.decoder.layers.20.fc2.weight', 'model.decoder.layers.20.fc2.bias', 'model.decoder.layers.20.final_layer_norm.weight', 'model.decoder.layers.20.final_layer_norm.bias', 'model.decoder.layers.21.self_attn.k_proj.weight', 'model.decoder.layers.21.self_attn.k_proj.bias', 'model.decoder.layers.21.self_attn.v_proj.weight', 'model.decoder.layers.21.self_attn.v_proj.bias', 'model.decoder.layers.21.self_attn.q_proj.weight', 'model.decoder.layers.21.self_attn.q_proj.bias', 'model.decoder.layers.21.self_attn.out_proj.weight', 'model.decoder.layers.21.self_attn.out_proj.bias', 'model.decoder.layers.21.self_attn_layer_norm.weight', 'model.decoder.layers.21.self_attn_layer_norm.bias', 'model.decoder.layers.21.fc1.weight', 'model.decoder.layers.21.fc1.bias', 'model.decoder.layers.21.fc2.weight', 'model.decoder.layers.21.fc2.bias', 'model.decoder.layers.21.final_layer_norm.weight', 'model.decoder.layers.21.final_layer_norm.bias', 'model.decoder.layers.22.self_attn.k_proj.weight', 'model.decoder.layers.22.self_attn.k_proj.bias', 'model.decoder.layers.22.self_attn.v_proj.weight', 'model.decoder.layers.22.self_attn.v_proj.bias', 'model.decoder.layers.22.self_attn.q_proj.weight', 'model.decoder.layers.22.self_attn.q_proj.bias', 'model.decoder.layers.22.self_attn.out_proj.weight', 'model.decoder.layers.22.self_attn.out_proj.bias', 'model.decoder.layers.22.self_attn_layer_norm.weight', 'model.decoder.layers.22.self_attn_layer_norm.bias', 'model.decoder.layers.22.fc1.weight', 'model.decoder.layers.22.fc1.bias', 'model.decoder.layers.22.fc2.weight', 'model.decoder.layers.22.fc2.bias', 'model.decoder.layers.22.final_layer_norm.weight', 'model.decoder.layers.22.final_layer_norm.bias', 'model.decoder.layers.23.self_attn.k_proj.weight', 'model.decoder.layers.23.self_attn.k_proj.bias', 'model.decoder.layers.23.self_attn.v_proj.weight', 'model.decoder.layers.23.self_attn.v_proj.bias', 'model.decoder.layers.23.self_attn.q_proj.weight', 'model.decoder.layers.23.self_attn.q_proj.bias', 'model.decoder.layers.23.self_attn.out_proj.weight', 'model.decoder.layers.23.self_attn.out_proj.bias', 'model.decoder.layers.23.self_attn_layer_norm.weight', 'model.decoder.layers.23.self_attn_layer_norm.bias', 'model.decoder.layers.23.fc1.weight', 'model.decoder.layers.23.fc1.bias', 'model.decoder.layers.23.fc2.weight', 'model.decoder.layers.23.fc2.bias', 'model.decoder.layers.23.final_layer_norm.weight', 'model.decoder.layers.23.final_layer_norm.bias', 'model.decoder.layers.24.self_attn.k_proj.weight', 'model.decoder.layers.24.self_attn.k_proj.bias', 'model.decoder.layers.24.self_attn.v_proj.weight', 'model.decoder.layers.24.self_attn.v_proj.bias', 'model.decoder.layers.24.self_attn.q_proj.weight', 'model.decoder.layers.24.self_attn.q_proj.bias', 'model.decoder.layers.24.self_attn.out_proj.weight', 'model.decoder.layers.24.self_attn.out_proj.bias', 'model.decoder.layers.24.self_attn_layer_norm.weight', 'model.decoder.layers.24.self_attn_layer_norm.bias', 'model.decoder.layers.24.fc1.weight', 'model.decoder.layers.24.fc1.bias', 'model.decoder.layers.24.fc2.weight', 'model.decoder.layers.24.fc2.bias', 'model.decoder.layers.24.final_layer_norm.weight', 'model.decoder.layers.24.final_layer_norm.bias', 'model.decoder.layers.25.self_attn.k_proj.weight', 'model.decoder.layers.25.self_attn.k_proj.bias', 'model.decoder.layers.25.self_attn.v_proj.weight', 'model.decoder.layers.25.self_attn.v_proj.bias', 'model.decoder.layers.25.self_attn.q_proj.weight', 'model.decoder.layers.25.self_attn.q_proj.bias', 'model.decoder.layers.25.self_attn.out_proj.weight', 'model.decoder.layers.25.self_attn.out_proj.bias', 'model.decoder.layers.25.self_attn_layer_norm.weight', 'model.decoder.layers.25.self_attn_layer_norm.bias', 'model.decoder.layers.25.fc1.weight', 'model.decoder.layers.25.fc1.bias', 'model.decoder.layers.25.fc2.weight', 'model.decoder.layers.25.fc2.bias', 'model.decoder.layers.25.final_layer_norm.weight', 'model.decoder.layers.25.final_layer_norm.bias', 'model.decoder.layers.26.self_attn.k_proj.weight', 'model.decoder.layers.26.self_attn.k_proj.bias', 'model.decoder.layers.26.self_attn.v_proj.weight', 'model.decoder.layers.26.self_attn.v_proj.bias', 'model.decoder.layers.26.self_attn.q_proj.weight', 'model.decoder.layers.26.self_attn.q_proj.bias', 'model.decoder.layers.26.self_attn.out_proj.weight', 'model.decoder.layers.26.self_attn.out_proj.bias', 'model.decoder.layers.26.self_attn_layer_norm.weight', 'model.decoder.layers.26.self_attn_layer_norm.bias', 'model.decoder.layers.26.fc1.weight', 'model.decoder.layers.26.fc1.bias', 'model.decoder.layers.26.fc2.weight', 'model.decoder.layers.26.fc2.bias', 'model.decoder.layers.26.final_layer_norm.weight', 'model.decoder.layers.26.final_layer_norm.bias', 'model.decoder.layers.27.self_attn.k_proj.weight', 'model.decoder.layers.27.self_attn.k_proj.bias', 'model.decoder.layers.27.self_attn.v_proj.weight', 'model.decoder.layers.27.self_attn.v_proj.bias', 'model.decoder.layers.27.self_attn.q_proj.weight', 'model.decoder.layers.27.self_attn.q_proj.bias', 'model.decoder.layers.27.self_attn.out_proj.weight', 'model.decoder.layers.27.self_attn.out_proj.bias', 'model.decoder.layers.27.self_attn_layer_norm.weight', 'model.decoder.layers.27.self_attn_layer_norm.bias', 'model.decoder.layers.27.fc1.weight', 'model.decoder.layers.27.fc1.bias', 'model.decoder.layers.27.fc2.weight', 'model.decoder.layers.27.fc2.bias', 'model.decoder.layers.27.final_layer_norm.weight', 'model.decoder.layers.27.final_layer_norm.bias', 'model.decoder.layers.28.self_attn.k_proj.weight', 'model.decoder.layers.28.self_attn.k_proj.bias', 'model.decoder.layers.28.self_attn.v_proj.weight', 'model.decoder.layers.28.self_attn.v_proj.bias', 'model.decoder.layers.28.self_attn.q_proj.weight', 'model.decoder.layers.28.self_attn.q_proj.bias', 'model.decoder.layers.28.self_attn.out_proj.weight', 'model.decoder.layers.28.self_attn.out_proj.bias', 'model.decoder.layers.28.self_attn_layer_norm.weight', 'model.decoder.layers.28.self_attn_layer_norm.bias', 'model.decoder.layers.28.fc1.weight', 'model.decoder.layers.28.fc1.bias', 'model.decoder.layers.28.fc2.weight', 'model.decoder.layers.28.fc2.bias', 'model.decoder.layers.28.final_layer_norm.weight', 'model.decoder.layers.28.final_layer_norm.bias', 'model.decoder.layers.29.self_attn.k_proj.weight', 'model.decoder.layers.29.self_attn.k_proj.bias', 'model.decoder.layers.29.self_attn.v_proj.weight', 'model.decoder.layers.29.self_attn.v_proj.bias', 'model.decoder.layers.29.self_attn.q_proj.weight', 'model.decoder.layers.29.self_attn.q_proj.bias', 'model.decoder.layers.29.self_attn.out_proj.weight', 'model.decoder.layers.29.self_attn.out_proj.bias', 'model.decoder.layers.29.self_attn_layer_norm.weight', 'model.decoder.layers.29.self_attn_layer_norm.bias', 'model.decoder.layers.29.fc1.weight', 'model.decoder.layers.29.fc1.bias', 'model.decoder.layers.29.fc2.weight', 'model.decoder.layers.29.fc2.bias', 'model.decoder.layers.29.final_layer_norm.weight', 'model.decoder.layers.29.final_layer_norm.bias', 'model.decoder.layers.30.self_attn.k_proj.weight', 'model.decoder.layers.30.self_attn.k_proj.bias', 'model.decoder.layers.30.self_attn.v_proj.weight', 'model.decoder.layers.30.self_attn.v_proj.bias', 'model.decoder.layers.30.self_attn.q_proj.weight', 'model.decoder.layers.30.self_attn.q_proj.bias', 'model.decoder.layers.30.self_attn.out_proj.weight', 'model.decoder.layers.30.self_attn.out_proj.bias', 'model.decoder.layers.30.self_attn_layer_norm.weight', 'model.decoder.layers.30.self_attn_layer_norm.bias', 'model.decoder.layers.30.fc1.weight', 'model.decoder.layers.30.fc1.bias', 'model.decoder.layers.30.fc2.weight', 'model.decoder.layers.30.fc2.bias', 'model.decoder.layers.30.final_layer_norm.weight', 'model.decoder.layers.30.final_layer_norm.bias', 'model.decoder.layers.31.self_attn.k_proj.weight', 'model.decoder.layers.31.self_attn.k_proj.bias', 'model.decoder.layers.31.self_attn.v_proj.weight', 'model.decoder.layers.31.self_attn.v_proj.bias', 'model.decoder.layers.31.self_attn.q_proj.weight', 'model.decoder.layers.31.self_attn.q_proj.bias', 'model.decoder.layers.31.self_attn.out_proj.weight', 'model.decoder.layers.31.self_attn.out_proj.bias', 'model.decoder.layers.31.self_attn_layer_norm.weight', 'model.decoder.layers.31.self_attn_layer_norm.bias', 'model.decoder.layers.31.fc1.weight', 'model.decoder.layers.31.fc1.bias', 'model.decoder.layers.31.fc2.weight', 'model.decoder.layers.31.fc2.bias', 'model.decoder.layers.31.final_layer_norm.weight', 'model.decoder.layers.31.final_layer_norm.bias']
- This IS expected if you are initializing GPTNeoForCausalLM from a Flax model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a FlaxBertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoForCausalLM from a Flax model that you expect to be exactly identical (e.g. initializing a BertForSequenceClassification model from a FlaxBertForSequenceClassification model).
Some weights of GPTNeoForCausalLM were not initialized from the Flax model and are newly initialized: ['transformer.h.8.attn.attention.out_proj.weight', 'transformer.h.1.attn.attention.k_proj.weight', 'transformer.h.25.attn.attention.out_proj.bias', 'transformer.h.8.ln_2.bias', 'transformer.h.1.attn.attention.v_proj.weight', 'transformer.h.0.attn.attention.masked_bias', 'transformer.h.14.ln_2.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.26.attn.attention.out_proj.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.10.attn.attention.k_proj.weight', 'transformer.h.14.ln_1.weight', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.31.attn.attention.out_proj.weight', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.8.attn.attention.k_proj.weight', 'transformer.h.5.attn.attention.v_proj.weight', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.15.ln_2.bias', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.14.attn.attention.bias', 'transformer.h.13.ln_1.weight', 'transformer.h.28.attn.attention.k_proj.weight', 'transformer.h.9.ln_2.weight', 'transformer.h.13.ln_1.bias', 'transformer.h.30.attn.attention.k_proj.weight', 'transformer.wpe.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.12.attn.attention.bias', 'transformer.h.30.attn.attention.q_proj.weight', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.10.attn.attention.bias', 'transformer.h.13.attn.attention.out_proj.bias', 'transformer.h.6.attn.attention.q_proj.weight', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.5.attn.attention.out_proj.bias', 'transformer.h.22.attn.attention.masked_bias', 'transformer.h.28.mlp.c_proj.weight', 'transformer.h.16.attn.attention.bias', 'transformer.h.15.ln_1.bias', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.5.attn.attention.out_proj.weight', 'transformer.h.10.ln_2.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.28.attn.attention.out_proj.weight', 'transformer.h.10.attn.attention.v_proj.weight', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.24.ln_1.bias', 'transformer.h.20.attn.attention.v_proj.weight', 'transformer.h.6.ln_1.weight', 'transformer.h.5.attn.attention.k_proj.weight', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.0.attn.attention.q_proj.weight', 'transformer.h.17.attn.attention.masked_bias', 'transformer.h.28.mlp.c_fc.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.11.ln_2.bias', 'transformer.h.19.attn.attention.k_proj.weight', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.17.ln_2.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.29.attn.attention.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.16.attn.attention.v_proj.weight', 'transformer.h.28.ln_1.bias', 'transformer.h.16.attn.attention.masked_bias', 'transformer.h.12.attn.attention.out_proj.bias', 'transformer.h.26.attn.attention.k_proj.weight', 'transformer.h.19.attn.attention.q_proj.weight', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.12.attn.attention.v_proj.weight', 'transformer.h.29.ln_2.bias', 'transformer.h.8.attn.attention.q_proj.weight', 'transformer.h.30.attn.attention.masked_bias', 'transformer.h.30.attn.attention.bias', 'transformer.h.22.ln_1.bias', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.10.attn.attention.q_proj.weight', 'transformer.h.23.attn.attention.out_proj.bias', 'transformer.h.29.attn.attention.out_proj.weight', 'transformer.h.22.attn.attention.bias', 'transformer.h.30.ln_1.weight', 'transformer.h.8.attn.attention.v_proj.weight', 'transformer.h.9.attn.attention.out_proj.bias', 'transformer.h.31.attn.attention.masked_bias', 'transformer.h.20.ln_1.bias', 'transformer.h.7.ln_1.bias', 'transformer.h.25.mlp.c_fc.weight', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.30.attn.attention.v_proj.weight', 'transformer.h.29.ln_1.bias', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.25.attn.attention.q_proj.weight', 'transformer.h.10.ln_1.weight', 'transformer.h.24.attn.attention.q_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.7.ln_1.weight', 'transformer.h.31.attn.attention.bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.30.mlp.c_fc.weight', 'transformer.h.22.ln_2.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.17.attn.attention.k_proj.weight', 'transformer.h.30.attn.attention.out_proj.weight', 'transformer.h.8.attn.attention.bias', 'transformer.h.19.attn.attention.bias', 'transformer.h.14.attn.attention.v_proj.weight', 'transformer.h.23.attn.attention.v_proj.weight', 'transformer.h.27.attn.attention.q_proj.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.29.ln_2.weight', 'transformer.h.20.attn.attention.q_proj.weight', 'transformer.h.29.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.18.attn.attention.masked_bias', 'transformer.h.9.attn.attention.masked_bias', 'transformer.h.11.attn.attention.out_proj.bias', 'transformer.h.23.ln_1.bias', 'transformer.h.4.attn.attention.q_proj.weight', 'transformer.h.19.ln_1.weight', 'transformer.h.9.ln_1.weight', 'transformer.h.29.attn.attention.q_proj.weight', 'transformer.h.13.ln_2.bias', 'transformer.h.3.attn.attention.k_proj.weight', 'transformer.h.20.ln_1.weight', 'transformer.h.7.attn.attention.out_proj.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.0.attn.attention.v_proj.weight', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.19.ln_2.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.17.ln_1.weight', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.6.attn.attention.out_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.0.attn.attention.out_proj.bias', 'transformer.h.16.attn.attention.out_proj.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.11.attn.attention.k_proj.weight', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.25.ln_1.weight', 'transformer.h.22.attn.attention.out_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.27.ln_2.weight', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.12.attn.attention.k_proj.weight', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.21.attn.attention.out_proj.bias', 'transformer.h.24.attn.attention.out_proj.bias', 'transformer.h.13.attn.attention.out_proj.weight', 'transformer.h.24.mlp.c_proj.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.19.attn.attention.out_proj.bias', 'transformer.h.5.attn.attention.masked_bias', 'transformer.h.24.mlp.c_fc.weight', 'transformer.h.21.attn.attention.out_proj.weight', 'transformer.h.20.attn.attention.masked_bias', 'transformer.h.25.ln_2.weight', 'transformer.h.2.attn.attention.k_proj.weight', 'transformer.h.7.attn.attention.k_proj.weight', 'transformer.h.27.attn.attention.bias', 'transformer.h.11.attn.attention.v_proj.weight', 'transformer.h.26.mlp.c_proj.weight', 'transformer.h.15.ln_2.weight', 'transformer.h.25.mlp.c_fc.bias', 'transformer.h.26.attn.attention.bias', 'transformer.h.12.ln_1.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.14.attn.attention.k_proj.weight', 'transformer.h.25.attn.attention.v_proj.weight', 'transformer.h.4.attn.attention.out_proj.weight', 'transformer.h.23.attn.attention.out_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.4.ln_2.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.25.mlp.c_proj.weight', 'transformer.h.3.attn.attention.masked_bias', 'transformer.h.1.attn.attention.q_proj.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.18.attn.attention.out_proj.bias', 'transformer.h.27.attn.attention.out_proj.weight', 'transformer.h.29.attn.attention.v_proj.weight', 'transformer.h.25.attn.attention.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.20.ln_2.bias', 'transformer.h.23.attn.attention.masked_bias', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.9.attn.attention.bias', 'transformer.h.26.attn.attention.v_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.4.ln_1.bias', 'transformer.h.27.attn.attention.out_proj.bias', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.30.mlp.c_fc.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.12.ln_2.bias', 'transformer.h.22.mlp.c_proj.weight', 'transformer.ln_f.bias', 'transformer.h.2.ln_2.bias', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.22.attn.attention.q_proj.weight', 'transformer.h.2.attn.attention.out_proj.weight', 'transformer.h.19.attn.attention.out_proj.weight', 'transformer.h.31.attn.attention.k_proj.weight', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.28.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.27.attn.attention.k_proj.weight', 'transformer.h.23.ln_2.bias', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.28.attn.attention.bias', 'transformer.h.24.attn.attention.k_proj.weight', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.3.attn.attention.out_proj.weight', 'transformer.h.1.attn.attention.masked_bias', 'transformer.h.16.attn.attention.out_proj.weight', 'transformer.h.17.attn.attention.q_proj.weight', 'transformer.h.14.attn.attention.out_proj.bias', 'transformer.h.12.attn.attention.q_proj.weight', 'transformer.h.22.attn.attention.v_proj.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.4.attn.attention.masked_bias', 'transformer.h.25.ln_2.bias', 'transformer.h.18.ln_1.weight', 'transformer.h.20.attn.attention.out_proj.weight', 'transformer.h.4.attn.attention.out_proj.bias', 'transformer.h.9.ln_2.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.15.attn.attention.masked_bias', 'transformer.h.23.attn.attention.k_proj.weight', 'transformer.h.22.ln_1.weight', 'transformer.h.31.mlp.c_fc.weight', 'transformer.h.4.attn.attention.v_proj.weight', 'transformer.h.20.attn.attention.bias', 'transformer.h.6.attn.attention.out_proj.weight', 'transformer.h.29.attn.attention.out_proj.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.21.attn.attention.q_proj.weight', 'transformer.h.3.ln_2.weight', 'transformer.h.3.attn.attention.bias', 'transformer.h.26.attn.attention.q_proj.weight', 'transformer.h.28.mlp.c_fc.bias', 'transformer.h.19.attn.attention.v_proj.weight', 'transformer.h.6.attn.attention.v_proj.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.16.ln_1.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.31.ln_2.bias', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.29.mlp.c_proj.weight', 'transformer.h.30.ln_2.bias', 'transformer.h.9.attn.attention.q_proj.weight', 'transformer.h.1.attn.attention.out_proj.bias', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.28.ln_2.weight', 'transformer.h.22.ln_2.weight', 'transformer.h.0.attn.attention.k_proj.weight', 'transformer.h.16.ln_2.weight', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.19.ln_1.bias', 'transformer.h.26.ln_1.weight', 'transformer.h.11.ln_1.weight', 'transformer.h.18.attn.attention.k_proj.weight', 'transformer.h.31.ln_1.weight', 'transformer.h.14.attn.attention.out_proj.weight', 'transformer.h.12.ln_2.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.16.attn.attention.k_proj.weight', 'transformer.h.30.attn.attention.out_proj.bias', 'transformer.h.24.ln_2.bias', 'transformer.h.4.attn.attention.k_proj.weight', 'transformer.h.26.ln_2.bias', 'transformer.h.7.attn.attention.out_proj.bias', 'transformer.h.27.attn.attention.masked_bias', 'transformer.h.29.ln_1.weight', 'transformer.h.29.mlp.c_fc.bias', 'transformer.h.15.ln_1.weight', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.27.attn.attention.v_proj.weight', 'transformer.h.16.ln_1.bias', 'transformer.h.21.attn.attention.k_proj.weight', 'transformer.h.22.attn.attention.k_proj.weight', 'transformer.h.14.ln_2.weight', 'transformer.h.17.attn.attention.bias', 'transformer.h.21.attn.attention.masked_bias', 'transformer.h.28.attn.attention.v_proj.weight', 'transformer.h.27.mlp.c_proj.weight', 'transformer.h.15.attn.attention.q_proj.weight', 'transformer.h.7.attn.attention.q_proj.weight', 'transformer.h.7.ln_2.weight', 'transformer.h.24.ln_2.weight', 'transformer.h.2.ln_2.weight', 'transformer.h.24.attn.attention.bias', 'transformer.h.2.ln_1.bias', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.17.ln_1.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.18.attn.attention.bias', 'lm_head.weight', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.18.attn.attention.v_proj.weight', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.30.ln_2.weight', 'transformer.h.17.ln_2.weight', 'transformer.h.4.ln_1.weight', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.10.ln_1.bias', 'transformer.h.24.attn.attention.v_proj.weight', 'transformer.h.2.attn.attention.q_proj.weight', 'transformer.h.24.attn.attention.out_proj.weight', 'transformer.h.23.ln_1.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.11.attn.attention.masked_bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.8.attn.attention.masked_bias', 'transformer.h.0.attn.attention.out_proj.weight', 'transformer.h.12.attn.attention.out_proj.weight', 'transformer.h.20.ln_2.weight', 'transformer.h.27.ln_1.bias', 'transformer.h.10.attn.attention.masked_bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.2.attn.attention.out_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.wte.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.attn.attention.q_proj.weight', 'transformer.h.25.ln_1.bias', 'transformer.h.4.attn.attention.bias', 'transformer.h.22.attn.attention.out_proj.bias', 'transformer.h.6.attn.attention.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.21.ln_1.bias', 'transformer.h.1.attn.attention.out_proj.weight', 'transformer.h.30.mlp.c_proj.weight', 'transformer.h.6.ln_2.weight', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.17.attn.attention.out_proj.bias', 'transformer.h.26.ln_2.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.26.attn.attention.masked_bias', 'transformer.h.16.attn.attention.q_proj.weight', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.15.attn.attention.out_proj.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.13.attn.attention.v_proj.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.28.attn.attention.out_proj.bias', 'transformer.h.28.attn.attention.masked_bias', 'transformer.h.9.attn.attention.out_proj.weight', 'transformer.h.9.attn.attention.k_proj.weight', 'transformer.h.26.mlp.c_fc.weight', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.20.attn.attention.out_proj.bias', 'transformer.h.18.attn.attention.q_proj.weight', 'transformer.h.24.ln_1.weight', 'transformer.h.7.attn.attention.masked_bias', 'transformer.h.25.attn.attention.out_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.ln_1.weight', 'transformer.h.2.ln_1.weight', 'transformer.h.10.attn.attention.out_proj.weight', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.26.mlp.c_fc.bias', 'transformer.h.27.mlp.c_fc.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.11.ln_1.bias', 'transformer.h.18.ln_2.weight', 'transformer.h.31.ln_1.bias', 'transformer.h.11.attn.attention.out_proj.weight', 'transformer.h.27.ln_2.bias', 'transformer.h.2.attn.attention.v_proj.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.25.attn.attention.k_proj.weight', 'transformer.h.8.attn.attention.out_proj.bias', 'transformer.h.5.attn.attention.q_proj.weight', 'transformer.h.19.ln_2.bias', 'transformer.h.6.attn.attention.k_proj.weight', 'transformer.h.6.attn.attention.masked_bias', 'transformer.ln_f.weight', 'transformer.h.13.attn.attention.k_proj.weight', 'transformer.h.25.attn.attention.masked_bias', 'transformer.h.9.attn.attention.v_proj.weight', 'transformer.h.3.attn.attention.q_proj.weight', 'transformer.h.12.attn.attention.masked_bias', 'transformer.h.31.attn.attention.q_proj.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.29.attn.attention.k_proj.weight', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.13.attn.attention.q_proj.weight', 'transformer.h.12.ln_1.weight', 'transformer.h.31.mlp.c_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.26.attn.attention.out_proj.weight', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.31.attn.attention.out_proj.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.17.attn.attention.out_proj.weight', 'transformer.h.28.attn.attention.q_proj.weight', 'transformer.h.3.attn.attention.v_proj.weight', 'transformer.h.8.ln_1.weight', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.0.attn.attention.bias', 'transformer.h.24.mlp.c_fc.bias', 'transformer.h.6.ln_1.bias', 'transformer.h.21.ln_1.weight', 'transformer.h.2.attn.attention.bias', 'transformer.h.10.ln_2.bias', 'transformer.h.31.attn.attention.v_proj.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.24.attn.attention.masked_bias', 'transformer.h.4.ln_2.bias', 'transformer.h.18.attn.attention.out_proj.weight', 'transformer.h.21.ln_2.weight', 'transformer.h.30.ln_1.bias', 'transformer.h.23.attn.attention.q_proj.weight', 'transformer.h.15.attn.attention.v_proj.weight', 'transformer.h.3.attn.attention.out_proj.bias', 'transformer.h.15.attn.attention.k_proj.weight', 'transformer.h.21.attn.attention.v_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.29.attn.attention.masked_bias', 'transformer.h.0.ln_1.weight', 'transformer.h.15.attn.attention.out_proj.weight', 'transformer.h.13.attn.attention.masked_bias', 'transformer.h.10.attn.attention.out_proj.bias', 'transformer.h.7.attn.attention.v_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.2.attn.attention.masked_bias', 'transformer.h.23.ln_2.weight', 'transformer.h.26.ln_1.bias', 'transformer.h.16.ln_2.bias', 'transformer.h.17.attn.attention.v_proj.weight', 'transformer.h.21.ln_2.bias', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.14.ln_1.bias', 'transformer.h.14.attn.attention.masked_bias', 'transformer.h.28.ln_2.bias', 'transformer.h.31.ln_2.weight', 'transformer.h.31.mlp.c_fc.bias', 'transformer.h.13.ln_2.weight', 'transformer.h.14.attn.attention.q_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.18.ln_1.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.18.ln_2.bias', 'transformer.h.27.mlp.c_fc.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.19.attn.attention.masked_bias', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.27.ln_1.weight', 'transformer.h.20.attn.attention.k_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2022-11-18 16:05:13,199][datasets.builder][WARNING] - Using custom data configuration ViktorThink--mountain_combined_813306-6918b9ea07482433
[2022-11-18 16:05:13,324][datasets.builder][WARNING] - Found cached dataset csv (/home/paperspace/.cache/huggingface/datasets/ViktorThink___csv/ViktorThink--mountain_combined_813306-6918b9ea07482433/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  5.43it/s]100%|██████████| 2/2 [00:00<00:00, 10.22it/s]
Running tokenizer on dataset:   0%|          | 0/814 [00:00<?, ?ba/s]Running tokenizer on dataset:   0%|          | 1/814 [00:03<46:55,  3.46s/ba]Running tokenizer on dataset:   0%|          | 2/814 [00:06<43:45,  3.23s/ba]Running tokenizer on dataset:   0%|          | 3/814 [00:09<41:58,  3.11s/ba]Running tokenizer on dataset:   0%|          | 4/814 [00:12<40:32,  3.00s/ba]Running tokenizer on dataset:   1%|          | 5/814 [00:15<39:52,  2.96s/ba]Running tokenizer on dataset:   1%|          | 6/814 [00:17<38:31,  2.86s/ba]Running tokenizer on dataset:   1%|          | 7/814 [00:20<37:59,  2.82s/ba]Running tokenizer on dataset:   1%|          | 8/814 [00:23<37:22,  2.78s/ba]Running tokenizer on dataset:   1%|          | 9/814 [00:25<36:37,  2.73s/ba]Running tokenizer on dataset:   1%|          | 10/814 [00:28<35:58,  2.69s/ba]Running tokenizer on dataset:   1%|          | 10/814 [00:30<40:14,  3.00s/ba]
Error in sys.excepthook:
Traceback (most recent call last):
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1694, in print
    extend(render(renderable, render_options))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/constrain.py", line 29, in __rich_console__
    yield from console.render(self.renderable, child_options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/panel.py", line 220, in __rich_console__
    lines = console.render_lines(renderable, child_options, style=style)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 511, in __rich_console__
    yield from self._render(console, render_options, widths)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 824, in _render
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/panel.py", line 220, in __rich_console__
    lines = console.render_lines(renderable, child_options, style=style)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 511, in __rich_console__
    yield from self._render(console, render_options, widths)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 824, in _render
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/pretty.py", line 342, in __rich_console__
    pretty_text = Text.from_ansi(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/text.py", line 313, in from_ansi
    result = joiner.join(line for line in decoder.decode(text))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/text.py", line 752, in join
    def iter_text() -> Iterable["Text"]:
  File "/usr/lib/python3.8/typing.py", line 258, in inner
    return cached(*args, **kwds)
  File "/usr/lib/python3.8/typing.py", line 723, in __hash__
    return hash((self.__origin__, self.__args__))
KeyboardInterrupt

Original exception was:
Traceback (most recent call last):
  File "finetune_using_clm_wandb.py", line 599, in <module>
    main()
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/main.py", line 90, in decorated_main
    _run_hydra(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 389, in _run_hydra
    _run_app(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 452, in _run_app
    run_and_report(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 213, in run_and_report
    return func()
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 453, in <lambda>
    lambda: hydra.run(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "finetune_using_clm_wandb.py", line 304, in main
    tokenized_datasets = preprocess(cfg, accelerator, tokenizer, raw_datasets)
  File "finetune_using_clm_wandb.py", line 215, in preprocess
    tokenized_datasets = raw_datasets.map(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/dataset_dict.py", line 777, in map
    {
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/dataset_dict.py", line 778, in <dictcomp>
    k: dataset.map(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2585, in map
    return self._map_single(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 585, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 552, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/fingerprint.py", line 480, in wrapper
    out = func(self, *args, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2982, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2865, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2545, in decorated
    result = f(decorated_item, *args, **kwargs)
  File "finetune_using_clm_wandb.py", line 204, in tokenize_fn
    result = tokenizer(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2488, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2574, in _call_one
    return self.batch_encode_plus(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2765, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 733, in _batch_encode_plus
    first_ids = get_input_ids(ids)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 701, in get_input_ids
    return self.convert_tokens_to_ids(tokens)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 579, in convert_tokens_to_ids
    ids.append(self._convert_token_to_id_with_added_voc(token))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 588, in _convert_token_to_id_with_added_voc
    return self._convert_token_to_id(token)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 308, in _convert_token_to_id
    return self.encoder.get(token, self.encoder.get(self.unk_token))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 988, in unk_token
    @property
KeyboardInterrupt
[2022-11-18 16:05:55,852][__main__][INFO] - Setting random seed to 17
[2022-11-18 16:05:55,853][__main__][INFO] - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

[2022-11-18 16:05:55,855][__main__][INFO] - output_dir: tuned-model
bittensor:
  network: nobunaga
dataset:
  name: ViktorThink/mountain_combined_813306
  config_name: null
  num_batches: 10
  block_size: 256
  overwrite_cache: false
  keep_linebreaks: true
  concatenate_raw: false
  load_tokenized_data: false
model:
  name: facebook/opt-2.7b
  config_name: null
tokenizer:
  name: null
  use_fast: true
  preprocessing_num_workers: null
  pad_token: '[PAD]'
training:
  seed: 17
  val_split_percent: 20
  train_batch_size: 32
  eval_batch_size: 16
  learning_rate: 3.0e-06
  weight_decay: 0.05
  num_epochs: 4
  max_train_steps: null
  gradient_accumulation_steps: 2
  lr_scheduler: constant
  lr_warmup_steps: 5
  eval_every: 250
  max_eval_steps: 500
  checkpoint:
    resume_from_checkpoint: 0
    every_n_steps: null
tracking:
  enabled: true
  report_to: all
testing:
  enabled: false

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/config.json
Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-2.7B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      16
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 20,
  "num_layers": 32,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading file vocab.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/vocab.json
loading file merges.txt from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/merges.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/tokenizer_config.json
loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading weights file flax_model.msgpack from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Loading Flax weights from /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Some weights of the Flax model were not used when initializing the PyTorch model GPTNeoForCausalLM: ['model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.final_layer_norm.weight', 'model.decoder.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.12.self_attn.k_proj.weight', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.weight', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.weight', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.12.self_attn_layer_norm.weight', 'model.decoder.layers.12.self_attn_layer_norm.bias', 'model.decoder.layers.12.fc1.weight', 'model.decoder.layers.12.fc1.bias', 'model.decoder.layers.12.fc2.weight', 'model.decoder.layers.12.fc2.bias', 'model.decoder.layers.12.final_layer_norm.weight', 'model.decoder.layers.12.final_layer_norm.bias', 'model.decoder.layers.13.self_attn.k_proj.weight', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.weight', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.weight', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.weight', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn_layer_norm.weight', 'model.decoder.layers.13.self_attn_layer_norm.bias', 'model.decoder.layers.13.fc1.weight', 'model.decoder.layers.13.fc1.bias', 'model.decoder.layers.13.fc2.weight', 'model.decoder.layers.13.fc2.bias', 'model.decoder.layers.13.final_layer_norm.weight', 'model.decoder.layers.13.final_layer_norm.bias', 'model.decoder.layers.14.self_attn.k_proj.weight', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.weight', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.weight', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.weight', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.self_attn_layer_norm.weight', 'model.decoder.layers.14.self_attn_layer_norm.bias', 'model.decoder.layers.14.fc1.weight', 'model.decoder.layers.14.fc1.bias', 'model.decoder.layers.14.fc2.weight', 'model.decoder.layers.14.fc2.bias', 'model.decoder.layers.14.final_layer_norm.weight', 'model.decoder.layers.14.final_layer_norm.bias', 'model.decoder.layers.15.self_attn.k_proj.weight', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.weight', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.weight', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.weight', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn_layer_norm.weight', 'model.decoder.layers.15.self_attn_layer_norm.bias', 'model.decoder.layers.15.fc1.weight', 'model.decoder.layers.15.fc1.bias', 'model.decoder.layers.15.fc2.weight', 'model.decoder.layers.15.fc2.bias', 'model.decoder.layers.15.final_layer_norm.weight', 'model.decoder.layers.15.final_layer_norm.bias', 'model.decoder.layers.16.self_attn.k_proj.weight', 'model.decoder.layers.16.self_attn.k_proj.bias', 'model.decoder.layers.16.self_attn.v_proj.weight', 'model.decoder.layers.16.self_attn.v_proj.bias', 'model.decoder.layers.16.self_attn.q_proj.weight', 'model.decoder.layers.16.self_attn.q_proj.bias', 'model.decoder.layers.16.self_attn.out_proj.weight', 'model.decoder.layers.16.self_attn.out_proj.bias', 'model.decoder.layers.16.self_attn_layer_norm.weight', 'model.decoder.layers.16.self_attn_layer_norm.bias', 'model.decoder.layers.16.fc1.weight', 'model.decoder.layers.16.fc1.bias', 'model.decoder.layers.16.fc2.weight', 'model.decoder.layers.16.fc2.bias', 'model.decoder.layers.16.final_layer_norm.weight', 'model.decoder.layers.16.final_layer_norm.bias', 'model.decoder.layers.17.self_attn.k_proj.weight', 'model.decoder.layers.17.self_attn.k_proj.bias', 'model.decoder.layers.17.self_attn.v_proj.weight', 'model.decoder.layers.17.self_attn.v_proj.bias', 'model.decoder.layers.17.self_attn.q_proj.weight', 'model.decoder.layers.17.self_attn.q_proj.bias', 'model.decoder.layers.17.self_attn.out_proj.weight', 'model.decoder.layers.17.self_attn.out_proj.bias', 'model.decoder.layers.17.self_attn_layer_norm.weight', 'model.decoder.layers.17.self_attn_layer_norm.bias', 'model.decoder.layers.17.fc1.weight', 'model.decoder.layers.17.fc1.bias', 'model.decoder.layers.17.fc2.weight', 'model.decoder.layers.17.fc2.bias', 'model.decoder.layers.17.final_layer_norm.weight', 'model.decoder.layers.17.final_layer_norm.bias', 'model.decoder.layers.18.self_attn.k_proj.weight', 'model.decoder.layers.18.self_attn.k_proj.bias', 'model.decoder.layers.18.self_attn.v_proj.weight', 'model.decoder.layers.18.self_attn.v_proj.bias', 'model.decoder.layers.18.self_attn.q_proj.weight', 'model.decoder.layers.18.self_attn.q_proj.bias', 'model.decoder.layers.18.self_attn.out_proj.weight', 'model.decoder.layers.18.self_attn.out_proj.bias', 'model.decoder.layers.18.self_attn_layer_norm.weight', 'model.decoder.layers.18.self_attn_layer_norm.bias', 'model.decoder.layers.18.fc1.weight', 'model.decoder.layers.18.fc1.bias', 'model.decoder.layers.18.fc2.weight', 'model.decoder.layers.18.fc2.bias', 'model.decoder.layers.18.final_layer_norm.weight', 'model.decoder.layers.18.final_layer_norm.bias', 'model.decoder.layers.19.self_attn.k_proj.weight', 'model.decoder.layers.19.self_attn.k_proj.bias', 'model.decoder.layers.19.self_attn.v_proj.weight', 'model.decoder.layers.19.self_attn.v_proj.bias', 'model.decoder.layers.19.self_attn.q_proj.weight', 'model.decoder.layers.19.self_attn.q_proj.bias', 'model.decoder.layers.19.self_attn.out_proj.weight', 'model.decoder.layers.19.self_attn.out_proj.bias', 'model.decoder.layers.19.self_attn_layer_norm.weight', 'model.decoder.layers.19.self_attn_layer_norm.bias', 'model.decoder.layers.19.fc1.weight', 'model.decoder.layers.19.fc1.bias', 'model.decoder.layers.19.fc2.weight', 'model.decoder.layers.19.fc2.bias', 'model.decoder.layers.19.final_layer_norm.weight', 'model.decoder.layers.19.final_layer_norm.bias', 'model.decoder.layers.20.self_attn.k_proj.weight', 'model.decoder.layers.20.self_attn.k_proj.bias', 'model.decoder.layers.20.self_attn.v_proj.weight', 'model.decoder.layers.20.self_attn.v_proj.bias', 'model.decoder.layers.20.self_attn.q_proj.weight', 'model.decoder.layers.20.self_attn.q_proj.bias', 'model.decoder.layers.20.self_attn.out_proj.weight', 'model.decoder.layers.20.self_attn.out_proj.bias', 'model.decoder.layers.20.self_attn_layer_norm.weight', 'model.decoder.layers.20.self_attn_layer_norm.bias', 'model.decoder.layers.20.fc1.weight', 'model.decoder.layers.20.fc1.bias', 'model.decoder.layers.20.fc2.weight', 'model.decoder.layers.20.fc2.bias', 'model.decoder.layers.20.final_layer_norm.weight', 'model.decoder.layers.20.final_layer_norm.bias', 'model.decoder.layers.21.self_attn.k_proj.weight', 'model.decoder.layers.21.self_attn.k_proj.bias', 'model.decoder.layers.21.self_attn.v_proj.weight', 'model.decoder.layers.21.self_attn.v_proj.bias', 'model.decoder.layers.21.self_attn.q_proj.weight', 'model.decoder.layers.21.self_attn.q_proj.bias', 'model.decoder.layers.21.self_attn.out_proj.weight', 'model.decoder.layers.21.self_attn.out_proj.bias', 'model.decoder.layers.21.self_attn_layer_norm.weight', 'model.decoder.layers.21.self_attn_layer_norm.bias', 'model.decoder.layers.21.fc1.weight', 'model.decoder.layers.21.fc1.bias', 'model.decoder.layers.21.fc2.weight', 'model.decoder.layers.21.fc2.bias', 'model.decoder.layers.21.final_layer_norm.weight', 'model.decoder.layers.21.final_layer_norm.bias', 'model.decoder.layers.22.self_attn.k_proj.weight', 'model.decoder.layers.22.self_attn.k_proj.bias', 'model.decoder.layers.22.self_attn.v_proj.weight', 'model.decoder.layers.22.self_attn.v_proj.bias', 'model.decoder.layers.22.self_attn.q_proj.weight', 'model.decoder.layers.22.self_attn.q_proj.bias', 'model.decoder.layers.22.self_attn.out_proj.weight', 'model.decoder.layers.22.self_attn.out_proj.bias', 'model.decoder.layers.22.self_attn_layer_norm.weight', 'model.decoder.layers.22.self_attn_layer_norm.bias', 'model.decoder.layers.22.fc1.weight', 'model.decoder.layers.22.fc1.bias', 'model.decoder.layers.22.fc2.weight', 'model.decoder.layers.22.fc2.bias', 'model.decoder.layers.22.final_layer_norm.weight', 'model.decoder.layers.22.final_layer_norm.bias', 'model.decoder.layers.23.self_attn.k_proj.weight', 'model.decoder.layers.23.self_attn.k_proj.bias', 'model.decoder.layers.23.self_attn.v_proj.weight', 'model.decoder.layers.23.self_attn.v_proj.bias', 'model.decoder.layers.23.self_attn.q_proj.weight', 'model.decoder.layers.23.self_attn.q_proj.bias', 'model.decoder.layers.23.self_attn.out_proj.weight', 'model.decoder.layers.23.self_attn.out_proj.bias', 'model.decoder.layers.23.self_attn_layer_norm.weight', 'model.decoder.layers.23.self_attn_layer_norm.bias', 'model.decoder.layers.23.fc1.weight', 'model.decoder.layers.23.fc1.bias', 'model.decoder.layers.23.fc2.weight', 'model.decoder.layers.23.fc2.bias', 'model.decoder.layers.23.final_layer_norm.weight', 'model.decoder.layers.23.final_layer_norm.bias', 'model.decoder.layers.24.self_attn.k_proj.weight', 'model.decoder.layers.24.self_attn.k_proj.bias', 'model.decoder.layers.24.self_attn.v_proj.weight', 'model.decoder.layers.24.self_attn.v_proj.bias', 'model.decoder.layers.24.self_attn.q_proj.weight', 'model.decoder.layers.24.self_attn.q_proj.bias', 'model.decoder.layers.24.self_attn.out_proj.weight', 'model.decoder.layers.24.self_attn.out_proj.bias', 'model.decoder.layers.24.self_attn_layer_norm.weight', 'model.decoder.layers.24.self_attn_layer_norm.bias', 'model.decoder.layers.24.fc1.weight', 'model.decoder.layers.24.fc1.bias', 'model.decoder.layers.24.fc2.weight', 'model.decoder.layers.24.fc2.bias', 'model.decoder.layers.24.final_layer_norm.weight', 'model.decoder.layers.24.final_layer_norm.bias', 'model.decoder.layers.25.self_attn.k_proj.weight', 'model.decoder.layers.25.self_attn.k_proj.bias', 'model.decoder.layers.25.self_attn.v_proj.weight', 'model.decoder.layers.25.self_attn.v_proj.bias', 'model.decoder.layers.25.self_attn.q_proj.weight', 'model.decoder.layers.25.self_attn.q_proj.bias', 'model.decoder.layers.25.self_attn.out_proj.weight', 'model.decoder.layers.25.self_attn.out_proj.bias', 'model.decoder.layers.25.self_attn_layer_norm.weight', 'model.decoder.layers.25.self_attn_layer_norm.bias', 'model.decoder.layers.25.fc1.weight', 'model.decoder.layers.25.fc1.bias', 'model.decoder.layers.25.fc2.weight', 'model.decoder.layers.25.fc2.bias', 'model.decoder.layers.25.final_layer_norm.weight', 'model.decoder.layers.25.final_layer_norm.bias', 'model.decoder.layers.26.self_attn.k_proj.weight', 'model.decoder.layers.26.self_attn.k_proj.bias', 'model.decoder.layers.26.self_attn.v_proj.weight', 'model.decoder.layers.26.self_attn.v_proj.bias', 'model.decoder.layers.26.self_attn.q_proj.weight', 'model.decoder.layers.26.self_attn.q_proj.bias', 'model.decoder.layers.26.self_attn.out_proj.weight', 'model.decoder.layers.26.self_attn.out_proj.bias', 'model.decoder.layers.26.self_attn_layer_norm.weight', 'model.decoder.layers.26.self_attn_layer_norm.bias', 'model.decoder.layers.26.fc1.weight', 'model.decoder.layers.26.fc1.bias', 'model.decoder.layers.26.fc2.weight', 'model.decoder.layers.26.fc2.bias', 'model.decoder.layers.26.final_layer_norm.weight', 'model.decoder.layers.26.final_layer_norm.bias', 'model.decoder.layers.27.self_attn.k_proj.weight', 'model.decoder.layers.27.self_attn.k_proj.bias', 'model.decoder.layers.27.self_attn.v_proj.weight', 'model.decoder.layers.27.self_attn.v_proj.bias', 'model.decoder.layers.27.self_attn.q_proj.weight', 'model.decoder.layers.27.self_attn.q_proj.bias', 'model.decoder.layers.27.self_attn.out_proj.weight', 'model.decoder.layers.27.self_attn.out_proj.bias', 'model.decoder.layers.27.self_attn_layer_norm.weight', 'model.decoder.layers.27.self_attn_layer_norm.bias', 'model.decoder.layers.27.fc1.weight', 'model.decoder.layers.27.fc1.bias', 'model.decoder.layers.27.fc2.weight', 'model.decoder.layers.27.fc2.bias', 'model.decoder.layers.27.final_layer_norm.weight', 'model.decoder.layers.27.final_layer_norm.bias', 'model.decoder.layers.28.self_attn.k_proj.weight', 'model.decoder.layers.28.self_attn.k_proj.bias', 'model.decoder.layers.28.self_attn.v_proj.weight', 'model.decoder.layers.28.self_attn.v_proj.bias', 'model.decoder.layers.28.self_attn.q_proj.weight', 'model.decoder.layers.28.self_attn.q_proj.bias', 'model.decoder.layers.28.self_attn.out_proj.weight', 'model.decoder.layers.28.self_attn.out_proj.bias', 'model.decoder.layers.28.self_attn_layer_norm.weight', 'model.decoder.layers.28.self_attn_layer_norm.bias', 'model.decoder.layers.28.fc1.weight', 'model.decoder.layers.28.fc1.bias', 'model.decoder.layers.28.fc2.weight', 'model.decoder.layers.28.fc2.bias', 'model.decoder.layers.28.final_layer_norm.weight', 'model.decoder.layers.28.final_layer_norm.bias', 'model.decoder.layers.29.self_attn.k_proj.weight', 'model.decoder.layers.29.self_attn.k_proj.bias', 'model.decoder.layers.29.self_attn.v_proj.weight', 'model.decoder.layers.29.self_attn.v_proj.bias', 'model.decoder.layers.29.self_attn.q_proj.weight', 'model.decoder.layers.29.self_attn.q_proj.bias', 'model.decoder.layers.29.self_attn.out_proj.weight', 'model.decoder.layers.29.self_attn.out_proj.bias', 'model.decoder.layers.29.self_attn_layer_norm.weight', 'model.decoder.layers.29.self_attn_layer_norm.bias', 'model.decoder.layers.29.fc1.weight', 'model.decoder.layers.29.fc1.bias', 'model.decoder.layers.29.fc2.weight', 'model.decoder.layers.29.fc2.bias', 'model.decoder.layers.29.final_layer_norm.weight', 'model.decoder.layers.29.final_layer_norm.bias', 'model.decoder.layers.30.self_attn.k_proj.weight', 'model.decoder.layers.30.self_attn.k_proj.bias', 'model.decoder.layers.30.self_attn.v_proj.weight', 'model.decoder.layers.30.self_attn.v_proj.bias', 'model.decoder.layers.30.self_attn.q_proj.weight', 'model.decoder.layers.30.self_attn.q_proj.bias', 'model.decoder.layers.30.self_attn.out_proj.weight', 'model.decoder.layers.30.self_attn.out_proj.bias', 'model.decoder.layers.30.self_attn_layer_norm.weight', 'model.decoder.layers.30.self_attn_layer_norm.bias', 'model.decoder.layers.30.fc1.weight', 'model.decoder.layers.30.fc1.bias', 'model.decoder.layers.30.fc2.weight', 'model.decoder.layers.30.fc2.bias', 'model.decoder.layers.30.final_layer_norm.weight', 'model.decoder.layers.30.final_layer_norm.bias', 'model.decoder.layers.31.self_attn.k_proj.weight', 'model.decoder.layers.31.self_attn.k_proj.bias', 'model.decoder.layers.31.self_attn.v_proj.weight', 'model.decoder.layers.31.self_attn.v_proj.bias', 'model.decoder.layers.31.self_attn.q_proj.weight', 'model.decoder.layers.31.self_attn.q_proj.bias', 'model.decoder.layers.31.self_attn.out_proj.weight', 'model.decoder.layers.31.self_attn.out_proj.bias', 'model.decoder.layers.31.self_attn_layer_norm.weight', 'model.decoder.layers.31.self_attn_layer_norm.bias', 'model.decoder.layers.31.fc1.weight', 'model.decoder.layers.31.fc1.bias', 'model.decoder.layers.31.fc2.weight', 'model.decoder.layers.31.fc2.bias', 'model.decoder.layers.31.final_layer_norm.weight', 'model.decoder.layers.31.final_layer_norm.bias']
- This IS expected if you are initializing GPTNeoForCausalLM from a Flax model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a FlaxBertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoForCausalLM from a Flax model that you expect to be exactly identical (e.g. initializing a BertForSequenceClassification model from a FlaxBertForSequenceClassification model).
Some weights of GPTNeoForCausalLM were not initialized from the Flax model and are newly initialized: ['transformer.h.17.attn.attention.q_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.9.attn.attention.q_proj.weight', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.22.attn.attention.k_proj.weight', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.19.attn.attention.v_proj.weight', 'transformer.h.22.ln_1.weight', 'transformer.h.23.attn.attention.k_proj.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.9.attn.attention.bias', 'transformer.h.19.ln_1.weight', 'transformer.h.21.attn.attention.k_proj.weight', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.21.attn.attention.v_proj.weight', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.18.ln_2.weight', 'transformer.h.0.attn.attention.v_proj.weight', 'transformer.h.15.attn.attention.bias', 'transformer.h.9.ln_2.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.25.attn.attention.out_proj.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.10.ln_2.weight', 'transformer.h.16.attn.attention.v_proj.weight', 'transformer.h.26.ln_1.weight', 'transformer.h.15.ln_1.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.31.attn.attention.masked_bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.1.attn.attention.out_proj.weight', 'transformer.h.25.ln_1.weight', 'transformer.h.29.ln_1.weight', 'transformer.h.25.attn.attention.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.2.ln_1.bias', 'transformer.h.22.ln_2.bias', 'transformer.h.14.attn.attention.q_proj.weight', 'transformer.h.5.attn.attention.k_proj.weight', 'transformer.h.1.attn.attention.v_proj.weight', 'transformer.h.9.attn.attention.v_proj.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.2.attn.attention.q_proj.weight', 'transformer.h.14.ln_2.weight', 'transformer.h.8.ln_2.weight', 'transformer.h.1.attn.attention.bias', 'transformer.h.3.attn.attention.out_proj.weight', 'transformer.h.0.attn.attention.k_proj.weight', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.25.mlp.c_fc.bias', 'transformer.h.0.attn.attention.q_proj.weight', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.25.attn.attention.out_proj.weight', 'transformer.h.9.ln_2.weight', 'transformer.h.17.ln_2.bias', 'transformer.h.22.attn.attention.masked_bias', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.8.ln_1.bias', 'transformer.h.24.ln_2.weight', 'transformer.h.12.attn.attention.masked_bias', 'transformer.h.19.attn.attention.out_proj.bias', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.25.ln_2.weight', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.16.attn.attention.bias', 'transformer.h.26.attn.attention.masked_bias', 'transformer.h.1.ln_1.bias', 'transformer.h.8.attn.attention.v_proj.weight', 'transformer.h.0.attn.attention.out_proj.bias', 'transformer.h.20.attn.attention.k_proj.weight', 'transformer.h.31.attn.attention.k_proj.weight', 'transformer.h.9.attn.attention.k_proj.weight', 'transformer.ln_f.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.14.ln_1.bias', 'transformer.h.30.attn.attention.out_proj.weight', 'transformer.h.22.attn.attention.v_proj.weight', 'transformer.h.27.attn.attention.q_proj.weight', 'transformer.h.11.ln_2.weight', 'transformer.h.31.attn.attention.v_proj.weight', 'transformer.h.8.attn.attention.bias', 'transformer.h.0.ln_2.bias', 'transformer.h.6.attn.attention.out_proj.bias', 'transformer.h.17.ln_1.weight', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.26.attn.attention.k_proj.weight', 'transformer.h.29.ln_2.weight', 'transformer.h.6.attn.attention.q_proj.weight', 'transformer.h.5.attn.attention.v_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.23.ln_1.weight', 'transformer.h.19.ln_2.weight', 'transformer.h.17.attn.attention.masked_bias', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.17.attn.attention.k_proj.weight', 'transformer.h.9.ln_1.weight', 'transformer.h.6.attn.attention.masked_bias', 'transformer.h.28.attn.attention.k_proj.weight', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.23.attn.attention.masked_bias', 'transformer.h.4.attn.attention.out_proj.bias', 'transformer.h.29.mlp.c_proj.weight', 'transformer.h.20.ln_2.weight', 'transformer.h.11.attn.attention.out_proj.bias', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.31.attn.attention.out_proj.bias', 'transformer.h.25.mlp.c_fc.weight', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.7.attn.attention.k_proj.weight', 'transformer.h.17.attn.attention.out_proj.bias', 'transformer.h.20.attn.attention.out_proj.weight', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.29.attn.attention.bias', 'transformer.h.29.attn.attention.k_proj.weight', 'transformer.h.17.attn.attention.out_proj.weight', 'transformer.h.30.ln_2.weight', 'transformer.h.3.attn.attention.bias', 'transformer.h.2.ln_2.bias', 'transformer.h.11.ln_2.bias', 'transformer.h.27.ln_2.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.22.attn.attention.out_proj.weight', 'transformer.h.26.attn.attention.v_proj.weight', 'transformer.h.1.attn.attention.q_proj.weight', 'transformer.h.30.mlp.c_fc.bias', 'transformer.h.24.attn.attention.out_proj.weight', 'transformer.h.11.attn.attention.masked_bias', 'transformer.h.22.ln_1.bias', 'transformer.h.16.attn.attention.out_proj.bias', 'transformer.h.31.attn.attention.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.31.ln_1.bias', 'transformer.h.26.attn.attention.bias', 'transformer.h.13.ln_2.bias', 'transformer.h.24.ln_1.weight', 'transformer.h.6.attn.attention.v_proj.weight', 'transformer.h.12.ln_2.weight', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.1.ln_2.bias', 'transformer.h.15.attn.attention.out_proj.weight', 'transformer.h.20.attn.attention.q_proj.weight', 'transformer.h.23.attn.attention.q_proj.weight', 'transformer.h.12.attn.attention.out_proj.weight', 'transformer.h.25.attn.attention.v_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.wte.weight', 'transformer.h.7.attn.attention.out_proj.bias', 'transformer.h.12.ln_2.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.13.attn.attention.masked_bias', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.2.attn.attention.bias', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.15.ln_2.bias', 'transformer.h.14.attn.attention.out_proj.bias', 'transformer.h.24.attn.attention.q_proj.weight', 'transformer.h.11.attn.attention.bias', 'transformer.h.5.attn.attention.out_proj.weight', 'transformer.h.16.attn.attention.q_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.18.attn.attention.v_proj.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.4.attn.attention.masked_bias', 'transformer.h.27.mlp.c_fc.weight', 'transformer.h.28.attn.attention.out_proj.bias', 'transformer.h.22.attn.attention.out_proj.bias', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.20.attn.attention.masked_bias', 'transformer.h.24.attn.attention.v_proj.weight', 'transformer.h.0.ln_2.weight', 'transformer.h.4.ln_2.weight', 'transformer.h.14.ln_2.bias', 'transformer.h.10.attn.attention.k_proj.weight', 'transformer.h.2.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.6.attn.attention.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.24.mlp.c_fc.weight', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.18.attn.attention.k_proj.weight', 'transformer.h.23.attn.attention.out_proj.bias', 'transformer.h.31.attn.attention.out_proj.weight', 'transformer.h.23.attn.attention.v_proj.weight', 'transformer.h.10.ln_1.weight', 'transformer.h.8.attn.attention.k_proj.weight', 'transformer.h.18.attn.attention.bias', 'transformer.h.25.ln_1.bias', 'transformer.h.27.attn.attention.out_proj.bias', 'transformer.h.13.attn.attention.out_proj.weight', 'transformer.h.27.attn.attention.masked_bias', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.18.ln_2.bias', 'transformer.h.18.attn.attention.out_proj.bias', 'transformer.h.31.mlp.c_proj.weight', 'transformer.h.5.ln_1.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.wpe.weight', 'transformer.h.28.ln_2.weight', 'transformer.h.13.attn.attention.bias', 'transformer.h.24.attn.attention.masked_bias', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.7.attn.attention.out_proj.weight', 'transformer.h.9.attn.attention.masked_bias', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.7.attn.attention.q_proj.weight', 'transformer.h.29.attn.attention.masked_bias', 'transformer.h.8.attn.attention.out_proj.bias', 'transformer.h.29.mlp.c_fc.bias', 'transformer.h.29.attn.attention.q_proj.weight', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.17.attn.attention.bias', 'transformer.h.0.attn.attention.masked_bias', 'transformer.h.13.attn.attention.v_proj.weight', 'transformer.h.15.attn.attention.v_proj.weight', 'transformer.h.4.attn.attention.v_proj.weight', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.10.ln_2.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.10.attn.attention.out_proj.bias', 'transformer.h.26.ln_2.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.20.attn.attention.out_proj.bias', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.13.attn.attention.out_proj.bias', 'transformer.h.13.ln_1.bias', 'transformer.h.22.attn.attention.q_proj.weight', 'transformer.h.5.attn.attention.q_proj.weight', 'transformer.h.3.attn.attention.k_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.11.attn.attention.out_proj.weight', 'transformer.h.11.ln_1.weight', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.20.attn.attention.v_proj.weight', 'transformer.h.27.attn.attention.k_proj.weight', 'transformer.h.30.attn.attention.masked_bias', 'transformer.h.31.ln_2.bias', 'transformer.h.0.attn.attention.out_proj.weight', 'transformer.h.2.attn.attention.masked_bias', 'transformer.h.28.attn.attention.bias', 'transformer.h.3.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.10.attn.attention.masked_bias', 'transformer.h.29.ln_2.bias', 'transformer.h.23.attn.attention.out_proj.weight', 'transformer.h.10.attn.attention.v_proj.weight', 'transformer.h.3.ln_2.weight', 'transformer.h.4.attn.attention.out_proj.weight', 'transformer.h.4.attn.attention.q_proj.weight', 'transformer.h.29.mlp.c_fc.weight', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.24.ln_1.bias', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.27.attn.attention.out_proj.weight', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.17.ln_1.bias', 'transformer.h.19.attn.attention.k_proj.weight', 'transformer.h.15.ln_1.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.12.attn.attention.k_proj.weight', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.28.attn.attention.v_proj.weight', 'transformer.h.5.attn.attention.out_proj.bias', 'transformer.h.10.attn.attention.q_proj.weight', 'transformer.h.23.ln_2.bias', 'transformer.h.20.ln_1.weight', 'transformer.h.30.ln_2.bias', 'transformer.h.6.ln_1.bias', 'transformer.h.18.ln_1.bias', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.28.mlp.c_fc.bias', 'transformer.h.16.ln_1.bias', 'transformer.h.29.attn.attention.out_proj.weight', 'transformer.h.21.ln_2.bias', 'transformer.ln_f.weight', 'transformer.h.15.attn.attention.q_proj.weight', 'transformer.h.5.attn.attention.bias', 'transformer.h.0.ln_1.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.21.attn.attention.out_proj.weight', 'transformer.h.19.attn.attention.q_proj.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.28.attn.attention.q_proj.weight', 'transformer.h.27.attn.attention.bias', 'transformer.h.30.attn.attention.bias', 'transformer.h.16.ln_1.weight', 'transformer.h.31.ln_2.weight', 'transformer.h.8.attn.attention.masked_bias', 'transformer.h.12.attn.attention.out_proj.bias', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.24.attn.attention.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.0.attn.attention.bias', 'transformer.h.8.attn.attention.q_proj.weight', 'transformer.h.28.mlp.c_fc.weight', 'transformer.h.28.ln_1.weight', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.17.ln_2.weight', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.11.attn.attention.v_proj.weight', 'transformer.h.21.ln_1.weight', 'transformer.h.19.ln_1.bias', 'transformer.h.2.attn.attention.k_proj.weight', 'transformer.h.6.attn.attention.k_proj.weight', 'transformer.h.2.attn.attention.out_proj.bias', 'transformer.h.4.attn.attention.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.3.attn.attention.q_proj.weight', 'transformer.h.15.ln_2.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.14.attn.attention.v_proj.weight', 'transformer.h.31.mlp.c_fc.bias', 'transformer.h.27.ln_1.weight', 'transformer.h.23.ln_1.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.2.attn.attention.v_proj.weight', 'transformer.h.6.attn.attention.out_proj.weight', 'transformer.h.29.ln_1.bias', 'transformer.h.3.attn.attention.out_proj.bias', 'transformer.h.13.ln_2.weight', 'transformer.h.19.ln_2.bias', 'transformer.h.28.mlp.c_proj.weight', 'transformer.h.21.ln_2.weight', 'transformer.h.26.ln_1.bias', 'transformer.h.14.attn.attention.k_proj.weight', 'transformer.h.27.mlp.c_proj.weight', 'lm_head.weight', 'transformer.h.13.attn.attention.q_proj.weight', 'transformer.h.30.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.27.mlp.c_fc.bias', 'transformer.h.7.attn.attention.v_proj.weight', 'transformer.h.16.ln_2.weight', 'transformer.h.6.ln_1.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.14.ln_1.weight', 'transformer.h.30.mlp.c_proj.weight', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.8.ln_1.weight', 'transformer.h.1.ln_2.weight', 'transformer.h.2.ln_2.weight', 'transformer.h.13.attn.attention.k_proj.weight', 'transformer.h.30.attn.attention.k_proj.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.22.ln_2.weight', 'transformer.h.16.ln_2.bias', 'transformer.h.26.attn.attention.out_proj.weight', 'transformer.h.28.ln_1.bias', 'transformer.h.27.ln_2.bias', 'transformer.h.9.attn.attention.out_proj.bias', 'transformer.h.28.attn.attention.masked_bias', 'transformer.h.30.attn.attention.v_proj.weight', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.20.ln_1.bias', 'transformer.h.10.attn.attention.out_proj.weight', 'transformer.h.31.ln_1.weight', 'transformer.h.6.ln_2.weight', 'transformer.h.25.mlp.c_proj.weight', 'transformer.h.25.attn.attention.masked_bias', 'transformer.h.30.ln_1.weight', 'transformer.h.8.attn.attention.out_proj.weight', 'transformer.h.30.attn.attention.q_proj.weight', 'transformer.h.4.attn.attention.k_proj.weight', 'transformer.h.24.ln_2.bias', 'transformer.h.30.ln_1.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.24.mlp.c_proj.weight', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.6.ln_2.bias', 'transformer.h.18.attn.attention.out_proj.weight', 'transformer.h.29.attn.attention.out_proj.bias', 'transformer.h.31.attn.attention.q_proj.weight', 'transformer.h.19.attn.attention.masked_bias', 'transformer.h.21.attn.attention.q_proj.weight', 'transformer.h.3.attn.attention.v_proj.weight', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.29.attn.attention.v_proj.weight', 'transformer.h.22.attn.attention.bias', 'transformer.h.18.attn.attention.q_proj.weight', 'transformer.h.10.attn.attention.bias', 'transformer.h.18.ln_1.weight', 'transformer.h.9.attn.attention.out_proj.weight', 'transformer.h.15.attn.attention.masked_bias', 'transformer.h.4.ln_1.bias', 'transformer.h.30.attn.attention.out_proj.bias', 'transformer.h.27.ln_1.bias', 'transformer.h.5.attn.attention.masked_bias', 'transformer.h.12.attn.attention.q_proj.weight', 'transformer.h.13.ln_1.weight', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.18.attn.attention.masked_bias', 'transformer.h.7.ln_1.bias', 'transformer.h.2.attn.attention.out_proj.weight', 'transformer.h.21.attn.attention.masked_bias', 'transformer.h.16.attn.attention.masked_bias', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.19.attn.attention.out_proj.weight', 'transformer.h.1.attn.attention.out_proj.bias', 'transformer.h.15.attn.attention.out_proj.bias', 'transformer.h.25.attn.attention.k_proj.weight', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.26.mlp.c_fc.bias', 'transformer.h.28.ln_2.bias', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.24.attn.attention.k_proj.weight', 'transformer.h.23.ln_2.weight', 'transformer.h.31.mlp.c_fc.weight', 'transformer.h.26.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.3.attn.attention.masked_bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.7.ln_2.bias', 'transformer.h.12.attn.attention.v_proj.weight', 'transformer.h.12.attn.attention.bias', 'transformer.h.26.ln_2.bias', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.14.attn.attention.out_proj.weight', 'transformer.h.26.mlp.c_proj.weight', 'transformer.h.11.attn.attention.q_proj.weight', 'transformer.h.12.ln_1.bias', 'transformer.h.16.attn.attention.k_proj.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.16.attn.attention.out_proj.weight', 'transformer.h.1.attn.attention.masked_bias', 'transformer.h.17.attn.attention.v_proj.weight', 'transformer.h.24.mlp.c_fc.bias', 'transformer.h.26.attn.attention.q_proj.weight', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.12.ln_1.weight', 'transformer.h.15.attn.attention.k_proj.weight', 'transformer.h.14.attn.attention.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.28.attn.attention.out_proj.weight', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.21.attn.attention.out_proj.bias', 'transformer.h.7.attn.attention.masked_bias', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.24.attn.attention.out_proj.bias', 'transformer.h.27.attn.attention.v_proj.weight', 'transformer.h.20.attn.attention.bias', 'transformer.h.26.attn.attention.out_proj.bias', 'transformer.h.4.ln_2.bias', 'transformer.h.19.attn.attention.bias', 'transformer.h.1.attn.attention.k_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.21.ln_1.bias', 'transformer.h.25.attn.attention.q_proj.weight', 'transformer.h.25.ln_2.bias', 'transformer.h.14.attn.attention.masked_bias', 'transformer.h.20.ln_2.bias', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.11.attn.attention.k_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2022-11-18 16:06:29,167][datasets.builder][WARNING] - Using custom data configuration ViktorThink--mountain_combined_813306-6918b9ea07482433
[2022-11-18 16:06:29,264][datasets.builder][WARNING] - Found cached dataset csv (/home/paperspace/.cache/huggingface/datasets/ViktorThink___csv/ViktorThink--mountain_combined_813306-6918b9ea07482433/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  5.42it/s]100%|██████████| 2/2 [00:00<00:00, 10.21it/s]
Running tokenizer on dataset:   0%|          | 0/814 [00:00<?, ?ba/s]Running tokenizer on dataset:   0%|          | 1/814 [00:02<34:49,  2.57s/ba]Running tokenizer on dataset:   0%|          | 2/814 [00:04<32:05,  2.37s/ba]Running tokenizer on dataset:   0%|          | 3/814 [00:07<30:58,  2.29s/ba]Running tokenizer on dataset:   0%|          | 4/814 [00:09<29:58,  2.22s/ba]Running tokenizer on dataset:   1%|          | 5/814 [00:11<29:15,  2.17s/ba]Running tokenizer on dataset:   1%|          | 6/814 [00:13<28:21,  2.11s/ba]Running tokenizer on dataset:   1%|          | 7/814 [00:15<28:04,  2.09s/ba]Running tokenizer on dataset:   1%|          | 8/814 [00:17<27:40,  2.06s/ba]Running tokenizer on dataset:   1%|          | 9/814 [00:19<27:11,  2.03s/ba]Running tokenizer on dataset:   1%|          | 10/814 [00:21<26:45,  2.00s/ba]Running tokenizer on dataset:   1%|▏         | 11/814 [00:23<26:44,  2.00s/ba]Running tokenizer on dataset:   1%|▏         | 12/814 [00:25<26:33,  1.99s/ba]Running tokenizer on dataset:   2%|▏         | 13/814 [00:27<26:28,  1.98s/ba]Running tokenizer on dataset:   2%|▏         | 14/814 [00:29<26:22,  1.98s/ba]Running tokenizer on dataset:   2%|▏         | 15/814 [00:30<26:14,  1.97s/ba]Running tokenizer on dataset:   2%|▏         | 16/814 [00:32<25:59,  1.95s/ba]Running tokenizer on dataset:   2%|▏         | 17/814 [00:34<25:43,  1.94s/ba]Running tokenizer on dataset:   2%|▏         | 18/814 [00:36<25:32,  1.92s/ba]Running tokenizer on dataset:   2%|▏         | 19/814 [00:38<25:20,  1.91s/ba]Running tokenizer on dataset:   2%|▏         | 20/814 [00:40<25:22,  1.92s/ba]Running tokenizer on dataset:   3%|▎         | 21/814 [00:42<25:19,  1.92s/ba]Running tokenizer on dataset:   3%|▎         | 22/814 [00:44<25:03,  1.90s/ba]Running tokenizer on dataset:   3%|▎         | 23/814 [00:46<24:50,  1.88s/ba]Running tokenizer on dataset:   3%|▎         | 24/814 [00:47<24:45,  1.88s/ba]Running tokenizer on dataset:   3%|▎         | 25/814 [00:49<24:43,  1.88s/ba]Running tokenizer on dataset:   3%|▎         | 26/814 [00:51<24:35,  1.87s/ba]Running tokenizer on dataset:   3%|▎         | 27/814 [00:53<24:18,  1.85s/ba]Running tokenizer on dataset:   3%|▎         | 28/814 [00:55<24:23,  1.86s/ba]Running tokenizer on dataset:   4%|▎         | 29/814 [00:57<24:30,  1.87s/ba]Running tokenizer on dataset:   4%|▎         | 30/814 [00:59<24:29,  1.87s/ba]Running tokenizer on dataset:   4%|▍         | 31/814 [01:01<24:18,  1.86s/ba]Running tokenizer on dataset:   4%|▍         | 32/814 [01:02<24:30,  1.88s/ba]Running tokenizer on dataset:   4%|▍         | 33/814 [01:04<24:23,  1.87s/ba]Running tokenizer on dataset:   4%|▍         | 34/814 [01:06<24:05,  1.85s/ba]Running tokenizer on dataset:   4%|▍         | 35/814 [01:08<23:55,  1.84s/ba]Running tokenizer on dataset:   4%|▍         | 36/814 [01:10<23:38,  1.82s/ba]Running tokenizer on dataset:   5%|▍         | 37/814 [01:11<23:31,  1.82s/ba]Running tokenizer on dataset:   5%|▍         | 38/814 [01:13<23:17,  1.80s/ba]Running tokenizer on dataset:   5%|▍         | 39/814 [01:15<23:34,  1.83s/ba]Running tokenizer on dataset:   5%|▍         | 40/814 [01:17<23:34,  1.83s/ba]Running tokenizer on dataset:   5%|▌         | 41/814 [01:19<23:33,  1.83s/ba]Running tokenizer on dataset:   5%|▌         | 42/814 [01:21<23:33,  1.83s/ba]Running tokenizer on dataset:   5%|▌         | 43/814 [01:22<23:27,  1.83s/ba]Running tokenizer on dataset:   5%|▌         | 44/814 [01:24<23:43,  1.85s/ba]Running tokenizer on dataset:   6%|▌         | 45/814 [01:26<23:34,  1.84s/ba]Running tokenizer on dataset:   6%|▌         | 46/814 [01:28<23:22,  1.83s/ba]Running tokenizer on dataset:   6%|▌         | 47/814 [01:30<23:12,  1.81s/ba]Running tokenizer on dataset:   6%|▌         | 48/814 [01:32<22:57,  1.80s/ba]Running tokenizer on dataset:   6%|▌         | 49/814 [01:33<23:00,  1.80s/ba]Running tokenizer on dataset:   6%|▌         | 50/814 [01:35<22:56,  1.80s/ba]Running tokenizer on dataset:   6%|▋         | 51/814 [01:37<22:45,  1.79s/ba]Running tokenizer on dataset:   6%|▋         | 52/814 [01:39<22:54,  1.80s/ba]Running tokenizer on dataset:   7%|▋         | 53/814 [01:41<22:49,  1.80s/ba]Running tokenizer on dataset:   7%|▋         | 54/814 [01:42<23:01,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 55/814 [01:44<23:02,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 56/814 [01:46<22:59,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 57/814 [01:48<23:15,  1.84s/ba]Running tokenizer on dataset:   7%|▋         | 58/814 [01:50<22:59,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 59/814 [01:51<22:49,  1.81s/ba]Running tokenizer on dataset:   7%|▋         | 60/814 [01:53<22:40,  1.80s/ba]Running tokenizer on dataset:   7%|▋         | 61/814 [01:55<22:38,  1.80s/ba]Running tokenizer on dataset:   8%|▊         | 62/814 [01:57<22:38,  1.81s/ba]Running tokenizer on dataset:   8%|▊         | 63/814 [01:59<22:29,  1.80s/ba]Running tokenizer on dataset:   8%|▊         | 64/814 [02:00<22:20,  1.79s/ba]Running tokenizer on dataset:   8%|▊         | 65/814 [02:02<22:28,  1.80s/ba]Running tokenizer on dataset:   8%|▊         | 66/814 [02:04<22:33,  1.81s/ba]Running tokenizer on dataset:   8%|▊         | 67/814 [02:06<22:17,  1.79s/ba]Running tokenizer on dataset:   8%|▊         | 68/814 [02:08<22:10,  1.78s/ba]Running tokenizer on dataset:   8%|▊         | 69/814 [02:09<22:05,  1.78s/ba]Running tokenizer on dataset:   9%|▊         | 70/814 [02:11<21:59,  1.77s/ba]Running tokenizer on dataset:   9%|▊         | 71/814 [02:13<21:52,  1.77s/ba]Running tokenizer on dataset:   9%|▉         | 72/814 [02:15<24:00,  1.94s/ba]Running tokenizer on dataset:   9%|▉         | 73/814 [02:17<23:21,  1.89s/ba]Running tokenizer on dataset:   9%|▉         | 74/814 [02:19<22:47,  1.85s/ba]Running tokenizer on dataset:   9%|▉         | 75/814 [02:21<22:26,  1.82s/ba]Running tokenizer on dataset:   9%|▉         | 76/814 [02:22<22:52,  1.86s/ba]Running tokenizer on dataset:   9%|▉         | 77/814 [02:24<22:30,  1.83s/ba]Running tokenizer on dataset:  10%|▉         | 78/814 [02:26<22:20,  1.82s/ba]Running tokenizer on dataset:  10%|▉         | 79/814 [02:28<22:03,  1.80s/ba]Running tokenizer on dataset:  10%|▉         | 79/814 [02:29<23:13,  1.90s/ba]
Error in sys.excepthook:
Traceback (most recent call last):
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1694, in print
    extend(render(renderable, render_options))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/constrain.py", line 29, in __rich_console__
    yield from console.render(self.renderable, child_options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/panel.py", line 220, in __rich_console__
    lines = console.render_lines(renderable, child_options, style=style)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 511, in __rich_console__
    yield from self._render(console, render_options, widths)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 824, in _render
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/panel.py", line 220, in __rich_console__
    lines = console.render_lines(renderable, child_options, style=style)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 511, in __rich_console__
    yield from self._render(console, render_options, widths)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 824, in _render
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/pretty.py", line 350, in __rich_console__
    self.highlighter(pretty_text)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/highlighter.py", line 38, in __call__
    self.highlight(highlight_text)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/highlighter.py", line 77, in highlight
    highlight_regex(re_highlight, style_prefix=self.base_style)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/text.py", line 579, in highlight_regex
    for match in re.finditer(re_highlight, plain):
KeyboardInterrupt

Original exception was:
Traceback (most recent call last):
  File "finetune_using_clm_wandb.py", line 599, in <module>
    main()
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/main.py", line 90, in decorated_main
    _run_hydra(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 389, in _run_hydra
    _run_app(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 452, in _run_app
    run_and_report(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 213, in run_and_report
    return func()
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 453, in <lambda>
    lambda: hydra.run(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "finetune_using_clm_wandb.py", line 304, in main
    tokenized_datasets = preprocess(cfg, accelerator, tokenizer, raw_datasets)
  File "finetune_using_clm_wandb.py", line 215, in preprocess
    tokenized_datasets = raw_datasets.map(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/dataset_dict.py", line 777, in map
    {
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/dataset_dict.py", line 778, in <dictcomp>
    k: dataset.map(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2585, in map
    return self._map_single(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 585, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 552, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/fingerprint.py", line 480, in wrapper
    out = func(self, *args, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2982, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2865, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2545, in decorated
    result = f(decorated_item, *args, **kwargs)
  File "finetune_using_clm_wandb.py", line 204, in tokenize_fn
    result = tokenizer(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2488, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2574, in _call_one
    return self.batch_encode_plus(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2765, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 733, in _batch_encode_plus
    first_ids = get_input_ids(ids)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 700, in get_input_ids
    tokens = self.tokenize(text, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 547, in tokenize
    tokenized_text.extend(self._tokenize(token))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 303, in _tokenize
    bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(" "))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 210, in bpe
    if token in self.cache:
KeyboardInterrupt
[2022-11-18 16:09:15,017][__main__][INFO] - Setting random seed to 17
[2022-11-18 16:09:15,018][__main__][INFO] - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

[2022-11-18 16:09:15,022][__main__][INFO] - output_dir: tuned-model
bittensor:
  network: nobunaga
dataset:
  name: ViktorThink/mountain_combined_813306
  config_name: null
  num_batches: 2
  block_size: 256
  overwrite_cache: true
  keep_linebreaks: true
  concatenate_raw: false
  load_tokenized_data: false
model:
  name: facebook/opt-2.7b
  config_name: null
tokenizer:
  name: null
  use_fast: true
  preprocessing_num_workers: null
  pad_token: '[PAD]'
training:
  seed: 17
  val_split_percent: 20
  train_batch_size: 32
  eval_batch_size: 16
  learning_rate: 3.0e-06
  weight_decay: 0.05
  num_epochs: 4
  max_train_steps: null
  gradient_accumulation_steps: 2
  lr_scheduler: constant
  lr_warmup_steps: 5
  eval_every: 250
  max_eval_steps: 500
  checkpoint:
    resume_from_checkpoint: 0
    every_n_steps: null
tracking:
  enabled: true
  report_to: all
testing:
  enabled: false

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/config.json
Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-2.7B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      16
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 20,
  "num_layers": 32,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading file vocab.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/vocab.json
loading file merges.txt from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/merges.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/tokenizer_config.json
loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading weights file flax_model.msgpack from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Loading Flax weights from /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Some weights of the Flax model were not used when initializing the PyTorch model GPTNeoForCausalLM: ['model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.final_layer_norm.weight', 'model.decoder.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.12.self_attn.k_proj.weight', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.weight', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.weight', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.12.self_attn_layer_norm.weight', 'model.decoder.layers.12.self_attn_layer_norm.bias', 'model.decoder.layers.12.fc1.weight', 'model.decoder.layers.12.fc1.bias', 'model.decoder.layers.12.fc2.weight', 'model.decoder.layers.12.fc2.bias', 'model.decoder.layers.12.final_layer_norm.weight', 'model.decoder.layers.12.final_layer_norm.bias', 'model.decoder.layers.13.self_attn.k_proj.weight', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.weight', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.weight', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.weight', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn_layer_norm.weight', 'model.decoder.layers.13.self_attn_layer_norm.bias', 'model.decoder.layers.13.fc1.weight', 'model.decoder.layers.13.fc1.bias', 'model.decoder.layers.13.fc2.weight', 'model.decoder.layers.13.fc2.bias', 'model.decoder.layers.13.final_layer_norm.weight', 'model.decoder.layers.13.final_layer_norm.bias', 'model.decoder.layers.14.self_attn.k_proj.weight', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.weight', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.weight', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.weight', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.self_attn_layer_norm.weight', 'model.decoder.layers.14.self_attn_layer_norm.bias', 'model.decoder.layers.14.fc1.weight', 'model.decoder.layers.14.fc1.bias', 'model.decoder.layers.14.fc2.weight', 'model.decoder.layers.14.fc2.bias', 'model.decoder.layers.14.final_layer_norm.weight', 'model.decoder.layers.14.final_layer_norm.bias', 'model.decoder.layers.15.self_attn.k_proj.weight', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.weight', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.weight', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.weight', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn_layer_norm.weight', 'model.decoder.layers.15.self_attn_layer_norm.bias', 'model.decoder.layers.15.fc1.weight', 'model.decoder.layers.15.fc1.bias', 'model.decoder.layers.15.fc2.weight', 'model.decoder.layers.15.fc2.bias', 'model.decoder.layers.15.final_layer_norm.weight', 'model.decoder.layers.15.final_layer_norm.bias', 'model.decoder.layers.16.self_attn.k_proj.weight', 'model.decoder.layers.16.self_attn.k_proj.bias', 'model.decoder.layers.16.self_attn.v_proj.weight', 'model.decoder.layers.16.self_attn.v_proj.bias', 'model.decoder.layers.16.self_attn.q_proj.weight', 'model.decoder.layers.16.self_attn.q_proj.bias', 'model.decoder.layers.16.self_attn.out_proj.weight', 'model.decoder.layers.16.self_attn.out_proj.bias', 'model.decoder.layers.16.self_attn_layer_norm.weight', 'model.decoder.layers.16.self_attn_layer_norm.bias', 'model.decoder.layers.16.fc1.weight', 'model.decoder.layers.16.fc1.bias', 'model.decoder.layers.16.fc2.weight', 'model.decoder.layers.16.fc2.bias', 'model.decoder.layers.16.final_layer_norm.weight', 'model.decoder.layers.16.final_layer_norm.bias', 'model.decoder.layers.17.self_attn.k_proj.weight', 'model.decoder.layers.17.self_attn.k_proj.bias', 'model.decoder.layers.17.self_attn.v_proj.weight', 'model.decoder.layers.17.self_attn.v_proj.bias', 'model.decoder.layers.17.self_attn.q_proj.weight', 'model.decoder.layers.17.self_attn.q_proj.bias', 'model.decoder.layers.17.self_attn.out_proj.weight', 'model.decoder.layers.17.self_attn.out_proj.bias', 'model.decoder.layers.17.self_attn_layer_norm.weight', 'model.decoder.layers.17.self_attn_layer_norm.bias', 'model.decoder.layers.17.fc1.weight', 'model.decoder.layers.17.fc1.bias', 'model.decoder.layers.17.fc2.weight', 'model.decoder.layers.17.fc2.bias', 'model.decoder.layers.17.final_layer_norm.weight', 'model.decoder.layers.17.final_layer_norm.bias', 'model.decoder.layers.18.self_attn.k_proj.weight', 'model.decoder.layers.18.self_attn.k_proj.bias', 'model.decoder.layers.18.self_attn.v_proj.weight', 'model.decoder.layers.18.self_attn.v_proj.bias', 'model.decoder.layers.18.self_attn.q_proj.weight', 'model.decoder.layers.18.self_attn.q_proj.bias', 'model.decoder.layers.18.self_attn.out_proj.weight', 'model.decoder.layers.18.self_attn.out_proj.bias', 'model.decoder.layers.18.self_attn_layer_norm.weight', 'model.decoder.layers.18.self_attn_layer_norm.bias', 'model.decoder.layers.18.fc1.weight', 'model.decoder.layers.18.fc1.bias', 'model.decoder.layers.18.fc2.weight', 'model.decoder.layers.18.fc2.bias', 'model.decoder.layers.18.final_layer_norm.weight', 'model.decoder.layers.18.final_layer_norm.bias', 'model.decoder.layers.19.self_attn.k_proj.weight', 'model.decoder.layers.19.self_attn.k_proj.bias', 'model.decoder.layers.19.self_attn.v_proj.weight', 'model.decoder.layers.19.self_attn.v_proj.bias', 'model.decoder.layers.19.self_attn.q_proj.weight', 'model.decoder.layers.19.self_attn.q_proj.bias', 'model.decoder.layers.19.self_attn.out_proj.weight', 'model.decoder.layers.19.self_attn.out_proj.bias', 'model.decoder.layers.19.self_attn_layer_norm.weight', 'model.decoder.layers.19.self_attn_layer_norm.bias', 'model.decoder.layers.19.fc1.weight', 'model.decoder.layers.19.fc1.bias', 'model.decoder.layers.19.fc2.weight', 'model.decoder.layers.19.fc2.bias', 'model.decoder.layers.19.final_layer_norm.weight', 'model.decoder.layers.19.final_layer_norm.bias', 'model.decoder.layers.20.self_attn.k_proj.weight', 'model.decoder.layers.20.self_attn.k_proj.bias', 'model.decoder.layers.20.self_attn.v_proj.weight', 'model.decoder.layers.20.self_attn.v_proj.bias', 'model.decoder.layers.20.self_attn.q_proj.weight', 'model.decoder.layers.20.self_attn.q_proj.bias', 'model.decoder.layers.20.self_attn.out_proj.weight', 'model.decoder.layers.20.self_attn.out_proj.bias', 'model.decoder.layers.20.self_attn_layer_norm.weight', 'model.decoder.layers.20.self_attn_layer_norm.bias', 'model.decoder.layers.20.fc1.weight', 'model.decoder.layers.20.fc1.bias', 'model.decoder.layers.20.fc2.weight', 'model.decoder.layers.20.fc2.bias', 'model.decoder.layers.20.final_layer_norm.weight', 'model.decoder.layers.20.final_layer_norm.bias', 'model.decoder.layers.21.self_attn.k_proj.weight', 'model.decoder.layers.21.self_attn.k_proj.bias', 'model.decoder.layers.21.self_attn.v_proj.weight', 'model.decoder.layers.21.self_attn.v_proj.bias', 'model.decoder.layers.21.self_attn.q_proj.weight', 'model.decoder.layers.21.self_attn.q_proj.bias', 'model.decoder.layers.21.self_attn.out_proj.weight', 'model.decoder.layers.21.self_attn.out_proj.bias', 'model.decoder.layers.21.self_attn_layer_norm.weight', 'model.decoder.layers.21.self_attn_layer_norm.bias', 'model.decoder.layers.21.fc1.weight', 'model.decoder.layers.21.fc1.bias', 'model.decoder.layers.21.fc2.weight', 'model.decoder.layers.21.fc2.bias', 'model.decoder.layers.21.final_layer_norm.weight', 'model.decoder.layers.21.final_layer_norm.bias', 'model.decoder.layers.22.self_attn.k_proj.weight', 'model.decoder.layers.22.self_attn.k_proj.bias', 'model.decoder.layers.22.self_attn.v_proj.weight', 'model.decoder.layers.22.self_attn.v_proj.bias', 'model.decoder.layers.22.self_attn.q_proj.weight', 'model.decoder.layers.22.self_attn.q_proj.bias', 'model.decoder.layers.22.self_attn.out_proj.weight', 'model.decoder.layers.22.self_attn.out_proj.bias', 'model.decoder.layers.22.self_attn_layer_norm.weight', 'model.decoder.layers.22.self_attn_layer_norm.bias', 'model.decoder.layers.22.fc1.weight', 'model.decoder.layers.22.fc1.bias', 'model.decoder.layers.22.fc2.weight', 'model.decoder.layers.22.fc2.bias', 'model.decoder.layers.22.final_layer_norm.weight', 'model.decoder.layers.22.final_layer_norm.bias', 'model.decoder.layers.23.self_attn.k_proj.weight', 'model.decoder.layers.23.self_attn.k_proj.bias', 'model.decoder.layers.23.self_attn.v_proj.weight', 'model.decoder.layers.23.self_attn.v_proj.bias', 'model.decoder.layers.23.self_attn.q_proj.weight', 'model.decoder.layers.23.self_attn.q_proj.bias', 'model.decoder.layers.23.self_attn.out_proj.weight', 'model.decoder.layers.23.self_attn.out_proj.bias', 'model.decoder.layers.23.self_attn_layer_norm.weight', 'model.decoder.layers.23.self_attn_layer_norm.bias', 'model.decoder.layers.23.fc1.weight', 'model.decoder.layers.23.fc1.bias', 'model.decoder.layers.23.fc2.weight', 'model.decoder.layers.23.fc2.bias', 'model.decoder.layers.23.final_layer_norm.weight', 'model.decoder.layers.23.final_layer_norm.bias', 'model.decoder.layers.24.self_attn.k_proj.weight', 'model.decoder.layers.24.self_attn.k_proj.bias', 'model.decoder.layers.24.self_attn.v_proj.weight', 'model.decoder.layers.24.self_attn.v_proj.bias', 'model.decoder.layers.24.self_attn.q_proj.weight', 'model.decoder.layers.24.self_attn.q_proj.bias', 'model.decoder.layers.24.self_attn.out_proj.weight', 'model.decoder.layers.24.self_attn.out_proj.bias', 'model.decoder.layers.24.self_attn_layer_norm.weight', 'model.decoder.layers.24.self_attn_layer_norm.bias', 'model.decoder.layers.24.fc1.weight', 'model.decoder.layers.24.fc1.bias', 'model.decoder.layers.24.fc2.weight', 'model.decoder.layers.24.fc2.bias', 'model.decoder.layers.24.final_layer_norm.weight', 'model.decoder.layers.24.final_layer_norm.bias', 'model.decoder.layers.25.self_attn.k_proj.weight', 'model.decoder.layers.25.self_attn.k_proj.bias', 'model.decoder.layers.25.self_attn.v_proj.weight', 'model.decoder.layers.25.self_attn.v_proj.bias', 'model.decoder.layers.25.self_attn.q_proj.weight', 'model.decoder.layers.25.self_attn.q_proj.bias', 'model.decoder.layers.25.self_attn.out_proj.weight', 'model.decoder.layers.25.self_attn.out_proj.bias', 'model.decoder.layers.25.self_attn_layer_norm.weight', 'model.decoder.layers.25.self_attn_layer_norm.bias', 'model.decoder.layers.25.fc1.weight', 'model.decoder.layers.25.fc1.bias', 'model.decoder.layers.25.fc2.weight', 'model.decoder.layers.25.fc2.bias', 'model.decoder.layers.25.final_layer_norm.weight', 'model.decoder.layers.25.final_layer_norm.bias', 'model.decoder.layers.26.self_attn.k_proj.weight', 'model.decoder.layers.26.self_attn.k_proj.bias', 'model.decoder.layers.26.self_attn.v_proj.weight', 'model.decoder.layers.26.self_attn.v_proj.bias', 'model.decoder.layers.26.self_attn.q_proj.weight', 'model.decoder.layers.26.self_attn.q_proj.bias', 'model.decoder.layers.26.self_attn.out_proj.weight', 'model.decoder.layers.26.self_attn.out_proj.bias', 'model.decoder.layers.26.self_attn_layer_norm.weight', 'model.decoder.layers.26.self_attn_layer_norm.bias', 'model.decoder.layers.26.fc1.weight', 'model.decoder.layers.26.fc1.bias', 'model.decoder.layers.26.fc2.weight', 'model.decoder.layers.26.fc2.bias', 'model.decoder.layers.26.final_layer_norm.weight', 'model.decoder.layers.26.final_layer_norm.bias', 'model.decoder.layers.27.self_attn.k_proj.weight', 'model.decoder.layers.27.self_attn.k_proj.bias', 'model.decoder.layers.27.self_attn.v_proj.weight', 'model.decoder.layers.27.self_attn.v_proj.bias', 'model.decoder.layers.27.self_attn.q_proj.weight', 'model.decoder.layers.27.self_attn.q_proj.bias', 'model.decoder.layers.27.self_attn.out_proj.weight', 'model.decoder.layers.27.self_attn.out_proj.bias', 'model.decoder.layers.27.self_attn_layer_norm.weight', 'model.decoder.layers.27.self_attn_layer_norm.bias', 'model.decoder.layers.27.fc1.weight', 'model.decoder.layers.27.fc1.bias', 'model.decoder.layers.27.fc2.weight', 'model.decoder.layers.27.fc2.bias', 'model.decoder.layers.27.final_layer_norm.weight', 'model.decoder.layers.27.final_layer_norm.bias', 'model.decoder.layers.28.self_attn.k_proj.weight', 'model.decoder.layers.28.self_attn.k_proj.bias', 'model.decoder.layers.28.self_attn.v_proj.weight', 'model.decoder.layers.28.self_attn.v_proj.bias', 'model.decoder.layers.28.self_attn.q_proj.weight', 'model.decoder.layers.28.self_attn.q_proj.bias', 'model.decoder.layers.28.self_attn.out_proj.weight', 'model.decoder.layers.28.self_attn.out_proj.bias', 'model.decoder.layers.28.self_attn_layer_norm.weight', 'model.decoder.layers.28.self_attn_layer_norm.bias', 'model.decoder.layers.28.fc1.weight', 'model.decoder.layers.28.fc1.bias', 'model.decoder.layers.28.fc2.weight', 'model.decoder.layers.28.fc2.bias', 'model.decoder.layers.28.final_layer_norm.weight', 'model.decoder.layers.28.final_layer_norm.bias', 'model.decoder.layers.29.self_attn.k_proj.weight', 'model.decoder.layers.29.self_attn.k_proj.bias', 'model.decoder.layers.29.self_attn.v_proj.weight', 'model.decoder.layers.29.self_attn.v_proj.bias', 'model.decoder.layers.29.self_attn.q_proj.weight', 'model.decoder.layers.29.self_attn.q_proj.bias', 'model.decoder.layers.29.self_attn.out_proj.weight', 'model.decoder.layers.29.self_attn.out_proj.bias', 'model.decoder.layers.29.self_attn_layer_norm.weight', 'model.decoder.layers.29.self_attn_layer_norm.bias', 'model.decoder.layers.29.fc1.weight', 'model.decoder.layers.29.fc1.bias', 'model.decoder.layers.29.fc2.weight', 'model.decoder.layers.29.fc2.bias', 'model.decoder.layers.29.final_layer_norm.weight', 'model.decoder.layers.29.final_layer_norm.bias', 'model.decoder.layers.30.self_attn.k_proj.weight', 'model.decoder.layers.30.self_attn.k_proj.bias', 'model.decoder.layers.30.self_attn.v_proj.weight', 'model.decoder.layers.30.self_attn.v_proj.bias', 'model.decoder.layers.30.self_attn.q_proj.weight', 'model.decoder.layers.30.self_attn.q_proj.bias', 'model.decoder.layers.30.self_attn.out_proj.weight', 'model.decoder.layers.30.self_attn.out_proj.bias', 'model.decoder.layers.30.self_attn_layer_norm.weight', 'model.decoder.layers.30.self_attn_layer_norm.bias', 'model.decoder.layers.30.fc1.weight', 'model.decoder.layers.30.fc1.bias', 'model.decoder.layers.30.fc2.weight', 'model.decoder.layers.30.fc2.bias', 'model.decoder.layers.30.final_layer_norm.weight', 'model.decoder.layers.30.final_layer_norm.bias', 'model.decoder.layers.31.self_attn.k_proj.weight', 'model.decoder.layers.31.self_attn.k_proj.bias', 'model.decoder.layers.31.self_attn.v_proj.weight', 'model.decoder.layers.31.self_attn.v_proj.bias', 'model.decoder.layers.31.self_attn.q_proj.weight', 'model.decoder.layers.31.self_attn.q_proj.bias', 'model.decoder.layers.31.self_attn.out_proj.weight', 'model.decoder.layers.31.self_attn.out_proj.bias', 'model.decoder.layers.31.self_attn_layer_norm.weight', 'model.decoder.layers.31.self_attn_layer_norm.bias', 'model.decoder.layers.31.fc1.weight', 'model.decoder.layers.31.fc1.bias', 'model.decoder.layers.31.fc2.weight', 'model.decoder.layers.31.fc2.bias', 'model.decoder.layers.31.final_layer_norm.weight', 'model.decoder.layers.31.final_layer_norm.bias']
- This IS expected if you are initializing GPTNeoForCausalLM from a Flax model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a FlaxBertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoForCausalLM from a Flax model that you expect to be exactly identical (e.g. initializing a BertForSequenceClassification model from a FlaxBertForSequenceClassification model).
Some weights of GPTNeoForCausalLM were not initialized from the Flax model and are newly initialized: ['transformer.h.28.ln_1.weight', 'transformer.h.24.attn.attention.bias', 'transformer.h.1.attn.attention.k_proj.weight', 'transformer.h.31.attn.attention.out_proj.weight', 'transformer.h.10.ln_2.weight', 'transformer.h.25.ln_2.bias', 'transformer.h.1.attn.attention.out_proj.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.29.ln_1.bias', 'transformer.h.2.attn.attention.k_proj.weight', 'transformer.h.31.attn.attention.q_proj.weight', 'transformer.h.30.ln_1.weight', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.10.ln_2.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.18.attn.attention.out_proj.weight', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.5.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.19.attn.attention.v_proj.weight', 'transformer.h.11.ln_1.weight', 'transformer.h.3.attn.attention.out_proj.weight', 'transformer.h.13.attn.attention.masked_bias', 'transformer.h.9.ln_1.weight', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.4.attn.attention.out_proj.bias', 'transformer.h.0.attn.attention.v_proj.weight', 'transformer.h.28.attn.attention.v_proj.weight', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.24.ln_2.bias', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.19.attn.attention.q_proj.weight', 'transformer.h.12.ln_1.weight', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.10.attn.attention.out_proj.weight', 'transformer.h.0.attn.attention.out_proj.weight', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.30.attn.attention.q_proj.weight', 'transformer.h.28.attn.attention.out_proj.bias', 'transformer.h.1.attn.attention.out_proj.weight', 'transformer.h.22.ln_2.weight', 'transformer.h.29.attn.attention.out_proj.weight', 'transformer.h.19.ln_2.weight', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.26.ln_1.weight', 'transformer.h.25.attn.attention.k_proj.weight', 'transformer.h.5.ln_2.weight', 'transformer.h.9.attn.attention.k_proj.weight', 'transformer.h.23.ln_1.bias', 'transformer.h.21.attn.attention.v_proj.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.16.attn.attention.v_proj.weight', 'transformer.h.2.attn.attention.bias', 'transformer.h.12.attn.attention.out_proj.bias', 'transformer.h.19.ln_1.bias', 'transformer.h.11.attn.attention.v_proj.weight', 'transformer.h.9.attn.attention.q_proj.weight', 'transformer.h.25.attn.attention.out_proj.bias', 'transformer.h.26.ln_2.bias', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.9.attn.attention.bias', 'transformer.h.8.attn.attention.out_proj.bias', 'transformer.h.23.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.7.ln_1.bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.10.attn.attention.masked_bias', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.25.attn.attention.q_proj.weight', 'transformer.h.7.attn.attention.v_proj.weight', 'transformer.h.2.attn.attention.out_proj.bias', 'transformer.h.2.attn.attention.out_proj.weight', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.23.attn.attention.q_proj.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.5.attn.attention.k_proj.weight', 'transformer.wpe.weight', 'transformer.h.25.mlp.c_proj.weight', 'transformer.h.1.attn.attention.q_proj.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.6.ln_2.bias', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.12.attn.attention.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.27.attn.attention.out_proj.weight', 'transformer.h.16.attn.attention.bias', 'transformer.h.24.ln_2.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.21.attn.attention.out_proj.bias', 'transformer.h.24.attn.attention.v_proj.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.12.ln_2.weight', 'transformer.h.22.attn.attention.v_proj.weight', 'transformer.h.7.attn.attention.out_proj.bias', 'transformer.h.26.ln_1.bias', 'transformer.h.31.ln_2.bias', 'transformer.h.4.attn.attention.k_proj.weight', 'transformer.h.1.attn.attention.bias', 'transformer.h.26.attn.attention.out_proj.bias', 'transformer.h.18.attn.attention.out_proj.bias', 'transformer.h.27.mlp.c_proj.weight', 'transformer.h.5.attn.attention.out_proj.bias', 'transformer.h.14.ln_1.weight', 'transformer.h.24.ln_1.weight', 'transformer.h.13.attn.attention.q_proj.weight', 'transformer.h.29.ln_2.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.28.attn.attention.out_proj.weight', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.18.attn.attention.k_proj.weight', 'transformer.h.3.ln_1.weight', 'transformer.h.29.mlp.c_fc.bias', 'lm_head.weight', 'transformer.h.24.attn.attention.masked_bias', 'transformer.h.20.ln_1.weight', 'transformer.h.24.mlp.c_fc.bias', 'transformer.h.31.attn.attention.v_proj.weight', 'transformer.h.8.attn.attention.v_proj.weight', 'transformer.h.15.ln_2.weight', 'transformer.h.23.ln_2.bias', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.31.ln_1.bias', 'transformer.h.20.attn.attention.masked_bias', 'transformer.h.2.attn.attention.masked_bias', 'transformer.h.21.ln_1.weight', 'transformer.h.20.attn.attention.v_proj.weight', 'transformer.h.9.ln_2.weight', 'transformer.h.21.ln_2.weight', 'transformer.h.27.attn.attention.v_proj.weight', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.30.mlp.c_proj.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.15.attn.attention.k_proj.weight', 'transformer.h.16.ln_1.weight', 'transformer.h.27.attn.attention.out_proj.bias', 'transformer.h.18.attn.attention.bias', 'transformer.h.4.ln_2.bias', 'transformer.h.31.attn.attention.k_proj.weight', 'transformer.h.29.attn.attention.masked_bias', 'transformer.h.24.attn.attention.out_proj.bias', 'transformer.h.30.attn.attention.v_proj.weight', 'transformer.h.29.mlp.c_proj.weight', 'transformer.h.19.attn.attention.out_proj.bias', 'transformer.h.31.attn.attention.masked_bias', 'transformer.h.8.attn.attention.masked_bias', 'transformer.h.19.ln_2.bias', 'transformer.h.22.attn.attention.k_proj.weight', 'transformer.h.23.attn.attention.masked_bias', 'transformer.h.20.attn.attention.out_proj.weight', 'transformer.h.23.attn.attention.out_proj.bias', 'transformer.h.22.ln_2.bias', 'transformer.h.21.attn.attention.q_proj.weight', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.5.ln_1.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.17.attn.attention.masked_bias', 'transformer.h.3.ln_1.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.21.ln_2.bias', 'transformer.h.13.ln_1.weight', 'transformer.h.27.mlp.c_fc.weight', 'transformer.h.17.ln_1.weight', 'transformer.h.3.attn.attention.masked_bias', 'transformer.h.28.attn.attention.q_proj.weight', 'transformer.h.12.attn.attention.out_proj.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.7.attn.attention.out_proj.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.11.ln_1.bias', 'transformer.h.25.attn.attention.masked_bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.7.ln_2.bias', 'transformer.h.18.ln_2.bias', 'transformer.h.19.attn.attention.masked_bias', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.4.attn.attention.out_proj.weight', 'transformer.h.18.ln_1.weight', 'transformer.h.14.attn.attention.k_proj.weight', 'transformer.h.22.ln_1.weight', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.8.attn.attention.k_proj.weight', 'transformer.h.27.attn.attention.q_proj.weight', 'transformer.h.8.attn.attention.q_proj.weight', 'transformer.h.30.ln_2.weight', 'transformer.h.28.attn.attention.k_proj.weight', 'transformer.h.14.attn.attention.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.14.attn.attention.masked_bias', 'transformer.h.25.attn.attention.v_proj.weight', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.3.attn.attention.v_proj.weight', 'transformer.h.8.ln_1.weight', 'transformer.h.7.ln_1.weight', 'transformer.h.8.attn.attention.bias', 'transformer.h.0.attn.attention.k_proj.weight', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.11.attn.attention.k_proj.weight', 'transformer.h.17.attn.attention.q_proj.weight', 'transformer.h.14.ln_1.bias', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.30.mlp.c_fc.weight', 'transformer.h.30.ln_2.bias', 'transformer.h.15.attn.attention.masked_bias', 'transformer.h.31.ln_1.weight', 'transformer.h.21.attn.attention.masked_bias', 'transformer.h.2.attn.attention.q_proj.weight', 'transformer.h.8.attn.attention.out_proj.weight', 'transformer.h.10.attn.attention.out_proj.bias', 'transformer.h.16.ln_2.bias', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.15.attn.attention.out_proj.bias', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.24.attn.attention.q_proj.weight', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.31.mlp.c_fc.bias', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.20.ln_1.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.24.attn.attention.k_proj.weight', 'transformer.h.13.ln_1.bias', 'transformer.h.15.attn.attention.q_proj.weight', 'transformer.h.18.attn.attention.masked_bias', 'transformer.h.6.attn.attention.v_proj.weight', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.0.attn.attention.q_proj.weight', 'transformer.h.17.ln_2.weight', 'transformer.h.26.attn.attention.k_proj.weight', 'transformer.h.6.attn.attention.out_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.31.mlp.c_fc.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.22.attn.attention.bias', 'transformer.h.20.attn.attention.out_proj.bias', 'transformer.h.16.attn.attention.k_proj.weight', 'transformer.h.30.attn.attention.bias', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.25.ln_2.weight', 'transformer.h.23.ln_1.weight', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.23.attn.attention.out_proj.weight', 'transformer.h.14.ln_2.weight', 'transformer.h.14.attn.attention.out_proj.weight', 'transformer.h.12.attn.attention.k_proj.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.15.attn.attention.out_proj.weight', 'transformer.h.17.ln_2.bias', 'transformer.h.29.attn.attention.q_proj.weight', 'transformer.h.9.attn.attention.out_proj.bias', 'transformer.h.27.ln_1.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.25.ln_1.weight', 'transformer.h.27.attn.attention.k_proj.weight', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.6.attn.attention.out_proj.weight', 'transformer.h.26.mlp.c_fc.weight', 'transformer.h.15.ln_2.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.28.attn.attention.bias', 'transformer.h.1.ln_1.bias', 'transformer.h.6.attn.attention.k_proj.weight', 'transformer.h.13.attn.attention.k_proj.weight', 'transformer.h.2.attn.attention.v_proj.weight', 'transformer.h.26.attn.attention.out_proj.weight', 'transformer.h.5.attn.attention.q_proj.weight', 'transformer.ln_f.weight', 'transformer.h.28.ln_2.weight', 'transformer.h.12.ln_1.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.11.attn.attention.out_proj.bias', 'transformer.h.28.ln_1.bias', 'transformer.h.6.attn.attention.q_proj.weight', 'transformer.h.0.attn.attention.masked_bias', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.27.ln_2.bias', 'transformer.h.10.attn.attention.k_proj.weight', 'transformer.h.29.mlp.c_fc.weight', 'transformer.h.27.attn.attention.bias', 'transformer.h.29.ln_2.weight', 'transformer.h.31.ln_2.weight', 'transformer.h.3.attn.attention.out_proj.bias', 'transformer.h.10.attn.attention.v_proj.weight', 'transformer.h.3.attn.attention.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.13.attn.attention.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.26.mlp.c_proj.weight', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.4.attn.attention.v_proj.weight', 'transformer.h.0.attn.attention.bias', 'transformer.h.9.attn.attention.masked_bias', 'transformer.h.20.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.21.attn.attention.out_proj.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.17.attn.attention.out_proj.weight', 'transformer.h.4.attn.attention.q_proj.weight', 'transformer.h.5.attn.attention.masked_bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.30.attn.attention.masked_bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.25.attn.attention.bias', 'transformer.h.8.ln_2.bias', 'transformer.h.28.ln_2.bias', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.26.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.8.ln_2.weight', 'transformer.h.26.attn.attention.v_proj.weight', 'transformer.h.7.attn.attention.masked_bias', 'transformer.h.18.attn.attention.v_proj.weight', 'transformer.h.24.attn.attention.out_proj.weight', 'transformer.h.15.ln_1.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.21.attn.attention.bias', 'transformer.h.15.ln_1.weight', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.17.ln_1.bias', 'transformer.h.11.attn.attention.q_proj.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.11.attn.attention.out_proj.weight', 'transformer.h.22.attn.attention.masked_bias', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.13.attn.attention.out_proj.bias', 'transformer.h.29.attn.attention.k_proj.weight', 'transformer.h.18.ln_2.weight', 'transformer.h.7.attn.attention.k_proj.weight', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.20.attn.attention.k_proj.weight', 'transformer.h.12.attn.attention.q_proj.weight', 'transformer.h.18.attn.attention.q_proj.weight', 'transformer.h.25.attn.attention.out_proj.weight', 'transformer.h.14.attn.attention.out_proj.bias', 'transformer.h.11.attn.attention.masked_bias', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.12.attn.attention.v_proj.weight', 'transformer.h.13.ln_2.bias', 'transformer.h.17.attn.attention.k_proj.weight', 'transformer.h.27.attn.attention.masked_bias', 'transformer.h.17.attn.attention.bias', 'transformer.h.24.ln_1.bias', 'transformer.h.9.attn.attention.v_proj.weight', 'transformer.h.31.attn.attention.bias', 'transformer.h.19.attn.attention.k_proj.weight', 'transformer.h.29.attn.attention.out_proj.bias', 'transformer.h.25.mlp.c_fc.bias', 'transformer.h.21.ln_1.bias', 'transformer.h.3.attn.attention.k_proj.weight', 'transformer.h.30.attn.attention.out_proj.weight', 'transformer.h.11.ln_2.weight', 'transformer.h.5.attn.attention.v_proj.weight', 'transformer.h.24.mlp.c_proj.weight', 'transformer.h.16.attn.attention.out_proj.weight', 'transformer.h.7.attn.attention.bias', 'transformer.h.26.ln_2.weight', 'transformer.h.12.ln_2.bias', 'transformer.h.20.ln_2.weight', 'transformer.h.30.ln_1.bias', 'transformer.h.30.mlp.c_fc.bias', 'transformer.h.9.attn.attention.out_proj.weight', 'transformer.h.2.ln_2.weight', 'transformer.h.10.attn.attention.bias', 'transformer.h.28.mlp.c_proj.weight', 'transformer.h.16.attn.attention.q_proj.weight', 'transformer.h.23.attn.attention.k_proj.weight', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.27.ln_2.weight', 'transformer.h.5.attn.attention.out_proj.weight', 'transformer.h.18.ln_1.bias', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.26.attn.attention.masked_bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.0.attn.attention.out_proj.bias', 'transformer.h.1.ln_2.bias', 'transformer.h.22.ln_1.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.14.attn.attention.q_proj.weight', 'transformer.h.23.attn.attention.v_proj.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.14.ln_2.bias', 'transformer.h.22.attn.attention.q_proj.weight', 'transformer.h.25.ln_1.bias', 'transformer.h.22.attn.attention.out_proj.bias', 'transformer.h.3.attn.attention.q_proj.weight', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.19.attn.attention.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.30.attn.attention.out_proj.bias', 'transformer.h.19.attn.attention.out_proj.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.4.ln_1.weight', 'transformer.h.29.attn.attention.v_proj.weight', 'transformer.h.29.ln_1.weight', 'transformer.h.24.mlp.c_fc.weight', 'transformer.h.30.attn.attention.k_proj.weight', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.12.attn.attention.masked_bias', 'transformer.h.27.mlp.c_fc.bias', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.26.attn.attention.bias', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.20.attn.attention.bias', 'transformer.wte.weight', 'transformer.h.20.attn.attention.q_proj.weight', 'transformer.h.13.ln_2.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.17.attn.attention.out_proj.bias', 'transformer.h.21.attn.attention.k_proj.weight', 'transformer.h.26.attn.attention.q_proj.weight', 'transformer.h.31.mlp.c_proj.weight', 'transformer.h.16.attn.attention.masked_bias', 'transformer.h.10.ln_1.weight', 'transformer.h.6.attn.attention.bias', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.29.attn.attention.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.7.ln_2.weight', 'transformer.h.25.mlp.c_fc.weight', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.28.attn.attention.masked_bias', 'transformer.h.9.ln_2.bias', 'transformer.h.13.attn.attention.v_proj.weight', 'transformer.h.22.attn.attention.out_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.7.attn.attention.q_proj.weight', 'transformer.h.6.attn.attention.masked_bias', 'transformer.ln_f.bias', 'transformer.h.10.ln_1.bias', 'transformer.h.1.attn.attention.masked_bias', 'transformer.h.14.attn.attention.v_proj.weight', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.28.mlp.c_fc.bias', 'transformer.h.16.attn.attention.out_proj.bias', 'transformer.h.2.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.16.ln_1.bias', 'transformer.h.17.attn.attention.v_proj.weight', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.16.ln_2.weight', 'transformer.h.15.attn.attention.v_proj.weight', 'transformer.h.31.attn.attention.out_proj.bias', 'transformer.h.4.attn.attention.bias', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.10.attn.attention.q_proj.weight', 'transformer.h.27.ln_1.bias', 'transformer.h.4.attn.attention.masked_bias', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.19.ln_1.weight', 'transformer.h.28.mlp.c_fc.weight', 'transformer.h.1.attn.attention.v_proj.weight', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.13.attn.attention.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2022-11-18 16:09:47,220][datasets.builder][WARNING] - Using custom data configuration ViktorThink--mountain_combined_813306-6918b9ea07482433
[2022-11-18 16:09:47,312][datasets.builder][WARNING] - Found cached dataset csv (/home/paperspace/.cache/huggingface/datasets/ViktorThink___csv/ViktorThink--mountain_combined_813306-6918b9ea07482433/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  5.95it/s]100%|██████████| 2/2 [00:00<00:00, 11.29it/s]
Running tokenizer on dataset:   0%|          | 0/814 [00:00<?, ?ba/s]Running tokenizer on dataset:   0%|          | 1/814 [00:02<35:05,  2.59s/ba]Running tokenizer on dataset:   0%|          | 2/814 [00:04<32:31,  2.40s/ba]Running tokenizer on dataset:   0%|          | 3/814 [00:07<31:10,  2.31s/ba]Running tokenizer on dataset:   0%|          | 4/814 [00:09<30:08,  2.23s/ba]Running tokenizer on dataset:   1%|          | 5/814 [00:11<29:27,  2.18s/ba]Running tokenizer on dataset:   1%|          | 6/814 [00:13<28:36,  2.12s/ba]Running tokenizer on dataset:   1%|          | 7/814 [00:15<28:15,  2.10s/ba]Running tokenizer on dataset:   1%|          | 8/814 [00:17<27:47,  2.07s/ba]Running tokenizer on dataset:   1%|          | 9/814 [00:19<27:19,  2.04s/ba]Running tokenizer on dataset:   1%|          | 10/814 [00:21<26:49,  2.00s/ba]Running tokenizer on dataset:   1%|▏         | 11/814 [00:23<26:50,  2.01s/ba]Running tokenizer on dataset:   1%|▏         | 12/814 [00:25<26:41,  2.00s/ba]Running tokenizer on dataset:   2%|▏         | 13/814 [00:27<26:32,  1.99s/ba]Running tokenizer on dataset:   2%|▏         | 14/814 [00:29<26:25,  1.98s/ba]Running tokenizer on dataset:   2%|▏         | 15/814 [00:31<26:10,  1.97s/ba]Running tokenizer on dataset:   2%|▏         | 16/814 [00:32<25:56,  1.95s/ba]Running tokenizer on dataset:   2%|▏         | 17/814 [00:34<25:43,  1.94s/ba]Running tokenizer on dataset:   2%|▏         | 18/814 [00:36<25:28,  1.92s/ba]Running tokenizer on dataset:   2%|▏         | 19/814 [00:38<25:22,  1.91s/ba]Running tokenizer on dataset:   2%|▏         | 20/814 [00:41<27:43,  2.10s/ba]Running tokenizer on dataset:   3%|▎         | 21/814 [00:43<29:31,  2.23s/ba]Running tokenizer on dataset:   3%|▎         | 22/814 [00:46<30:25,  2.31s/ba]Running tokenizer on dataset:   3%|▎         | 23/814 [00:48<31:08,  2.36s/ba]Running tokenizer on dataset:   3%|▎         | 24/814 [00:51<31:39,  2.40s/ba]Running tokenizer on dataset:   3%|▎         | 25/814 [00:53<32:08,  2.44s/ba]Running tokenizer on dataset:   3%|▎         | 26/814 [00:56<32:14,  2.45s/ba]Running tokenizer on dataset:   3%|▎         | 27/814 [00:58<32:09,  2.45s/ba]Running tokenizer on dataset:   3%|▎         | 28/814 [01:01<32:25,  2.48s/ba]Running tokenizer on dataset:   4%|▎         | 29/814 [01:03<32:44,  2.50s/ba]Running tokenizer on dataset:   4%|▎         | 30/814 [01:06<32:42,  2.50s/ba]Running tokenizer on dataset:   4%|▍         | 31/814 [01:08<32:30,  2.49s/ba]Running tokenizer on dataset:   4%|▍         | 32/814 [01:11<32:37,  2.50s/ba]Running tokenizer on dataset:   4%|▍         | 33/814 [01:13<30:05,  2.31s/ba]Running tokenizer on dataset:   4%|▍         | 34/814 [01:14<28:07,  2.16s/ba]Running tokenizer on dataset:   4%|▍         | 35/814 [01:16<26:44,  2.06s/ba]Running tokenizer on dataset:   4%|▍         | 36/814 [01:18<25:38,  1.98s/ba]Running tokenizer on dataset:   5%|▍         | 37/814 [01:20<24:56,  1.93s/ba]Running tokenizer on dataset:   5%|▍         | 38/814 [01:22<24:16,  1.88s/ba]Running tokenizer on dataset:   5%|▍         | 39/814 [01:24<24:18,  1.88s/ba]Running tokenizer on dataset:   5%|▍         | 40/814 [01:25<24:03,  1.86s/ba]Running tokenizer on dataset:   5%|▌         | 41/814 [01:27<23:57,  1.86s/ba]Running tokenizer on dataset:   5%|▌         | 42/814 [01:29<23:50,  1.85s/ba]Running tokenizer on dataset:   5%|▌         | 43/814 [01:31<23:42,  1.84s/ba]Running tokenizer on dataset:   5%|▌         | 44/814 [01:33<23:54,  1.86s/ba]Running tokenizer on dataset:   6%|▌         | 45/814 [01:35<23:48,  1.86s/ba]Running tokenizer on dataset:   6%|▌         | 46/814 [01:36<23:28,  1.83s/ba]Running tokenizer on dataset:   6%|▌         | 47/814 [01:38<23:19,  1.83s/ba]Running tokenizer on dataset:   6%|▌         | 48/814 [01:40<23:03,  1.81s/ba]Running tokenizer on dataset:   6%|▌         | 49/814 [01:42<23:04,  1.81s/ba]Running tokenizer on dataset:   6%|▌         | 50/814 [01:44<22:59,  1.81s/ba]Running tokenizer on dataset:   6%|▋         | 51/814 [01:45<22:47,  1.79s/ba]Running tokenizer on dataset:   6%|▋         | 52/814 [01:47<22:54,  1.80s/ba]Running tokenizer on dataset:   7%|▋         | 53/814 [01:49<22:50,  1.80s/ba]Running tokenizer on dataset:   7%|▋         | 54/814 [01:51<22:47,  1.80s/ba]Running tokenizer on dataset:   7%|▋         | 55/814 [01:53<23:02,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 56/814 [01:54<22:58,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 57/814 [01:56<23:14,  1.84s/ba]Running tokenizer on dataset:   7%|▋         | 58/814 [01:58<22:57,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 59/814 [02:00<22:50,  1.82s/ba]Running tokenizer on dataset:   7%|▋         | 60/814 [02:02<22:39,  1.80s/ba]Running tokenizer on dataset:   7%|▋         | 61/814 [02:03<22:36,  1.80s/ba]Running tokenizer on dataset:   8%|▊         | 62/814 [02:05<22:39,  1.81s/ba]Running tokenizer on dataset:   8%|▊         | 63/814 [02:07<22:30,  1.80s/ba]Running tokenizer on dataset:   8%|▊         | 64/814 [02:09<22:22,  1.79s/ba]Running tokenizer on dataset:   8%|▊         | 65/814 [02:11<22:32,  1.81s/ba]Running tokenizer on dataset:   8%|▊         | 66/814 [02:13<22:31,  1.81s/ba]Running tokenizer on dataset:   8%|▊         | 67/814 [02:14<22:20,  1.79s/ba]Running tokenizer on dataset:   8%|▊         | 68/814 [02:16<22:12,  1.79s/ba]Running tokenizer on dataset:   8%|▊         | 69/814 [02:18<22:04,  1.78s/ba]Running tokenizer on dataset:   9%|▊         | 70/814 [02:20<22:00,  1.77s/ba]Running tokenizer on dataset:   9%|▊         | 71/814 [02:21<21:55,  1.77s/ba]Running tokenizer on dataset:   9%|▉         | 72/814 [02:24<24:01,  1.94s/ba]Running tokenizer on dataset:   9%|▉         | 73/814 [02:25<23:23,  1.89s/ba]Running tokenizer on dataset:   9%|▉         | 74/814 [02:27<22:51,  1.85s/ba]Running tokenizer on dataset:   9%|▉         | 75/814 [02:29<22:32,  1.83s/ba]Running tokenizer on dataset:   9%|▉         | 76/814 [02:31<23:05,  1.88s/ba]Running tokenizer on dataset:   9%|▉         | 77/814 [02:33<22:40,  1.85s/ba]Running tokenizer on dataset:  10%|▉         | 78/814 [02:35<22:28,  1.83s/ba]Running tokenizer on dataset:  10%|▉         | 79/814 [02:36<22:11,  1.81s/ba]Running tokenizer on dataset:  10%|▉         | 80/814 [02:38<22:04,  1.80s/ba]Running tokenizer on dataset:  10%|▉         | 81/814 [02:40<22:04,  1.81s/ba]Running tokenizer on dataset:  10%|█         | 82/814 [02:42<22:03,  1.81s/ba]Running tokenizer on dataset:  10%|█         | 83/814 [02:44<21:57,  1.80s/ba]Running tokenizer on dataset:  10%|█         | 84/814 [02:45<21:51,  1.80s/ba]Running tokenizer on dataset:  10%|█         | 85/814 [02:47<21:43,  1.79s/ba]Running tokenizer on dataset:  11%|█         | 86/814 [02:49<21:32,  1.78s/ba]Running tokenizer on dataset:  11%|█         | 87/814 [02:51<21:32,  1.78s/ba]Running tokenizer on dataset:  11%|█         | 88/814 [02:52<21:29,  1.78s/ba]Running tokenizer on dataset:  11%|█         | 89/814 [02:54<21:33,  1.78s/ba]Running tokenizer on dataset:  11%|█         | 90/814 [02:56<21:29,  1.78s/ba]Running tokenizer on dataset:  11%|█         | 91/814 [02:58<21:26,  1.78s/ba]Running tokenizer on dataset:  11%|█▏        | 92/814 [02:59<21:12,  1.76s/ba]Running tokenizer on dataset:  11%|█▏        | 93/814 [03:01<21:17,  1.77s/ba]Running tokenizer on dataset:  12%|█▏        | 94/814 [03:03<21:10,  1.76s/ba]Running tokenizer on dataset:  12%|█▏        | 95/814 [03:05<21:00,  1.75s/ba]Running tokenizer on dataset:  12%|█▏        | 96/814 [03:07<21:09,  1.77s/ba]Running tokenizer on dataset:  12%|█▏        | 97/814 [03:08<21:09,  1.77s/ba]Running tokenizer on dataset:  12%|█▏        | 98/814 [03:10<21:07,  1.77s/ba]Running tokenizer on dataset:  12%|█▏        | 99/814 [03:12<21:10,  1.78s/ba]Running tokenizer on dataset:  12%|█▏        | 100/814 [03:14<21:02,  1.77s/ba]Running tokenizer on dataset:  12%|█▏        | 101/814 [03:15<20:56,  1.76s/ba]Running tokenizer on dataset:  13%|█▎        | 102/814 [03:17<20:52,  1.76s/ba]Running tokenizer on dataset:  13%|█▎        | 103/814 [03:19<20:56,  1.77s/ba]Running tokenizer on dataset:  13%|█▎        | 104/814 [03:21<20:48,  1.76s/ba]Running tokenizer on dataset:  13%|█▎        | 105/814 [03:22<20:45,  1.76s/ba]Running tokenizer on dataset:  13%|█▎        | 106/814 [03:24<20:46,  1.76s/ba]Running tokenizer on dataset:  13%|█▎        | 107/814 [03:26<20:48,  1.77s/ba]Running tokenizer on dataset:  13%|█▎        | 108/814 [03:28<20:51,  1.77s/ba]Running tokenizer on dataset:  13%|█▎        | 109/814 [03:29<20:50,  1.77s/ba]Running tokenizer on dataset:  14%|█▎        | 110/814 [03:31<20:41,  1.76s/ba]Running tokenizer on dataset:  14%|█▎        | 111/814 [03:33<20:38,  1.76s/ba]Running tokenizer on dataset:  14%|█▍        | 112/814 [03:35<20:35,  1.76s/ba]Running tokenizer on dataset:  14%|█▍        | 113/814 [03:36<20:31,  1.76s/ba]Running tokenizer on dataset:  14%|█▍        | 114/814 [03:38<20:34,  1.76s/ba]Running tokenizer on dataset:  14%|█▍        | 115/814 [03:40<20:39,  1.77s/ba]Running tokenizer on dataset:  14%|█▍        | 116/814 [03:42<20:33,  1.77s/ba]Running tokenizer on dataset:  14%|█▍        | 117/814 [03:44<20:31,  1.77s/ba]Running tokenizer on dataset:  14%|█▍        | 118/814 [03:45<20:39,  1.78s/ba]Running tokenizer on dataset:  15%|█▍        | 119/814 [03:47<20:18,  1.75s/ba]Running tokenizer on dataset:  15%|█▍        | 120/814 [03:49<20:08,  1.74s/ba]Running tokenizer on dataset:  15%|█▍        | 121/814 [03:51<20:22,  1.76s/ba]Running tokenizer on dataset:  15%|█▍        | 122/814 [03:52<20:16,  1.76s/ba]Running tokenizer on dataset:  15%|█▌        | 123/814 [03:54<20:09,  1.75s/ba]Running tokenizer on dataset:  15%|█▌        | 124/814 [03:56<20:08,  1.75s/ba]Running tokenizer on dataset:  15%|█▌        | 125/814 [03:58<20:06,  1.75s/ba]Running tokenizer on dataset:  15%|█▌        | 126/814 [03:59<20:11,  1.76s/ba]Running tokenizer on dataset:  16%|█▌        | 127/814 [04:01<20:11,  1.76s/ba]Running tokenizer on dataset:  16%|█▌        | 128/814 [04:03<19:56,  1.74s/ba]Running tokenizer on dataset:  16%|█▌        | 129/814 [04:05<20:03,  1.76s/ba]Running tokenizer on dataset:  16%|█▌        | 130/814 [04:06<19:48,  1.74s/ba]Running tokenizer on dataset:  16%|█▌        | 131/814 [04:08<19:51,  1.74s/ba]Running tokenizer on dataset:  16%|█▌        | 132/814 [04:10<19:41,  1.73s/ba]Running tokenizer on dataset:  16%|█▋        | 133/814 [04:12<19:43,  1.74s/ba]Running tokenizer on dataset:  16%|█▋        | 134/814 [04:13<19:35,  1.73s/ba]Running tokenizer on dataset:  17%|█▋        | 135/814 [04:15<19:34,  1.73s/ba]Running tokenizer on dataset:  17%|█▋        | 136/814 [04:17<19:32,  1.73s/ba]Running tokenizer on dataset:  17%|█▋        | 137/814 [04:18<19:33,  1.73s/ba]Running tokenizer on dataset:  17%|█▋        | 138/814 [04:20<19:45,  1.75s/ba]Running tokenizer on dataset:  17%|█▋        | 139/814 [04:22<19:36,  1.74s/ba]Running tokenizer on dataset:  17%|█▋        | 140/814 [04:24<19:28,  1.73s/ba]Running tokenizer on dataset:  17%|█▋        | 141/814 [04:25<19:24,  1.73s/ba]Running tokenizer on dataset:  17%|█▋        | 142/814 [04:27<19:32,  1.75s/ba]Running tokenizer on dataset:  18%|█▊        | 143/814 [04:29<19:26,  1.74s/ba]Running tokenizer on dataset:  18%|█▊        | 144/814 [04:31<19:25,  1.74s/ba]Running tokenizer on dataset:  18%|█▊        | 145/814 [04:32<19:22,  1.74s/ba]Running tokenizer on dataset:  18%|█▊        | 146/814 [04:34<19:10,  1.72s/ba]Running tokenizer on dataset:  18%|█▊        | 147/814 [04:36<19:21,  1.74s/ba]Running tokenizer on dataset:  18%|█▊        | 148/814 [04:38<19:21,  1.74s/ba]Running tokenizer on dataset:  18%|█▊        | 149/814 [04:39<19:04,  1.72s/ba]Running tokenizer on dataset:  18%|█▊        | 150/814 [04:41<18:59,  1.72s/ba]Running tokenizer on dataset:  19%|█▊        | 151/814 [04:43<18:52,  1.71s/ba]Running tokenizer on dataset:  19%|█▊        | 152/814 [04:44<18:49,  1.71s/ba]Running tokenizer on dataset:  19%|█▉        | 153/814 [04:46<18:59,  1.72s/ba]Running tokenizer on dataset:  19%|█▉        | 154/814 [04:48<19:01,  1.73s/ba]Running tokenizer on dataset:  19%|█▉        | 155/814 [04:50<18:56,  1.72s/ba]Running tokenizer on dataset:  19%|█▉        | 156/814 [04:51<18:58,  1.73s/ba]Running tokenizer on dataset:  19%|█▉        | 157/814 [04:53<19:31,  1.78s/ba]Running tokenizer on dataset:  19%|█▉        | 158/814 [04:55<19:14,  1.76s/ba]Running tokenizer on dataset:  20%|█▉        | 159/814 [04:57<19:04,  1.75s/ba]Running tokenizer on dataset:  20%|█▉        | 160/814 [04:58<18:58,  1.74s/ba]Running tokenizer on dataset:  20%|█▉        | 161/814 [05:00<18:49,  1.73s/ba]Running tokenizer on dataset:  20%|█▉        | 162/814 [05:02<18:49,  1.73s/ba]Running tokenizer on dataset:  20%|██        | 163/814 [05:04<18:45,  1.73s/ba]Running tokenizer on dataset:  20%|██        | 164/814 [05:05<18:51,  1.74s/ba]Running tokenizer on dataset:  20%|██        | 165/814 [05:07<18:56,  1.75s/ba]Running tokenizer on dataset:  20%|██        | 166/814 [05:09<18:56,  1.75s/ba]Running tokenizer on dataset:  21%|██        | 167/814 [05:11<18:46,  1.74s/ba]Running tokenizer on dataset:  21%|██        | 168/814 [05:12<18:36,  1.73s/ba]Running tokenizer on dataset:  21%|██        | 169/814 [05:14<18:33,  1.73s/ba]Running tokenizer on dataset:  21%|██        | 170/814 [05:16<18:28,  1.72s/ba]Running tokenizer on dataset:  21%|██        | 171/814 [05:17<18:27,  1.72s/ba]Running tokenizer on dataset:  21%|██        | 172/814 [05:19<18:43,  1.75s/ba]Running tokenizer on dataset:  21%|██▏       | 173/814 [05:21<18:56,  1.77s/ba]Running tokenizer on dataset:  21%|██▏       | 174/814 [05:23<18:45,  1.76s/ba]Running tokenizer on dataset:  21%|██▏       | 175/814 [05:24<18:34,  1.74s/ba]Running tokenizer on dataset:  22%|██▏       | 176/814 [05:26<18:29,  1.74s/ba]Running tokenizer on dataset:  22%|██▏       | 177/814 [05:28<18:29,  1.74s/ba]Running tokenizer on dataset:  22%|██▏       | 178/814 [05:30<18:36,  1.75s/ba]Running tokenizer on dataset:  22%|██▏       | 179/814 [05:32<18:34,  1.76s/ba]Running tokenizer on dataset:  22%|██▏       | 180/814 [05:33<18:38,  1.76s/ba]Running tokenizer on dataset:  22%|██▏       | 181/814 [05:35<18:33,  1.76s/ba]Running tokenizer on dataset:  22%|██▏       | 182/814 [05:37<18:32,  1.76s/ba]Running tokenizer on dataset:  22%|██▏       | 183/814 [05:39<18:32,  1.76s/ba]Running tokenizer on dataset:  23%|██▎       | 184/814 [05:40<18:23,  1.75s/ba]Running tokenizer on dataset:  23%|██▎       | 185/814 [05:42<18:19,  1.75s/ba]Running tokenizer on dataset:  23%|██▎       | 186/814 [05:44<18:17,  1.75s/ba]Running tokenizer on dataset:  23%|██▎       | 187/814 [05:46<18:10,  1.74s/ba]Running tokenizer on dataset:  23%|██▎       | 188/814 [05:47<18:05,  1.73s/ba]Running tokenizer on dataset:  23%|██▎       | 189/814 [05:49<18:02,  1.73s/ba]Running tokenizer on dataset:  23%|██▎       | 190/814 [05:51<18:01,  1.73s/ba]Running tokenizer on dataset:  23%|██▎       | 191/814 [05:53<18:12,  1.75s/ba]Running tokenizer on dataset:  24%|██▎       | 192/814 [05:54<18:08,  1.75s/ba]Running tokenizer on dataset:  24%|██▎       | 193/814 [05:56<18:10,  1.76s/ba]Running tokenizer on dataset:  24%|██▍       | 194/814 [05:58<18:04,  1.75s/ba]Running tokenizer on dataset:  24%|██▍       | 195/814 [06:00<18:05,  1.75s/ba]Running tokenizer on dataset:  24%|██▍       | 196/814 [06:01<18:01,  1.75s/ba]Running tokenizer on dataset:  24%|██▍       | 197/814 [06:03<17:58,  1.75s/ba]Running tokenizer on dataset:  24%|██▍       | 198/814 [06:05<18:04,  1.76s/ba]Running tokenizer on dataset:  24%|██▍       | 199/814 [06:07<17:55,  1.75s/ba]Running tokenizer on dataset:  25%|██▍       | 200/814 [06:08<17:47,  1.74s/ba]Running tokenizer on dataset:  25%|██▍       | 201/814 [06:10<17:44,  1.74s/ba]Running tokenizer on dataset:  25%|██▍       | 202/814 [06:12<17:45,  1.74s/ba]Running tokenizer on dataset:  25%|██▍       | 203/814 [06:13<17:43,  1.74s/ba]Running tokenizer on dataset:  25%|██▌       | 204/814 [06:15<17:44,  1.75s/ba]Running tokenizer on dataset:  25%|██▌       | 205/814 [06:17<17:36,  1.74s/ba]Running tokenizer on dataset:  25%|██▌       | 206/814 [06:19<17:38,  1.74s/ba]Running tokenizer on dataset:  25%|██▌       | 207/814 [06:20<17:41,  1.75s/ba]Running tokenizer on dataset:  26%|██▌       | 208/814 [06:22<17:37,  1.74s/ba]Running tokenizer on dataset:  26%|██▌       | 209/814 [06:24<17:35,  1.74s/ba]Running tokenizer on dataset:  26%|██▌       | 210/814 [06:26<17:35,  1.75s/ba]Running tokenizer on dataset:  26%|██▌       | 211/814 [06:27<17:33,  1.75s/ba]Running tokenizer on dataset:  26%|██▌       | 211/814 [06:28<18:31,  1.84s/ba]
Error in sys.excepthook:
Traceback (most recent call last):
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1694, in print
    extend(render(renderable, render_options))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/constrain.py", line 29, in __rich_console__
    yield from console.render(self.renderable, child_options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/panel.py", line 220, in __rich_console__
    lines = console.render_lines(renderable, child_options, style=style)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1330, in render
    yield from self.render(render_output, _options)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 511, in __rich_console__
    yield from self._render(console, render_options, widths)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/table.py", line 824, in _render
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/padding.py", line 97, in __rich_console__
    lines = console.render_lines(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1366, in render_lines
    lines = list(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 292, in split_and_crop_lines
    for segment in segments:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/console.py", line 1326, in render
    for render_output in iter_render:
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/syntax.py", line 609, in __rich_console__
    segments = Segments(self._get_syntax(console, options))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/segment.py", line 668, in __init__
    self.segments = list(segments)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/syntax.py", line 637, in _get_syntax
    text = self.highlight(processed_code, self.line_range)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/syntax.py", line 509, in highlight
    text.append_tokens(tokens_to_spans())
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/rich/text.py", line 999, in append_tokens
    offset += len(content)
KeyboardInterrupt

Original exception was:
Traceback (most recent call last):
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2982, in _map_single
    batch = apply_function_on_filtered_inputs(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2865, in apply_function_on_filtered_inputs
    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2545, in decorated
    result = f(decorated_item, *args, **kwargs)
  File "finetune_using_clm_wandb.py", line 204, in tokenize_fn
    result = tokenizer(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2488, in __call__
    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2574, in _call_one
    return self.batch_encode_plus(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils_base.py", line 2765, in batch_encode_plus
    return self._batch_encode_plus(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 733, in _batch_encode_plus
    first_ids = get_input_ids(ids)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 700, in get_input_ids
    tokens = self.tokenize(text, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/tokenization_utils.py", line 547, in tokenize
    tokenized_text.extend(self._tokenize(token))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/transformers/models/gpt2/tokenization_gpt2.py", line 299, in _tokenize
    for token in re.findall(self.pat, text):
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/regex/regex.py", line 338, in findall
    return pat.findall(string, pos, endpos, overlapped, concurrent, timeout)
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "finetune_using_clm_wandb.py", line 599, in <module>
    main()
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/main.py", line 90, in decorated_main
    _run_hydra(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 389, in _run_hydra
    _run_app(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 452, in _run_app
    run_and_report(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 213, in run_and_report
    return func()
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/utils.py", line 453, in <lambda>
    lambda: hydra.run(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/_internal/hydra.py", line 119, in run
    ret = run_job(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
  File "finetune_using_clm_wandb.py", line 304, in main
    tokenized_datasets = preprocess(cfg, accelerator, tokenizer, raw_datasets)
  File "finetune_using_clm_wandb.py", line 215, in preprocess
    tokenized_datasets = raw_datasets.map(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/dataset_dict.py", line 777, in map
    {
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/dataset_dict.py", line 778, in <dictcomp>
    k: dataset.map(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 2585, in map
    return self._map_single(
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 585, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 552, in wrapper
    out: Union["Dataset", "DatasetDict"] = func(self, *args, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/fingerprint.py", line 480, in wrapper
    out = func(self, *args, **kwargs)
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/arrow_dataset.py", line 3009, in _map_single
    os.remove(tmp_file.name)
KeyboardInterrupt
[2022-11-18 16:16:26,906][__main__][INFO] - Setting random seed to 17
[2022-11-18 16:16:26,907][__main__][INFO] - Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

[2022-11-18 16:16:26,909][__main__][INFO] - output_dir: tuned-model
bittensor:
  network: nobunaga
dataset:
  name: ViktorThink/mountain_combined_813306
  config_name: null
  num_batches: 2
  block_size: 256
  overwrite_cache: false
  keep_linebreaks: true
  concatenate_raw: false
  load_tokenized_data: false
model:
  name: facebook/opt-2.7b
  config_name: null
tokenizer:
  name: null
  use_fast: true
  preprocessing_num_workers: 12
  pad_token: '[PAD]'
training:
  seed: 17
  val_split_percent: 20
  train_batch_size: 32
  eval_batch_size: 16
  learning_rate: 3.0e-06
  weight_decay: 0.05
  num_epochs: 4
  max_train_steps: null
  gradient_accumulation_steps: 2
  lr_scheduler: constant
  lr_warmup_steps: 5
  eval_every: 250
  max_eval_steps: 500
  checkpoint:
    resume_from_checkpoint: 0
    every_n_steps: null
tracking:
  enabled: true
  report_to: all
testing:
  enabled: false

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--EleutherAI--gpt-neo-2.7B/snapshots/51568a6e0ae813a3f2a9da558ab7beac5e3acc24/config.json
Model config GPTNeoConfig {
  "_name_or_path": "EleutherAI/gpt-neo-2.7B",
  "activation_function": "gelu_new",
  "architectures": [
    "GPTNeoForCausalLM"
  ],
  "attention_dropout": 0,
  "attention_layers": [
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local",
    "global",
    "local"
  ],
  "attention_types": [
    [
      [
        "global",
        "local"
      ],
      16
    ]
  ],
  "bos_token_id": 50256,
  "embed_dropout": 0,
  "eos_token_id": 50256,
  "gradient_checkpointing": false,
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": null,
  "layer_norm_epsilon": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "gpt_neo",
  "num_heads": 20,
  "num_layers": 32,
  "resid_dropout": 0,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50,
      "temperature": 0.9
    }
  },
  "tokenizer_class": "GPT2Tokenizer",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50257,
  "window_size": 256
}

loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading file vocab.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/vocab.json
loading file merges.txt from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/merges.txt
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/special_tokens_map.json
loading file tokenizer_config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/tokenizer_config.json
loading configuration file config.json from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/config.json
Model config OPTConfig {
  "_name_or_path": "facebook/opt-2.7b",
  "_remove_final_layer_norm": false,
  "activation_dropout": 0.0,
  "activation_function": "relu",
  "architectures": [
    "OPTForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 2,
  "do_layer_norm_before": true,
  "dropout": 0.1,
  "eos_token_id": 2,
  "ffn_dim": 10240,
  "hidden_size": 2560,
  "init_std": 0.02,
  "layerdrop": 0.0,
  "max_position_embeddings": 2048,
  "model_type": "opt",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "pad_token_id": 1,
  "prefix": "</s>",
  "torch_dtype": "float16",
  "transformers_version": "4.24.0",
  "use_cache": true,
  "vocab_size": 50272,
  "word_embed_proj_dim": 2560
}

loading weights file flax_model.msgpack from cache at /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Loading Flax weights from /home/paperspace/.cache/huggingface/hub/models--facebook--opt-2.7b/snapshots/c9c15109b9dac40871c063892227d45b85cb3952/flax_model.msgpack
Some weights of the Flax model were not used when initializing the PyTorch model GPTNeoForCausalLM: ['model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'model.decoder.final_layer_norm.weight', 'model.decoder.final_layer_norm.bias', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.1.fc1.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.decoder.layers.1.fc2.bias', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.3.fc2.bias', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.4.fc2.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.6.self_attn.k_proj.weight', 'model.decoder.layers.6.self_attn.k_proj.bias', 'model.decoder.layers.6.self_attn.v_proj.weight', 'model.decoder.layers.6.self_attn.v_proj.bias', 'model.decoder.layers.6.self_attn.q_proj.weight', 'model.decoder.layers.6.self_attn.q_proj.bias', 'model.decoder.layers.6.self_attn.out_proj.weight', 'model.decoder.layers.6.self_attn.out_proj.bias', 'model.decoder.layers.6.self_attn_layer_norm.weight', 'model.decoder.layers.6.self_attn_layer_norm.bias', 'model.decoder.layers.6.fc1.weight', 'model.decoder.layers.6.fc1.bias', 'model.decoder.layers.6.fc2.weight', 'model.decoder.layers.6.fc2.bias', 'model.decoder.layers.6.final_layer_norm.weight', 'model.decoder.layers.6.final_layer_norm.bias', 'model.decoder.layers.7.self_attn.k_proj.weight', 'model.decoder.layers.7.self_attn.k_proj.bias', 'model.decoder.layers.7.self_attn.v_proj.weight', 'model.decoder.layers.7.self_attn.v_proj.bias', 'model.decoder.layers.7.self_attn.q_proj.weight', 'model.decoder.layers.7.self_attn.q_proj.bias', 'model.decoder.layers.7.self_attn.out_proj.weight', 'model.decoder.layers.7.self_attn.out_proj.bias', 'model.decoder.layers.7.self_attn_layer_norm.weight', 'model.decoder.layers.7.self_attn_layer_norm.bias', 'model.decoder.layers.7.fc1.weight', 'model.decoder.layers.7.fc1.bias', 'model.decoder.layers.7.fc2.weight', 'model.decoder.layers.7.fc2.bias', 'model.decoder.layers.7.final_layer_norm.weight', 'model.decoder.layers.7.final_layer_norm.bias', 'model.decoder.layers.8.self_attn.k_proj.weight', 'model.decoder.layers.8.self_attn.k_proj.bias', 'model.decoder.layers.8.self_attn.v_proj.weight', 'model.decoder.layers.8.self_attn.v_proj.bias', 'model.decoder.layers.8.self_attn.q_proj.weight', 'model.decoder.layers.8.self_attn.q_proj.bias', 'model.decoder.layers.8.self_attn.out_proj.weight', 'model.decoder.layers.8.self_attn.out_proj.bias', 'model.decoder.layers.8.self_attn_layer_norm.weight', 'model.decoder.layers.8.self_attn_layer_norm.bias', 'model.decoder.layers.8.fc1.weight', 'model.decoder.layers.8.fc1.bias', 'model.decoder.layers.8.fc2.weight', 'model.decoder.layers.8.fc2.bias', 'model.decoder.layers.8.final_layer_norm.weight', 'model.decoder.layers.8.final_layer_norm.bias', 'model.decoder.layers.9.self_attn.k_proj.weight', 'model.decoder.layers.9.self_attn.k_proj.bias', 'model.decoder.layers.9.self_attn.v_proj.weight', 'model.decoder.layers.9.self_attn.v_proj.bias', 'model.decoder.layers.9.self_attn.q_proj.weight', 'model.decoder.layers.9.self_attn.q_proj.bias', 'model.decoder.layers.9.self_attn.out_proj.weight', 'model.decoder.layers.9.self_attn.out_proj.bias', 'model.decoder.layers.9.self_attn_layer_norm.weight', 'model.decoder.layers.9.self_attn_layer_norm.bias', 'model.decoder.layers.9.fc1.weight', 'model.decoder.layers.9.fc1.bias', 'model.decoder.layers.9.fc2.weight', 'model.decoder.layers.9.fc2.bias', 'model.decoder.layers.9.final_layer_norm.weight', 'model.decoder.layers.9.final_layer_norm.bias', 'model.decoder.layers.10.self_attn.k_proj.weight', 'model.decoder.layers.10.self_attn.k_proj.bias', 'model.decoder.layers.10.self_attn.v_proj.weight', 'model.decoder.layers.10.self_attn.v_proj.bias', 'model.decoder.layers.10.self_attn.q_proj.weight', 'model.decoder.layers.10.self_attn.q_proj.bias', 'model.decoder.layers.10.self_attn.out_proj.weight', 'model.decoder.layers.10.self_attn.out_proj.bias', 'model.decoder.layers.10.self_attn_layer_norm.weight', 'model.decoder.layers.10.self_attn_layer_norm.bias', 'model.decoder.layers.10.fc1.weight', 'model.decoder.layers.10.fc1.bias', 'model.decoder.layers.10.fc2.weight', 'model.decoder.layers.10.fc2.bias', 'model.decoder.layers.10.final_layer_norm.weight', 'model.decoder.layers.10.final_layer_norm.bias', 'model.decoder.layers.11.self_attn.k_proj.weight', 'model.decoder.layers.11.self_attn.k_proj.bias', 'model.decoder.layers.11.self_attn.v_proj.weight', 'model.decoder.layers.11.self_attn.v_proj.bias', 'model.decoder.layers.11.self_attn.q_proj.weight', 'model.decoder.layers.11.self_attn.q_proj.bias', 'model.decoder.layers.11.self_attn.out_proj.weight', 'model.decoder.layers.11.self_attn.out_proj.bias', 'model.decoder.layers.11.self_attn_layer_norm.weight', 'model.decoder.layers.11.self_attn_layer_norm.bias', 'model.decoder.layers.11.fc1.weight', 'model.decoder.layers.11.fc1.bias', 'model.decoder.layers.11.fc2.weight', 'model.decoder.layers.11.fc2.bias', 'model.decoder.layers.11.final_layer_norm.weight', 'model.decoder.layers.11.final_layer_norm.bias', 'model.decoder.layers.12.self_attn.k_proj.weight', 'model.decoder.layers.12.self_attn.k_proj.bias', 'model.decoder.layers.12.self_attn.v_proj.weight', 'model.decoder.layers.12.self_attn.v_proj.bias', 'model.decoder.layers.12.self_attn.q_proj.weight', 'model.decoder.layers.12.self_attn.q_proj.bias', 'model.decoder.layers.12.self_attn.out_proj.weight', 'model.decoder.layers.12.self_attn.out_proj.bias', 'model.decoder.layers.12.self_attn_layer_norm.weight', 'model.decoder.layers.12.self_attn_layer_norm.bias', 'model.decoder.layers.12.fc1.weight', 'model.decoder.layers.12.fc1.bias', 'model.decoder.layers.12.fc2.weight', 'model.decoder.layers.12.fc2.bias', 'model.decoder.layers.12.final_layer_norm.weight', 'model.decoder.layers.12.final_layer_norm.bias', 'model.decoder.layers.13.self_attn.k_proj.weight', 'model.decoder.layers.13.self_attn.k_proj.bias', 'model.decoder.layers.13.self_attn.v_proj.weight', 'model.decoder.layers.13.self_attn.v_proj.bias', 'model.decoder.layers.13.self_attn.q_proj.weight', 'model.decoder.layers.13.self_attn.q_proj.bias', 'model.decoder.layers.13.self_attn.out_proj.weight', 'model.decoder.layers.13.self_attn.out_proj.bias', 'model.decoder.layers.13.self_attn_layer_norm.weight', 'model.decoder.layers.13.self_attn_layer_norm.bias', 'model.decoder.layers.13.fc1.weight', 'model.decoder.layers.13.fc1.bias', 'model.decoder.layers.13.fc2.weight', 'model.decoder.layers.13.fc2.bias', 'model.decoder.layers.13.final_layer_norm.weight', 'model.decoder.layers.13.final_layer_norm.bias', 'model.decoder.layers.14.self_attn.k_proj.weight', 'model.decoder.layers.14.self_attn.k_proj.bias', 'model.decoder.layers.14.self_attn.v_proj.weight', 'model.decoder.layers.14.self_attn.v_proj.bias', 'model.decoder.layers.14.self_attn.q_proj.weight', 'model.decoder.layers.14.self_attn.q_proj.bias', 'model.decoder.layers.14.self_attn.out_proj.weight', 'model.decoder.layers.14.self_attn.out_proj.bias', 'model.decoder.layers.14.self_attn_layer_norm.weight', 'model.decoder.layers.14.self_attn_layer_norm.bias', 'model.decoder.layers.14.fc1.weight', 'model.decoder.layers.14.fc1.bias', 'model.decoder.layers.14.fc2.weight', 'model.decoder.layers.14.fc2.bias', 'model.decoder.layers.14.final_layer_norm.weight', 'model.decoder.layers.14.final_layer_norm.bias', 'model.decoder.layers.15.self_attn.k_proj.weight', 'model.decoder.layers.15.self_attn.k_proj.bias', 'model.decoder.layers.15.self_attn.v_proj.weight', 'model.decoder.layers.15.self_attn.v_proj.bias', 'model.decoder.layers.15.self_attn.q_proj.weight', 'model.decoder.layers.15.self_attn.q_proj.bias', 'model.decoder.layers.15.self_attn.out_proj.weight', 'model.decoder.layers.15.self_attn.out_proj.bias', 'model.decoder.layers.15.self_attn_layer_norm.weight', 'model.decoder.layers.15.self_attn_layer_norm.bias', 'model.decoder.layers.15.fc1.weight', 'model.decoder.layers.15.fc1.bias', 'model.decoder.layers.15.fc2.weight', 'model.decoder.layers.15.fc2.bias', 'model.decoder.layers.15.final_layer_norm.weight', 'model.decoder.layers.15.final_layer_norm.bias', 'model.decoder.layers.16.self_attn.k_proj.weight', 'model.decoder.layers.16.self_attn.k_proj.bias', 'model.decoder.layers.16.self_attn.v_proj.weight', 'model.decoder.layers.16.self_attn.v_proj.bias', 'model.decoder.layers.16.self_attn.q_proj.weight', 'model.decoder.layers.16.self_attn.q_proj.bias', 'model.decoder.layers.16.self_attn.out_proj.weight', 'model.decoder.layers.16.self_attn.out_proj.bias', 'model.decoder.layers.16.self_attn_layer_norm.weight', 'model.decoder.layers.16.self_attn_layer_norm.bias', 'model.decoder.layers.16.fc1.weight', 'model.decoder.layers.16.fc1.bias', 'model.decoder.layers.16.fc2.weight', 'model.decoder.layers.16.fc2.bias', 'model.decoder.layers.16.final_layer_norm.weight', 'model.decoder.layers.16.final_layer_norm.bias', 'model.decoder.layers.17.self_attn.k_proj.weight', 'model.decoder.layers.17.self_attn.k_proj.bias', 'model.decoder.layers.17.self_attn.v_proj.weight', 'model.decoder.layers.17.self_attn.v_proj.bias', 'model.decoder.layers.17.self_attn.q_proj.weight', 'model.decoder.layers.17.self_attn.q_proj.bias', 'model.decoder.layers.17.self_attn.out_proj.weight', 'model.decoder.layers.17.self_attn.out_proj.bias', 'model.decoder.layers.17.self_attn_layer_norm.weight', 'model.decoder.layers.17.self_attn_layer_norm.bias', 'model.decoder.layers.17.fc1.weight', 'model.decoder.layers.17.fc1.bias', 'model.decoder.layers.17.fc2.weight', 'model.decoder.layers.17.fc2.bias', 'model.decoder.layers.17.final_layer_norm.weight', 'model.decoder.layers.17.final_layer_norm.bias', 'model.decoder.layers.18.self_attn.k_proj.weight', 'model.decoder.layers.18.self_attn.k_proj.bias', 'model.decoder.layers.18.self_attn.v_proj.weight', 'model.decoder.layers.18.self_attn.v_proj.bias', 'model.decoder.layers.18.self_attn.q_proj.weight', 'model.decoder.layers.18.self_attn.q_proj.bias', 'model.decoder.layers.18.self_attn.out_proj.weight', 'model.decoder.layers.18.self_attn.out_proj.bias', 'model.decoder.layers.18.self_attn_layer_norm.weight', 'model.decoder.layers.18.self_attn_layer_norm.bias', 'model.decoder.layers.18.fc1.weight', 'model.decoder.layers.18.fc1.bias', 'model.decoder.layers.18.fc2.weight', 'model.decoder.layers.18.fc2.bias', 'model.decoder.layers.18.final_layer_norm.weight', 'model.decoder.layers.18.final_layer_norm.bias', 'model.decoder.layers.19.self_attn.k_proj.weight', 'model.decoder.layers.19.self_attn.k_proj.bias', 'model.decoder.layers.19.self_attn.v_proj.weight', 'model.decoder.layers.19.self_attn.v_proj.bias', 'model.decoder.layers.19.self_attn.q_proj.weight', 'model.decoder.layers.19.self_attn.q_proj.bias', 'model.decoder.layers.19.self_attn.out_proj.weight', 'model.decoder.layers.19.self_attn.out_proj.bias', 'model.decoder.layers.19.self_attn_layer_norm.weight', 'model.decoder.layers.19.self_attn_layer_norm.bias', 'model.decoder.layers.19.fc1.weight', 'model.decoder.layers.19.fc1.bias', 'model.decoder.layers.19.fc2.weight', 'model.decoder.layers.19.fc2.bias', 'model.decoder.layers.19.final_layer_norm.weight', 'model.decoder.layers.19.final_layer_norm.bias', 'model.decoder.layers.20.self_attn.k_proj.weight', 'model.decoder.layers.20.self_attn.k_proj.bias', 'model.decoder.layers.20.self_attn.v_proj.weight', 'model.decoder.layers.20.self_attn.v_proj.bias', 'model.decoder.layers.20.self_attn.q_proj.weight', 'model.decoder.layers.20.self_attn.q_proj.bias', 'model.decoder.layers.20.self_attn.out_proj.weight', 'model.decoder.layers.20.self_attn.out_proj.bias', 'model.decoder.layers.20.self_attn_layer_norm.weight', 'model.decoder.layers.20.self_attn_layer_norm.bias', 'model.decoder.layers.20.fc1.weight', 'model.decoder.layers.20.fc1.bias', 'model.decoder.layers.20.fc2.weight', 'model.decoder.layers.20.fc2.bias', 'model.decoder.layers.20.final_layer_norm.weight', 'model.decoder.layers.20.final_layer_norm.bias', 'model.decoder.layers.21.self_attn.k_proj.weight', 'model.decoder.layers.21.self_attn.k_proj.bias', 'model.decoder.layers.21.self_attn.v_proj.weight', 'model.decoder.layers.21.self_attn.v_proj.bias', 'model.decoder.layers.21.self_attn.q_proj.weight', 'model.decoder.layers.21.self_attn.q_proj.bias', 'model.decoder.layers.21.self_attn.out_proj.weight', 'model.decoder.layers.21.self_attn.out_proj.bias', 'model.decoder.layers.21.self_attn_layer_norm.weight', 'model.decoder.layers.21.self_attn_layer_norm.bias', 'model.decoder.layers.21.fc1.weight', 'model.decoder.layers.21.fc1.bias', 'model.decoder.layers.21.fc2.weight', 'model.decoder.layers.21.fc2.bias', 'model.decoder.layers.21.final_layer_norm.weight', 'model.decoder.layers.21.final_layer_norm.bias', 'model.decoder.layers.22.self_attn.k_proj.weight', 'model.decoder.layers.22.self_attn.k_proj.bias', 'model.decoder.layers.22.self_attn.v_proj.weight', 'model.decoder.layers.22.self_attn.v_proj.bias', 'model.decoder.layers.22.self_attn.q_proj.weight', 'model.decoder.layers.22.self_attn.q_proj.bias', 'model.decoder.layers.22.self_attn.out_proj.weight', 'model.decoder.layers.22.self_attn.out_proj.bias', 'model.decoder.layers.22.self_attn_layer_norm.weight', 'model.decoder.layers.22.self_attn_layer_norm.bias', 'model.decoder.layers.22.fc1.weight', 'model.decoder.layers.22.fc1.bias', 'model.decoder.layers.22.fc2.weight', 'model.decoder.layers.22.fc2.bias', 'model.decoder.layers.22.final_layer_norm.weight', 'model.decoder.layers.22.final_layer_norm.bias', 'model.decoder.layers.23.self_attn.k_proj.weight', 'model.decoder.layers.23.self_attn.k_proj.bias', 'model.decoder.layers.23.self_attn.v_proj.weight', 'model.decoder.layers.23.self_attn.v_proj.bias', 'model.decoder.layers.23.self_attn.q_proj.weight', 'model.decoder.layers.23.self_attn.q_proj.bias', 'model.decoder.layers.23.self_attn.out_proj.weight', 'model.decoder.layers.23.self_attn.out_proj.bias', 'model.decoder.layers.23.self_attn_layer_norm.weight', 'model.decoder.layers.23.self_attn_layer_norm.bias', 'model.decoder.layers.23.fc1.weight', 'model.decoder.layers.23.fc1.bias', 'model.decoder.layers.23.fc2.weight', 'model.decoder.layers.23.fc2.bias', 'model.decoder.layers.23.final_layer_norm.weight', 'model.decoder.layers.23.final_layer_norm.bias', 'model.decoder.layers.24.self_attn.k_proj.weight', 'model.decoder.layers.24.self_attn.k_proj.bias', 'model.decoder.layers.24.self_attn.v_proj.weight', 'model.decoder.layers.24.self_attn.v_proj.bias', 'model.decoder.layers.24.self_attn.q_proj.weight', 'model.decoder.layers.24.self_attn.q_proj.bias', 'model.decoder.layers.24.self_attn.out_proj.weight', 'model.decoder.layers.24.self_attn.out_proj.bias', 'model.decoder.layers.24.self_attn_layer_norm.weight', 'model.decoder.layers.24.self_attn_layer_norm.bias', 'model.decoder.layers.24.fc1.weight', 'model.decoder.layers.24.fc1.bias', 'model.decoder.layers.24.fc2.weight', 'model.decoder.layers.24.fc2.bias', 'model.decoder.layers.24.final_layer_norm.weight', 'model.decoder.layers.24.final_layer_norm.bias', 'model.decoder.layers.25.self_attn.k_proj.weight', 'model.decoder.layers.25.self_attn.k_proj.bias', 'model.decoder.layers.25.self_attn.v_proj.weight', 'model.decoder.layers.25.self_attn.v_proj.bias', 'model.decoder.layers.25.self_attn.q_proj.weight', 'model.decoder.layers.25.self_attn.q_proj.bias', 'model.decoder.layers.25.self_attn.out_proj.weight', 'model.decoder.layers.25.self_attn.out_proj.bias', 'model.decoder.layers.25.self_attn_layer_norm.weight', 'model.decoder.layers.25.self_attn_layer_norm.bias', 'model.decoder.layers.25.fc1.weight', 'model.decoder.layers.25.fc1.bias', 'model.decoder.layers.25.fc2.weight', 'model.decoder.layers.25.fc2.bias', 'model.decoder.layers.25.final_layer_norm.weight', 'model.decoder.layers.25.final_layer_norm.bias', 'model.decoder.layers.26.self_attn.k_proj.weight', 'model.decoder.layers.26.self_attn.k_proj.bias', 'model.decoder.layers.26.self_attn.v_proj.weight', 'model.decoder.layers.26.self_attn.v_proj.bias', 'model.decoder.layers.26.self_attn.q_proj.weight', 'model.decoder.layers.26.self_attn.q_proj.bias', 'model.decoder.layers.26.self_attn.out_proj.weight', 'model.decoder.layers.26.self_attn.out_proj.bias', 'model.decoder.layers.26.self_attn_layer_norm.weight', 'model.decoder.layers.26.self_attn_layer_norm.bias', 'model.decoder.layers.26.fc1.weight', 'model.decoder.layers.26.fc1.bias', 'model.decoder.layers.26.fc2.weight', 'model.decoder.layers.26.fc2.bias', 'model.decoder.layers.26.final_layer_norm.weight', 'model.decoder.layers.26.final_layer_norm.bias', 'model.decoder.layers.27.self_attn.k_proj.weight', 'model.decoder.layers.27.self_attn.k_proj.bias', 'model.decoder.layers.27.self_attn.v_proj.weight', 'model.decoder.layers.27.self_attn.v_proj.bias', 'model.decoder.layers.27.self_attn.q_proj.weight', 'model.decoder.layers.27.self_attn.q_proj.bias', 'model.decoder.layers.27.self_attn.out_proj.weight', 'model.decoder.layers.27.self_attn.out_proj.bias', 'model.decoder.layers.27.self_attn_layer_norm.weight', 'model.decoder.layers.27.self_attn_layer_norm.bias', 'model.decoder.layers.27.fc1.weight', 'model.decoder.layers.27.fc1.bias', 'model.decoder.layers.27.fc2.weight', 'model.decoder.layers.27.fc2.bias', 'model.decoder.layers.27.final_layer_norm.weight', 'model.decoder.layers.27.final_layer_norm.bias', 'model.decoder.layers.28.self_attn.k_proj.weight', 'model.decoder.layers.28.self_attn.k_proj.bias', 'model.decoder.layers.28.self_attn.v_proj.weight', 'model.decoder.layers.28.self_attn.v_proj.bias', 'model.decoder.layers.28.self_attn.q_proj.weight', 'model.decoder.layers.28.self_attn.q_proj.bias', 'model.decoder.layers.28.self_attn.out_proj.weight', 'model.decoder.layers.28.self_attn.out_proj.bias', 'model.decoder.layers.28.self_attn_layer_norm.weight', 'model.decoder.layers.28.self_attn_layer_norm.bias', 'model.decoder.layers.28.fc1.weight', 'model.decoder.layers.28.fc1.bias', 'model.decoder.layers.28.fc2.weight', 'model.decoder.layers.28.fc2.bias', 'model.decoder.layers.28.final_layer_norm.weight', 'model.decoder.layers.28.final_layer_norm.bias', 'model.decoder.layers.29.self_attn.k_proj.weight', 'model.decoder.layers.29.self_attn.k_proj.bias', 'model.decoder.layers.29.self_attn.v_proj.weight', 'model.decoder.layers.29.self_attn.v_proj.bias', 'model.decoder.layers.29.self_attn.q_proj.weight', 'model.decoder.layers.29.self_attn.q_proj.bias', 'model.decoder.layers.29.self_attn.out_proj.weight', 'model.decoder.layers.29.self_attn.out_proj.bias', 'model.decoder.layers.29.self_attn_layer_norm.weight', 'model.decoder.layers.29.self_attn_layer_norm.bias', 'model.decoder.layers.29.fc1.weight', 'model.decoder.layers.29.fc1.bias', 'model.decoder.layers.29.fc2.weight', 'model.decoder.layers.29.fc2.bias', 'model.decoder.layers.29.final_layer_norm.weight', 'model.decoder.layers.29.final_layer_norm.bias', 'model.decoder.layers.30.self_attn.k_proj.weight', 'model.decoder.layers.30.self_attn.k_proj.bias', 'model.decoder.layers.30.self_attn.v_proj.weight', 'model.decoder.layers.30.self_attn.v_proj.bias', 'model.decoder.layers.30.self_attn.q_proj.weight', 'model.decoder.layers.30.self_attn.q_proj.bias', 'model.decoder.layers.30.self_attn.out_proj.weight', 'model.decoder.layers.30.self_attn.out_proj.bias', 'model.decoder.layers.30.self_attn_layer_norm.weight', 'model.decoder.layers.30.self_attn_layer_norm.bias', 'model.decoder.layers.30.fc1.weight', 'model.decoder.layers.30.fc1.bias', 'model.decoder.layers.30.fc2.weight', 'model.decoder.layers.30.fc2.bias', 'model.decoder.layers.30.final_layer_norm.weight', 'model.decoder.layers.30.final_layer_norm.bias', 'model.decoder.layers.31.self_attn.k_proj.weight', 'model.decoder.layers.31.self_attn.k_proj.bias', 'model.decoder.layers.31.self_attn.v_proj.weight', 'model.decoder.layers.31.self_attn.v_proj.bias', 'model.decoder.layers.31.self_attn.q_proj.weight', 'model.decoder.layers.31.self_attn.q_proj.bias', 'model.decoder.layers.31.self_attn.out_proj.weight', 'model.decoder.layers.31.self_attn.out_proj.bias', 'model.decoder.layers.31.self_attn_layer_norm.weight', 'model.decoder.layers.31.self_attn_layer_norm.bias', 'model.decoder.layers.31.fc1.weight', 'model.decoder.layers.31.fc1.bias', 'model.decoder.layers.31.fc2.weight', 'model.decoder.layers.31.fc2.bias', 'model.decoder.layers.31.final_layer_norm.weight', 'model.decoder.layers.31.final_layer_norm.bias']
- This IS expected if you are initializing GPTNeoForCausalLM from a Flax model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a FlaxBertForPreTraining model).
- This IS NOT expected if you are initializing GPTNeoForCausalLM from a Flax model that you expect to be exactly identical (e.g. initializing a BertForSequenceClassification model from a FlaxBertForSequenceClassification model).
Some weights of GPTNeoForCausalLM were not initialized from the Flax model and are newly initialized: ['transformer.h.10.ln_2.bias', 'transformer.h.31.attn.attention.bias', 'transformer.h.20.ln_1.weight', 'transformer.h.6.ln_2.weight', 'transformer.h.30.ln_2.bias', 'transformer.h.16.attn.attention.out_proj.weight', 'transformer.h.14.mlp.c_proj.bias', 'transformer.h.29.ln_2.weight', 'transformer.h.17.attn.attention.bias', 'transformer.h.2.ln_1.bias', 'transformer.h.6.ln_2.bias', 'transformer.h.14.mlp.c_proj.weight', 'transformer.h.2.attn.attention.masked_bias', 'transformer.h.2.attn.attention.k_proj.weight', 'transformer.h.4.attn.attention.bias', 'transformer.h.18.ln_2.bias', 'transformer.h.29.attn.attention.masked_bias', 'transformer.h.25.attn.attention.q_proj.weight', 'transformer.h.12.attn.attention.v_proj.weight', 'transformer.h.18.attn.attention.v_proj.weight', 'transformer.h.1.attn.attention.bias', 'transformer.h.8.ln_1.bias', 'transformer.h.15.mlp.c_fc.weight', 'transformer.h.27.ln_2.weight', 'transformer.h.31.attn.attention.masked_bias', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.21.attn.attention.v_proj.weight', 'transformer.h.27.ln_2.bias', 'transformer.h.19.ln_2.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.23.ln_2.weight', 'transformer.h.18.attn.attention.bias', 'transformer.h.19.mlp.c_proj.weight', 'transformer.h.19.ln_2.weight', 'transformer.h.18.mlp.c_fc.weight', 'transformer.h.24.ln_1.weight', 'transformer.h.6.ln_1.weight', 'transformer.h.17.attn.attention.out_proj.bias', 'transformer.h.13.mlp.c_fc.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.7.attn.attention.out_proj.bias', 'transformer.h.18.attn.attention.out_proj.bias', 'transformer.h.19.mlp.c_fc.weight', 'transformer.h.27.attn.attention.k_proj.weight', 'transformer.h.31.ln_2.bias', 'transformer.h.11.attn.attention.masked_bias', 'transformer.h.1.attn.attention.out_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.7.ln_1.weight', 'transformer.h.26.attn.attention.k_proj.weight', 'transformer.h.26.mlp.c_fc.bias', 'transformer.h.24.ln_1.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.9.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.17.ln_1.bias', 'transformer.h.4.ln_1.bias', 'transformer.h.10.attn.attention.k_proj.weight', 'transformer.h.24.attn.attention.k_proj.weight', 'transformer.h.8.attn.attention.v_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.13.mlp.c_proj.weight', 'transformer.h.27.attn.attention.out_proj.bias', 'transformer.h.31.attn.attention.k_proj.weight', 'transformer.h.8.attn.attention.q_proj.weight', 'transformer.h.19.attn.attention.out_proj.bias', 'transformer.h.13.ln_2.weight', 'transformer.h.0.attn.attention.out_proj.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.28.attn.attention.out_proj.bias', 'transformer.h.30.ln_1.weight', 'transformer.h.14.ln_1.weight', 'transformer.h.1.attn.attention.k_proj.weight', 'transformer.h.30.attn.attention.out_proj.weight', 'transformer.h.15.attn.attention.v_proj.weight', 'transformer.h.19.attn.attention.q_proj.weight', 'transformer.h.0.attn.attention.q_proj.weight', 'transformer.h.0.attn.attention.bias', 'transformer.h.15.attn.attention.k_proj.weight', 'transformer.h.28.ln_2.weight', 'transformer.h.14.attn.attention.k_proj.weight', 'transformer.h.27.mlp.c_fc.bias', 'transformer.ln_f.weight', 'transformer.h.15.attn.attention.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.12.mlp.c_proj.weight', 'transformer.h.17.attn.attention.out_proj.weight', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.30.mlp.c_proj.weight', 'transformer.h.22.attn.attention.q_proj.weight', 'transformer.h.24.mlp.c_proj.weight', 'transformer.h.20.mlp.c_proj.bias', 'transformer.h.14.ln_2.weight', 'transformer.h.20.attn.attention.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.29.mlp.c_proj.weight', 'transformer.h.14.attn.attention.q_proj.weight', 'transformer.h.21.ln_2.bias', 'transformer.h.25.attn.attention.out_proj.weight', 'transformer.h.17.ln_2.weight', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.5.ln_1.weight', 'transformer.h.25.attn.attention.k_proj.weight', 'transformer.h.23.attn.attention.out_proj.weight', 'transformer.h.9.attn.attention.q_proj.weight', 'transformer.h.7.attn.attention.out_proj.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.9.attn.attention.out_proj.bias', 'transformer.h.31.attn.attention.q_proj.weight', 'transformer.h.19.mlp.c_proj.bias', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.24.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.24.mlp.c_fc.bias', 'transformer.h.16.ln_1.weight', 'transformer.h.5.attn.attention.out_proj.bias', 'transformer.h.25.ln_2.bias', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.15.mlp.c_fc.bias', 'transformer.h.8.ln_2.bias', 'transformer.h.16.attn.attention.masked_bias', 'transformer.h.23.ln_1.bias', 'transformer.h.13.mlp.c_fc.weight', 'transformer.h.5.attn.attention.k_proj.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.13.ln_1.bias', 'transformer.h.21.ln_1.bias', 'transformer.h.0.ln_1.weight', 'transformer.h.10.attn.attention.out_proj.bias', 'transformer.h.27.ln_1.weight', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.5.attn.attention.masked_bias', 'transformer.h.17.mlp.c_fc.weight', 'transformer.h.28.ln_1.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.30.attn.attention.out_proj.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.3.ln_1.bias', 'transformer.h.11.ln_1.bias', 'transformer.h.3.attn.attention.out_proj.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.23.attn.attention.q_proj.weight', 'transformer.h.23.ln_2.bias', 'transformer.h.16.attn.attention.v_proj.weight', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.26.mlp.c_proj.bias', 'transformer.h.27.attn.attention.masked_bias', 'transformer.h.10.ln_2.weight', 'transformer.h.1.attn.attention.v_proj.weight', 'transformer.h.28.mlp.c_fc.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.11.attn.attention.k_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.14.mlp.c_fc.bias', 'transformer.h.12.attn.attention.masked_bias', 'transformer.h.0.ln_2.bias', 'transformer.h.22.ln_2.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.4.attn.attention.v_proj.weight', 'transformer.h.13.ln_2.bias', 'transformer.h.6.ln_1.bias', 'transformer.h.15.ln_1.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.23.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.18.ln_2.weight', 'transformer.h.30.attn.attention.v_proj.weight', 'transformer.h.14.ln_1.bias', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.14.ln_2.bias', 'transformer.h.30.attn.attention.masked_bias', 'transformer.h.0.attn.attention.k_proj.weight', 'transformer.h.12.ln_2.weight', 'transformer.h.21.attn.attention.out_proj.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.25.ln_1.bias', 'transformer.h.22.attn.attention.bias', 'transformer.h.8.attn.attention.bias', 'transformer.h.1.ln_1.bias', 'transformer.h.15.attn.attention.q_proj.weight', 'transformer.h.8.ln_2.weight', 'transformer.h.1.ln_1.weight', 'transformer.h.4.attn.attention.q_proj.weight', 'transformer.h.15.attn.attention.masked_bias', 'transformer.ln_f.bias', 'transformer.h.19.mlp.c_fc.bias', 'transformer.h.28.attn.attention.masked_bias', 'transformer.h.22.mlp.c_proj.bias', 'transformer.h.15.ln_2.weight', 'transformer.h.29.attn.attention.q_proj.weight', 'transformer.h.5.ln_2.weight', 'transformer.h.31.mlp.c_fc.bias', 'transformer.h.14.attn.attention.v_proj.weight', 'transformer.h.20.mlp.c_fc.bias', 'transformer.h.13.attn.attention.k_proj.weight', 'transformer.h.26.ln_1.bias', 'transformer.h.31.mlp.c_fc.weight', 'transformer.h.31.mlp.c_proj.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.5.attn.attention.out_proj.weight', 'transformer.h.6.attn.attention.v_proj.weight', 'transformer.h.18.ln_1.bias', 'transformer.h.20.ln_2.weight', 'transformer.h.21.attn.attention.q_proj.weight', 'transformer.h.6.attn.attention.bias', 'transformer.h.27.mlp.c_proj.bias', 'transformer.h.21.ln_2.weight', 'transformer.h.3.attn.attention.k_proj.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.23.mlp.c_proj.bias', 'transformer.h.0.attn.attention.out_proj.weight', 'transformer.h.11.attn.attention.bias', 'transformer.h.13.attn.attention.v_proj.weight', 'transformer.h.19.attn.attention.masked_bias', 'transformer.h.31.attn.attention.v_proj.weight', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.11.attn.attention.q_proj.weight', 'transformer.h.18.attn.attention.masked_bias', 'transformer.h.24.ln_2.weight', 'transformer.h.27.attn.attention.q_proj.weight', 'transformer.h.27.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.15.mlp.c_proj.weight', 'transformer.h.30.ln_1.bias', 'transformer.h.24.attn.attention.out_proj.weight', 'transformer.h.10.attn.attention.masked_bias', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.17.ln_2.bias', 'transformer.h.28.attn.attention.out_proj.weight', 'transformer.h.31.ln_2.weight', 'transformer.h.3.attn.attention.masked_bias', 'transformer.h.16.ln_2.bias', 'transformer.h.28.ln_1.weight', 'transformer.h.22.attn.attention.k_proj.weight', 'transformer.h.0.attn.attention.masked_bias', 'transformer.h.2.attn.attention.out_proj.bias', 'transformer.h.13.ln_1.weight', 'transformer.h.19.ln_1.bias', 'transformer.h.6.attn.attention.out_proj.bias', 'transformer.h.11.mlp.c_proj.bias', 'transformer.h.17.mlp.c_proj.weight', 'transformer.h.2.attn.attention.bias', 'transformer.h.26.attn.attention.q_proj.weight', 'transformer.h.29.ln_1.weight', 'transformer.h.15.ln_1.bias', 'transformer.h.18.mlp.c_fc.bias', 'transformer.h.22.attn.attention.masked_bias', 'transformer.h.30.mlp.c_fc.weight', 'transformer.h.8.attn.attention.k_proj.weight', 'transformer.h.26.ln_2.bias', 'transformer.h.7.ln_1.bias', 'transformer.h.25.mlp.c_fc.weight', 'transformer.h.25.mlp.c_fc.bias', 'transformer.h.22.mlp.c_proj.weight', 'transformer.h.20.attn.attention.masked_bias', 'transformer.h.29.ln_2.bias', 'transformer.h.15.mlp.c_proj.bias', 'transformer.h.12.attn.attention.out_proj.weight', 'transformer.h.7.attn.attention.masked_bias', 'transformer.h.31.mlp.c_proj.weight', 'transformer.h.26.ln_1.weight', 'transformer.h.12.attn.attention.bias', 'transformer.h.25.attn.attention.bias', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.16.mlp.c_fc.weight', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.29.attn.attention.bias', 'transformer.h.23.attn.attention.k_proj.weight', 'transformer.h.21.mlp.c_proj.bias', 'transformer.h.13.mlp.c_proj.bias', 'transformer.h.21.mlp.c_fc.weight', 'transformer.h.28.mlp.c_proj.weight', 'transformer.h.29.attn.attention.k_proj.weight', 'transformer.h.24.mlp.c_proj.bias', 'transformer.h.26.attn.attention.out_proj.weight', 'transformer.h.11.attn.attention.out_proj.weight', 'transformer.h.16.mlp.c_fc.bias', 'transformer.h.23.ln_1.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.26.attn.attention.v_proj.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.22.ln_1.weight', 'transformer.h.12.attn.attention.k_proj.weight', 'transformer.h.13.attn.attention.out_proj.weight', 'transformer.h.2.attn.attention.q_proj.weight', 'transformer.h.14.attn.attention.out_proj.bias', 'transformer.h.14.attn.attention.out_proj.weight', 'transformer.h.25.ln_1.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.29.mlp.c_proj.bias', 'transformer.h.30.attn.attention.q_proj.weight', 'transformer.h.21.ln_1.weight', 'transformer.h.29.ln_1.bias', 'transformer.h.9.attn.attention.out_proj.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.12.attn.attention.q_proj.weight', 'transformer.h.16.mlp.c_proj.bias', 'transformer.h.29.attn.attention.v_proj.weight', 'transformer.h.11.ln_2.weight', 'transformer.h.20.ln_2.bias', 'transformer.h.27.attn.attention.out_proj.weight', 'transformer.h.8.attn.attention.out_proj.weight', 'transformer.h.26.mlp.c_proj.weight', 'transformer.h.21.attn.attention.bias', 'transformer.h.15.attn.attention.out_proj.bias', 'transformer.h.24.attn.attention.q_proj.weight', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.31.attn.attention.out_proj.bias', 'transformer.h.2.ln_2.bias', 'transformer.h.20.attn.attention.v_proj.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.2.attn.attention.out_proj.weight', 'transformer.h.13.attn.attention.masked_bias', 'transformer.h.6.attn.attention.k_proj.weight', 'transformer.h.29.mlp.c_fc.weight', 'transformer.h.6.attn.attention.q_proj.weight', 'transformer.h.9.attn.attention.v_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.20.mlp.c_proj.weight', 'transformer.h.31.ln_1.bias', 'transformer.h.0.attn.attention.v_proj.weight', 'transformer.h.6.attn.attention.out_proj.weight', 'transformer.h.30.mlp.c_proj.bias', 'transformer.h.21.attn.attention.k_proj.weight', 'transformer.h.22.ln_2.weight', 'transformer.h.5.attn.attention.q_proj.weight', 'transformer.h.17.attn.attention.q_proj.weight', 'transformer.h.31.attn.attention.out_proj.weight', 'transformer.h.19.attn.attention.v_proj.weight', 'transformer.h.18.attn.attention.out_proj.weight', 'transformer.h.28.attn.attention.v_proj.weight', 'transformer.h.24.attn.attention.out_proj.bias', 'transformer.h.24.attn.attention.masked_bias', 'transformer.h.4.ln_1.weight', 'transformer.h.23.mlp.c_fc.bias', 'transformer.h.29.mlp.c_fc.bias', 'transformer.h.12.attn.attention.out_proj.bias', 'transformer.h.29.attn.attention.out_proj.weight', 'transformer.h.28.attn.attention.k_proj.weight', 'transformer.h.14.attn.attention.bias', 'transformer.h.14.attn.attention.masked_bias', 'transformer.h.16.ln_1.bias', 'transformer.h.20.attn.attention.out_proj.bias', 'transformer.h.12.mlp.c_fc.weight', 'transformer.h.20.attn.attention.q_proj.weight', 'transformer.h.10.attn.attention.v_proj.weight', 'transformer.h.20.mlp.c_fc.weight', 'transformer.h.17.attn.attention.v_proj.weight', 'transformer.h.25.mlp.c_proj.weight', 'transformer.h.7.attn.attention.v_proj.weight', 'transformer.h.28.mlp.c_proj.bias', 'transformer.h.31.ln_1.weight', 'transformer.h.17.attn.attention.k_proj.weight', 'transformer.h.22.attn.attention.out_proj.bias', 'transformer.h.12.ln_1.weight', 'transformer.h.26.attn.attention.bias', 'transformer.h.16.attn.attention.out_proj.bias', 'transformer.h.11.attn.attention.v_proj.weight', 'transformer.h.27.attn.attention.v_proj.weight', 'transformer.h.4.attn.attention.out_proj.bias', 'transformer.h.4.attn.attention.masked_bias', 'transformer.h.7.attn.attention.k_proj.weight', 'transformer.h.27.ln_1.bias', 'transformer.h.28.attn.attention.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.29.attn.attention.out_proj.bias', 'transformer.h.26.attn.attention.out_proj.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.15.attn.attention.out_proj.weight', 'transformer.h.21.mlp.c_proj.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.26.attn.attention.masked_bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.20.ln_1.bias', 'transformer.h.18.ln_1.weight', 'transformer.h.22.ln_1.bias', 'transformer.h.17.mlp.c_proj.bias', 'transformer.h.1.attn.attention.masked_bias', 'transformer.h.19.ln_1.weight', 'transformer.h.23.attn.attention.bias', 'transformer.h.3.attn.attention.out_proj.weight', 'transformer.h.24.attn.attention.bias', 'transformer.h.28.mlp.c_fc.weight', 'transformer.h.12.ln_1.bias', 'transformer.h.15.ln_2.bias', 'transformer.h.8.attn.attention.masked_bias', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.25.ln_2.weight', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.14.mlp.c_fc.weight', 'transformer.h.19.attn.attention.bias', 'transformer.h.25.attn.attention.masked_bias', 'transformer.h.1.attn.attention.out_proj.weight', 'transformer.h.24.attn.attention.v_proj.weight', 'transformer.h.18.mlp.c_proj.bias', 'transformer.h.22.mlp.c_fc.bias', 'transformer.h.10.attn.attention.q_proj.weight', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.20.attn.attention.k_proj.weight', 'transformer.h.23.attn.attention.masked_bias', 'transformer.h.22.mlp.c_fc.weight', 'transformer.h.13.attn.attention.q_proj.weight', 'transformer.h.16.attn.attention.q_proj.weight', 'transformer.h.5.attn.attention.bias', 'transformer.h.4.attn.attention.k_proj.weight', 'transformer.h.19.attn.attention.out_proj.weight', 'transformer.h.1.ln_2.weight', 'transformer.h.25.attn.attention.v_proj.weight', 'transformer.h.10.ln_1.weight', 'transformer.h.9.attn.attention.k_proj.weight', 'transformer.h.4.attn.attention.out_proj.weight', 'transformer.h.26.ln_2.weight', 'transformer.h.30.ln_2.weight', 'transformer.h.21.attn.attention.out_proj.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.21.attn.attention.masked_bias', 'transformer.h.30.mlp.c_fc.bias', 'transformer.h.16.attn.attention.bias', 'transformer.h.13.attn.attention.out_proj.bias', 'transformer.h.10.attn.attention.out_proj.weight', 'transformer.h.7.attn.attention.q_proj.weight', 'transformer.h.22.attn.attention.out_proj.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.18.attn.attention.k_proj.weight', 'transformer.h.16.attn.attention.k_proj.weight', 'transformer.h.30.attn.attention.bias', 'transformer.h.9.attn.attention.masked_bias', 'transformer.h.27.mlp.c_proj.weight', 'transformer.h.12.ln_2.bias', 'transformer.h.12.mlp.c_proj.bias', 'transformer.h.21.mlp.c_fc.bias', 'transformer.h.23.mlp.c_fc.weight', 'transformer.h.16.ln_2.weight', 'transformer.h.10.attn.attention.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.17.attn.attention.masked_bias', 'transformer.h.22.attn.attention.v_proj.weight', 'transformer.h.23.attn.attention.out_proj.bias', 'transformer.h.23.attn.attention.v_proj.weight', 'transformer.h.8.attn.attention.out_proj.bias', 'lm_head.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.16.mlp.c_proj.weight', 'transformer.h.1.attn.attention.q_proj.weight', 'transformer.h.18.attn.attention.q_proj.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.27.attn.attention.bias', 'transformer.h.17.mlp.c_fc.bias', 'transformer.h.25.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.3.attn.attention.q_proj.weight', 'transformer.h.20.attn.attention.out_proj.weight', 'transformer.h.6.attn.attention.masked_bias', 'transformer.h.19.attn.attention.k_proj.weight', 'transformer.h.11.attn.attention.out_proj.bias', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.30.attn.attention.k_proj.weight', 'transformer.h.28.attn.attention.q_proj.weight', 'transformer.wpe.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.2.attn.attention.v_proj.weight', 'transformer.h.2.ln_1.weight', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.28.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.12.mlp.c_fc.bias', 'transformer.h.24.ln_2.bias', 'transformer.h.5.attn.attention.v_proj.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.3.attn.attention.v_proj.weight', 'transformer.wte.weight', 'transformer.h.25.attn.attention.out_proj.bias', 'transformer.h.26.mlp.c_fc.weight', 'transformer.h.18.mlp.c_proj.weight', 'transformer.h.17.ln_1.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2022-11-18 16:16:55,225][datasets.builder][WARNING] - Using custom data configuration ViktorThink--mountain_combined_813306-6918b9ea07482433
[2022-11-18 16:16:55,322][datasets.builder][WARNING] - Found cached dataset csv (/home/paperspace/.cache/huggingface/datasets/ViktorThink___csv/ViktorThink--mountain_combined_813306-6918b9ea07482433/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  5.29it/s]100%|██████████| 2/2 [00:00<00:00,  9.96it/s]
Running tokenizer on dataset #0:   0%|          | 0/68 [00:00<?, ?ba/s]
Running tokenizer on dataset #1:   0%|          | 0/68 [00:00<?, ?ba/s][A

Running tokenizer on dataset #2:   0%|          | 0/68 [00:00<?, ?ba/s][A[ARunning tokenizer on dataset #0:   1%|▏         | 1/68 [00:02<02:56,  2.63s/ba]


Running tokenizer on dataset #3:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A



Running tokenizer on dataset #4:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A
Running tokenizer on dataset #1:   1%|▏         | 1/68 [00:02<03:11,  2.86s/ba][A




Running tokenizer on dataset #5:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[ARunning tokenizer on dataset #0:   3%|▎         | 2/68 [00:04<02:41,  2.44s/ba]





Running tokenizer on dataset #6:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A

Running tokenizer on dataset #2:   1%|▏         | 1/68 [00:03<04:02,  3.62s/ba][A[A


Running tokenizer on dataset #3:   1%|▏         | 1/68 [00:03<03:32,  3.17s/ba][A[A[A






Running tokenizer on dataset #7:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A
Running tokenizer on dataset #1:   3%|▎         | 2/68 [00:05<03:15,  2.96s/ba][A



Running tokenizer on dataset #4:   1%|▏         | 1/68 [00:03<03:42,  3.32s/ba][A[A[A[A







Running tokenizer on dataset #8:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:   4%|▍         | 3/68 [00:07<02:34,  2.38s/ba]




Running tokenizer on dataset #5:   1%|▏         | 1/68 [00:02<03:18,  2.97s/ba][A[A[A[A[A





Running tokenizer on dataset #6:   1%|▏         | 1/68 [00:02<03:02,  2.73s/ba][A[A[A[A[A[A


Running tokenizer on dataset #3:   3%|▎         | 2/68 [00:05<02:56,  2.67s/ba][A[A[A








Running tokenizer on dataset #9:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:   3%|▎         | 2/68 [00:06<03:37,  3.29s/ba][A[A
Running tokenizer on dataset #1:   4%|▍         | 3/68 [00:08<02:54,  2.69s/ba][A









Running tokenizer on dataset #10:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:   1%|▏         | 1/68 [00:02<03:13,  2.89s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:   6%|▌         | 4/68 [00:09<02:28,  2.32s/ba]







Running tokenizer on dataset #8:   1%|▏         | 1/68 [00:02<03:01,  2.71s/ba][A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:   3%|▎         | 2/68 [00:06<03:31,  3.21s/ba][A[A[A[A










Running tokenizer on dataset #11:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:   3%|▎         | 2/68 [00:05<02:44,  2.49s/ba][A[A[A[A[A[A




Running tokenizer on dataset #5:   3%|▎         | 2/68 [00:06<03:22,  3.07s/ba][A[A[A[A[A


Running tokenizer on dataset #3:   4%|▍         | 3/68 [00:07<02:42,  2.49s/ba][A[A[A

Running tokenizer on dataset #2:   4%|▍         | 3/68 [00:09<03:23,  3.12s/ba][A[A
Running tokenizer on dataset #1:   6%|▌         | 4/68 [00:11<02:54,  2.73s/ba][A






Running tokenizer on dataset #7:   3%|▎         | 2/68 [00:05<02:50,  2.58s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:   1%|▏         | 1/68 [00:03<04:00,  3.59s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:   7%|▋         | 5/68 [00:12<02:36,  2.48s/ba]







Running tokenizer on dataset #8:   3%|▎         | 2/68 [00:05<02:44,  2.49s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:   4%|▍         | 3/68 [00:07<02:33,  2.36s/ba][A[A[A[A[A[A


Running tokenizer on dataset #3:   6%|▌         | 4/68 [00:09<02:31,  2.37s/ba][A[A[A



Running tokenizer on dataset #4:   4%|▍         | 3/68 [00:09<03:19,  3.08s/ba][A[A[A[A









Running tokenizer on dataset #10:   1%|▏         | 1/68 [00:03<04:02,  3.62s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:   4%|▍         | 3/68 [00:09<03:15,  3.01s/ba][A[A[A[A[A










Running tokenizer on dataset #11:   1%|▏         | 1/68 [00:03<03:58,  3.55s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:   7%|▋         | 5/68 [00:13<02:39,  2.53s/ba][A






Running tokenizer on dataset #7:   4%|▍         | 3/68 [00:07<02:38,  2.43s/ba][A[A[A[A[A[A[A

Running tokenizer on dataset #2:   6%|▌         | 4/68 [00:12<03:14,  3.04s/ba][A[A


Running tokenizer on dataset #3:   7%|▋         | 5/68 [00:12<02:23,  2.28s/ba][A[A[ARunning tokenizer on dataset #0:   9%|▉         | 6/68 [00:14<02:39,  2.57s/ba]







Running tokenizer on dataset #8:   4%|▍         | 3/68 [00:07<02:50,  2.63s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:   3%|▎         | 2/68 [00:06<03:36,  3.29s/ba][A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:   6%|▌         | 4/68 [00:09<02:34,  2.42s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:   6%|▌         | 4/68 [00:12<03:10,  2.97s/ba][A[A[A[A









Running tokenizer on dataset #10:   3%|▎         | 2/68 [00:06<03:37,  3.29s/ba][A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:   9%|▉         | 6/68 [00:15<02:26,  2.37s/ba][A






Running tokenizer on dataset #7:   6%|▌         | 4/68 [00:09<02:29,  2.33s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:   6%|▌         | 4/68 [00:12<03:11,  2.99s/ba][A[A[A[A[A










Running tokenizer on dataset #11:   3%|▎         | 2/68 [00:06<03:34,  3.25s/ba][A[A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:   9%|▉         | 6/68 [00:14<02:17,  2.21s/ba][A[A[A

Running tokenizer on dataset #2:   7%|▋         | 5/68 [00:15<03:05,  2.95s/ba][A[ARunning tokenizer on dataset #0:  10%|█         | 7/68 [00:17<02:41,  2.64s/ba]







Running tokenizer on dataset #8:   6%|▌         | 4/68 [00:10<02:54,  2.73s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:   7%|▋         | 5/68 [00:12<02:42,  2.57s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  10%|█         | 7/68 [00:17<02:18,  2.27s/ba][A








Running tokenizer on dataset #9:   4%|▍         | 3/68 [00:09<03:27,  3.19s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:   7%|▋         | 5/68 [00:11<02:23,  2.28s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:   7%|▋         | 5/68 [00:14<03:03,  2.91s/ba][A[A[A[A









Running tokenizer on dataset #10:   4%|▍         | 3/68 [00:09<03:24,  3.14s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  10%|█         | 7/68 [00:16<02:12,  2.18s/ba][A[A[A




Running tokenizer on dataset #5:   7%|▋         | 5/68 [00:14<03:04,  2.92s/ba][A[A[A[A[A










Running tokenizer on dataset #11:   4%|▍         | 3/68 [00:09<03:22,  3.11s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:   9%|▉         | 6/68 [00:18<02:59,  2.89s/ba][A[A
Running tokenizer on dataset #1:  12%|█▏        | 8/68 [00:19<02:13,  2.22s/ba][A






Running tokenizer on dataset #7:   9%|▉         | 6/68 [00:13<02:17,  2.21s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  12%|█▏        | 8/68 [00:20<02:40,  2.68s/ba]





Running tokenizer on dataset #6:   9%|▉         | 6/68 [00:15<02:43,  2.63s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:   7%|▋         | 5/68 [00:13<02:52,  2.74s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:   6%|▌         | 4/68 [00:12<03:17,  3.08s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  12%|█▏        | 8/68 [00:18<02:07,  2.13s/ba][A[A[A



Running tokenizer on dataset #4:   9%|▉         | 6/68 [00:17<02:57,  2.86s/ba][A[A[A[A









Running tokenizer on dataset #10:   6%|▌         | 4/68 [00:12<03:12,  3.01s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:   9%|▉         | 6/68 [00:17<02:59,  2.89s/ba][A[A[A[A[A
Running tokenizer on dataset #1:  13%|█▎        | 9/68 [00:21<02:08,  2.18s/ba][A






Running tokenizer on dataset #7:  10%|█         | 7/68 [00:16<02:13,  2.20s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  13%|█▎        | 9/68 [00:22<02:28,  2.52s/ba]










Running tokenizer on dataset #11:   6%|▌         | 4/68 [00:12<03:13,  3.03s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  10%|█         | 7/68 [00:20<02:54,  2.86s/ba][A[A


Running tokenizer on dataset #3:  13%|█▎        | 9/68 [00:20<02:03,  2.09s/ba][A[A[A







Running tokenizer on dataset #8:   9%|▉         | 6/68 [00:16<02:49,  2.74s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  10%|█         | 7/68 [00:18<02:47,  2.74s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:   7%|▋         | 5/68 [00:15<03:08,  2.99s/ba][A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  10%|█         | 7/68 [00:20<02:53,  2.84s/ba][A[A[A[A
Running tokenizer on dataset #1:  15%|█▍        | 10/68 [00:23<02:04,  2.15s/ba][A









Running tokenizer on dataset #10:   7%|▋         | 5/68 [00:15<03:05,  2.95s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  15%|█▍        | 10/68 [00:24<02:16,  2.36s/ba]






Running tokenizer on dataset #7:  12%|█▏        | 8/68 [00:18<02:10,  2.18s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  10%|█         | 7/68 [00:20<02:54,  2.85s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  15%|█▍        | 10/68 [00:22<01:59,  2.07s/ba][A[A[A










Running tokenizer on dataset #11:   7%|▋         | 5/68 [00:15<03:05,  2.94s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  12%|█▏        | 8/68 [00:23<02:48,  2.81s/ba][A[A







Running tokenizer on dataset #8:  10%|█         | 7/68 [00:19<02:49,  2.78s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  16%|█▌        | 11/68 [00:25<02:00,  2.11s/ba][A





Running tokenizer on dataset #6:  12%|█▏        | 8/68 [00:21<02:43,  2.73s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:   9%|▉         | 6/68 [00:18<03:01,  2.93s/ba][A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  12%|█▏        | 8/68 [00:23<02:47,  2.79s/ba][A[A[A[ARunning tokenizer on dataset #0:  16%|█▌        | 11/68 [00:26<02:09,  2.28s/ba]






Running tokenizer on dataset #7:  13%|█▎        | 9/68 [00:20<02:07,  2.16s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  16%|█▌        | 11/68 [00:24<01:56,  2.04s/ba][A[A[A









Running tokenizer on dataset #10:   9%|▉         | 6/68 [00:18<02:58,  2.87s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  12%|█▏        | 8/68 [00:23<02:49,  2.83s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  13%|█▎        | 9/68 [00:26<02:44,  2.78s/ba][A[A










Running tokenizer on dataset #11:   9%|▉         | 6/68 [00:18<02:59,  2.90s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  18%|█▊        | 12/68 [00:27<01:57,  2.09s/ba][ARunning tokenizer on dataset #0:  18%|█▊        | 12/68 [00:28<02:03,  2.20s/ba]







Running tokenizer on dataset #8:  12%|█▏        | 8/68 [00:21<02:45,  2.75s/ba][A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  15%|█▍        | 10/68 [00:22<02:03,  2.14s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  18%|█▊        | 12/68 [00:26<01:53,  2.03s/ba][A[A[A





Running tokenizer on dataset #6:  13%|█▎        | 9/68 [00:23<02:41,  2.74s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  13%|█▎        | 9/68 [00:25<02:43,  2.76s/ba][A[A[A[A








Running tokenizer on dataset #9:  10%|█         | 7/68 [00:21<02:55,  2.88s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  10%|█         | 7/68 [00:20<02:53,  2.85s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  13%|█▎        | 9/68 [00:25<02:44,  2.79s/ba][A[A[A[A[A
Running tokenizer on dataset #1:  19%|█▉        | 13/68 [00:29<01:54,  2.08s/ba][A

Running tokenizer on dataset #2:  15%|█▍        | 10/68 [00:28<02:38,  2.73s/ba][A[ARunning tokenizer on dataset #0:  19%|█▉        | 13/68 [00:30<01:58,  2.15s/ba]


Running tokenizer on dataset #3:  19%|█▉        | 13/68 [00:28<01:51,  2.02s/ba][A[A[A










Running tokenizer on dataset #11:  10%|█         | 7/68 [00:20<02:56,  2.89s/ba][A[A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  16%|█▌        | 11/68 [00:24<02:06,  2.22s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  13%|█▎        | 9/68 [00:24<02:41,  2.74s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  15%|█▍        | 10/68 [00:26<02:36,  2.70s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  15%|█▍        | 10/68 [00:28<02:38,  2.73s/ba][A[A[A[A








Running tokenizer on dataset #9:  12%|█▏        | 8/68 [00:23<02:49,  2.83s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  21%|██        | 14/68 [00:31<01:51,  2.06s/ba][A









Running tokenizer on dataset #10:  12%|█▏        | 8/68 [00:23<02:48,  2.82s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  21%|██        | 14/68 [00:32<01:54,  2.12s/ba]




Running tokenizer on dataset #5:  15%|█▍        | 10/68 [00:28<02:39,  2.74s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  21%|██        | 14/68 [00:30<01:49,  2.02s/ba][A[A[A

Running tokenizer on dataset #2:  16%|█▌        | 11/68 [00:31<02:34,  2.71s/ba][A[A





Running tokenizer on dataset #6:  16%|█▌        | 11/68 [00:28<02:22,  2.50s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  18%|█▊        | 12/68 [00:27<02:08,  2.29s/ba][A[A[A[A[A[A[A










Running tokenizer on dataset #11:  12%|█▏        | 8/68 [00:23<02:50,  2.84s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  15%|█▍        | 10/68 [00:27<02:37,  2.71s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  22%|██▏       | 15/68 [00:33<01:48,  2.05s/ba][A



Running tokenizer on dataset #4:  16%|█▌        | 11/68 [00:31<02:35,  2.73s/ba][A[A[A[A








Running tokenizer on dataset #9:  13%|█▎        | 9/68 [00:26<02:44,  2.79s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  22%|██▏       | 15/68 [00:34<01:50,  2.09s/ba]


Running tokenizer on dataset #3:  22%|██▏       | 15/68 [00:32<01:46,  2.02s/ba][A[A[A









Running tokenizer on dataset #10:  13%|█▎        | 9/68 [00:26<02:44,  2.79s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  16%|█▌        | 11/68 [00:31<02:36,  2.74s/ba][A[A[A[A[A





Running tokenizer on dataset #6:  18%|█▊        | 12/68 [00:30<02:11,  2.34s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  19%|█▉        | 13/68 [00:29<02:01,  2.21s/ba][A[A[A[A[A[A[A

Running tokenizer on dataset #2:  18%|█▊        | 12/68 [00:34<02:32,  2.73s/ba][A[A
Running tokenizer on dataset #1:  24%|██▎       | 16/68 [00:35<01:45,  2.02s/ba][A










Running tokenizer on dataset #11:  13%|█▎        | 9/68 [00:26<02:46,  2.81s/ba][A[A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  24%|██▎       | 16/68 [00:36<01:47,  2.06s/ba]







Running tokenizer on dataset #8:  16%|█▌        | 11/68 [00:29<02:32,  2.68s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  24%|██▎       | 16/68 [00:34<01:44,  2.01s/ba][A[A[A



Running tokenizer on dataset #4:  18%|█▊        | 12/68 [00:34<02:32,  2.72s/ba][A[A[A[A








Running tokenizer on dataset #9:  15%|█▍        | 10/68 [00:29<02:39,  2.75s/ba][A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  19%|█▉        | 13/68 [00:32<02:03,  2.25s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  21%|██        | 14/68 [00:31<01:56,  2.16s/ba][A[A[A[A[A[A[A









Running tokenizer on dataset #10:  15%|█▍        | 10/68 [00:28<02:38,  2.74s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  18%|█▊        | 12/68 [00:33<02:31,  2.71s/ba][A[A[A[A[A
Running tokenizer on dataset #1:  25%|██▌       | 17/68 [00:37<01:42,  2.01s/ba][A

Running tokenizer on dataset #2:  19%|█▉        | 13/68 [00:36<02:28,  2.69s/ba][A[ARunning tokenizer on dataset #0:  25%|██▌       | 17/68 [00:38<01:43,  2.04s/ba]


Running tokenizer on dataset #3:  25%|██▌       | 17/68 [00:36<01:42,  2.00s/ba][A[A[A










Running tokenizer on dataset #11:  15%|█▍        | 10/68 [00:29<02:40,  2.77s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  18%|█▊        | 12/68 [00:32<02:29,  2.66s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  21%|██        | 14/68 [00:34<01:57,  2.18s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  22%|██▏       | 15/68 [00:33<01:51,  2.11s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  19%|█▉        | 13/68 [00:36<02:28,  2.71s/ba][A[A[A[A








Running tokenizer on dataset #9:  16%|█▌        | 11/68 [00:31<02:35,  2.74s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  26%|██▋       | 18/68 [00:39<01:39,  2.00s/ba][A









Running tokenizer on dataset #10:  16%|█▌        | 11/68 [00:31<02:34,  2.71s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  26%|██▋       | 18/68 [00:40<01:40,  2.02s/ba]




Running tokenizer on dataset #5:  19%|█▉        | 13/68 [00:36<02:26,  2.67s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  26%|██▋       | 18/68 [00:38<01:39,  1.99s/ba][A[A[A

Running tokenizer on dataset #2:  21%|██        | 14/68 [00:39<02:23,  2.65s/ba][A[A





Running tokenizer on dataset #6:  22%|██▏       | 15/68 [00:36<01:53,  2.15s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  24%|██▎       | 16/68 [00:35<01:48,  2.09s/ba][A[A[A[A[A[A[A










Running tokenizer on dataset #11:  16%|█▌        | 11/68 [00:31<02:36,  2.75s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  19%|█▉        | 13/68 [00:34<02:25,  2.65s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  28%|██▊       | 19/68 [00:41<01:37,  1.99s/ba][A



Running tokenizer on dataset #4:  21%|██        | 14/68 [00:39<02:24,  2.67s/ba][A[A[A[ARunning tokenizer on dataset #0:  28%|██▊       | 19/68 [00:42<01:38,  2.00s/ba]








Running tokenizer on dataset #9:  18%|█▊        | 12/68 [00:34<02:32,  2.73s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  28%|██▊       | 19/68 [00:40<01:36,  1.98s/ba][A[A[A









Running tokenizer on dataset #10:  18%|█▊        | 12/68 [00:34<02:31,  2.70s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  21%|██        | 14/68 [00:39<02:23,  2.66s/ba][A[A[A[A[A





Running tokenizer on dataset #6:  24%|██▎       | 16/68 [00:38<01:49,  2.10s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  25%|██▌       | 17/68 [00:37<01:45,  2.06s/ba][A[A[A[A[A[A[A

Running tokenizer on dataset #2:  22%|██▏       | 15/68 [00:42<02:19,  2.64s/ba][A[A
Running tokenizer on dataset #1:  29%|██▉       | 20/68 [00:43<01:34,  1.98s/ba][A










Running tokenizer on dataset #11:  18%|█▊        | 12/68 [00:34<02:32,  2.72s/ba][A[A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  29%|██▉       | 20/68 [00:44<01:35,  2.00s/ba]







Running tokenizer on dataset #8:  21%|██        | 14/68 [00:37<02:22,  2.65s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  29%|██▉       | 20/68 [00:42<01:34,  1.97s/ba][A[A[A



Running tokenizer on dataset #4:  22%|██▏       | 15/68 [00:41<02:20,  2.65s/ba][A[A[A[A








Running tokenizer on dataset #9:  19%|█▉        | 13/68 [00:37<02:28,  2.70s/ba][A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  25%|██▌       | 17/68 [00:40<01:44,  2.06s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  26%|██▋       | 18/68 [00:39<01:42,  2.04s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  22%|██▏       | 15/68 [00:41<02:20,  2.65s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  19%|█▉        | 13/68 [00:36<02:28,  2.70s/ba][A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  31%|███       | 21/68 [00:45<01:32,  1.97s/ba][A

Running tokenizer on dataset #2:  24%|██▎       | 16/68 [00:44<02:16,  2.63s/ba][A[ARunning tokenizer on dataset #0:  31%|███       | 21/68 [00:46<01:33,  2.00s/ba]


Running tokenizer on dataset #3:  31%|███       | 21/68 [00:44<01:33,  1.99s/ba][A[A[A










Running tokenizer on dataset #11:  19%|█▉        | 13/68 [00:37<02:29,  2.71s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  22%|██▏       | 15/68 [00:40<02:19,  2.64s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  26%|██▋       | 18/68 [00:42<01:44,  2.09s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  28%|██▊       | 19/68 [00:41<01:39,  2.04s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  24%|██▎       | 16/68 [00:44<02:17,  2.65s/ba][A[A[A[A
Running tokenizer on dataset #1:  32%|███▏      | 22/68 [00:47<01:30,  1.97s/ba][A








Running tokenizer on dataset #9:  21%|██        | 14/68 [00:39<02:25,  2.70s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  32%|███▏      | 22/68 [00:48<01:31,  1.98s/ba]









Running tokenizer on dataset #10:  21%|██        | 14/68 [00:39<02:25,  2.70s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  24%|██▎       | 16/68 [00:44<02:20,  2.71s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  32%|███▏      | 22/68 [00:46<01:31,  1.99s/ba][A[A[A

Running tokenizer on dataset #2:  25%|██▌       | 17/68 [00:47<02:13,  2.62s/ba][A[A






Running tokenizer on dataset #7:  29%|██▉       | 20/68 [00:43<01:36,  2.01s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  24%|██▎       | 16/68 [00:42<02:16,  2.62s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  21%|██        | 14/68 [00:39<02:25,  2.70s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  34%|███▍      | 23/68 [00:49<01:28,  1.96s/ba][A



Running tokenizer on dataset #4:  25%|██▌       | 17/68 [00:47<02:13,  2.63s/ba][A[A[A[A





Running tokenizer on dataset #6:  28%|██▊       | 19/68 [00:45<01:50,  2.25s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  34%|███▍      | 23/68 [00:50<01:28,  1.96s/ba]








Running tokenizer on dataset #9:  22%|██▏       | 15/68 [00:42<02:22,  2.68s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  34%|███▍      | 23/68 [00:48<01:28,  1.97s/ba][A[A[A









Running tokenizer on dataset #10:  22%|██▏       | 15/68 [00:42<02:21,  2.67s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  25%|██▌       | 17/68 [00:47<02:16,  2.68s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  31%|███       | 21/68 [00:45<01:33,  2.00s/ba][A[A[A[A[A[A[A

Running tokenizer on dataset #2:  26%|██▋       | 18/68 [00:50<02:12,  2.64s/ba][A[A
Running tokenizer on dataset #1:  35%|███▌      | 24/68 [00:51<01:25,  1.95s/ba][ARunning tokenizer on dataset #0:  35%|███▌      | 24/68 [00:52<01:26,  1.96s/ba]







Running tokenizer on dataset #8:  25%|██▌       | 17/68 [00:45<02:13,  2.62s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  22%|██▏       | 15/68 [00:42<02:23,  2.70s/ba][A[A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  35%|███▌      | 24/68 [00:50<01:26,  1.96s/ba][A[A[A





Running tokenizer on dataset #6:  29%|██▉       | 20/68 [00:47<01:52,  2.35s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  26%|██▋       | 18/68 [00:49<02:11,  2.63s/ba][A[A[A[A








Running tokenizer on dataset #9:  24%|██▎       | 16/68 [00:45<02:19,  2.68s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  32%|███▏      | 22/68 [00:47<01:31,  2.00s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  37%|███▋      | 25/68 [00:53<01:23,  1.95s/ba][A









Running tokenizer on dataset #10:  24%|██▎       | 16/68 [00:44<02:18,  2.66s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  26%|██▋       | 18/68 [00:49<02:12,  2.64s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  28%|██▊       | 19/68 [00:52<02:08,  2.62s/ba][A[ARunning tokenizer on dataset #0:  37%|███▋      | 25/68 [00:54<01:24,  1.96s/ba]


Running tokenizer on dataset #3:  37%|███▋      | 25/68 [00:51<01:23,  1.94s/ba][A[A[A







Running tokenizer on dataset #8:  26%|██▋       | 18/68 [00:48<02:10,  2.60s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  24%|██▎       | 16/68 [00:45<02:18,  2.67s/ba][A[A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  31%|███       | 21/68 [00:50<01:53,  2.42s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  28%|██▊       | 19/68 [00:52<02:07,  2.61s/ba][A[A[A[A






Running tokenizer on dataset #7:  34%|███▍      | 23/68 [00:49<01:28,  1.97s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  38%|███▊      | 26/68 [00:55<01:21,  1.94s/ba][A








Running tokenizer on dataset #9:  25%|██▌       | 17/68 [00:47<02:15,  2.66s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  38%|███▊      | 26/68 [00:56<01:21,  1.95s/ba]









Running tokenizer on dataset #10:  25%|██▌       | 17/68 [00:47<02:14,  2.63s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  38%|███▊      | 26/68 [00:53<01:21,  1.95s/ba][A[A[A




Running tokenizer on dataset #5:  28%|██▊       | 19/68 [00:52<02:08,  2.63s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  29%|██▉       | 20/68 [00:55<02:05,  2.61s/ba][A[A






Running tokenizer on dataset #7:  35%|███▌      | 24/68 [00:51<01:26,  1.96s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  40%|███▉      | 27/68 [00:57<01:18,  1.93s/ba][A







Running tokenizer on dataset #8:  28%|██▊       | 19/68 [00:50<02:07,  2.61s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  25%|██▌       | 17/68 [00:47<02:15,  2.66s/ba][A[A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  32%|███▏      | 22/68 [00:53<01:53,  2.47s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  40%|███▉      | 27/68 [00:58<01:19,  1.94s/ba]



Running tokenizer on dataset #4:  29%|██▉       | 20/68 [00:54<02:05,  2.62s/ba][A[A[A[A


Running tokenizer on dataset #3:  40%|███▉      | 27/68 [00:55<01:19,  1.94s/ba][A[A[A








Running tokenizer on dataset #9:  26%|██▋       | 18/68 [00:50<02:13,  2.67s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  26%|██▋       | 18/68 [00:50<02:10,  2.61s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  29%|██▉       | 20/68 [00:54<02:06,  2.63s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  37%|███▋      | 25/68 [00:53<01:23,  1.94s/ba][A[A[A[A[A[A[A

Running tokenizer on dataset #2:  31%|███       | 21/68 [00:57<02:02,  2.60s/ba][A[A
Running tokenizer on dataset #1:  41%|████      | 28/68 [00:59<01:16,  1.92s/ba][ARunning tokenizer on dataset #0:  41%|████      | 28/68 [01:00<01:17,  1.94s/ba]







Running tokenizer on dataset #8:  29%|██▉       | 20/68 [00:53<02:05,  2.60s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  41%|████      | 28/68 [00:57<01:17,  1.93s/ba][A[A[A










Running tokenizer on dataset #11:  26%|██▋       | 18/68 [00:50<02:13,  2.67s/ba][A[A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  34%|███▍      | 23/68 [00:55<01:52,  2.50s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  31%|███       | 21/68 [00:57<02:02,  2.60s/ba][A[A[A[A








Running tokenizer on dataset #9:  28%|██▊       | 19/68 [00:53<02:10,  2.66s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  38%|███▊      | 26/68 [00:55<01:21,  1.94s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  43%|████▎     | 29/68 [01:01<01:15,  1.93s/ba][A









Running tokenizer on dataset #10:  28%|██▊       | 19/68 [00:52<02:07,  2.61s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  31%|███       | 21/68 [00:57<02:02,  2.60s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  32%|███▏      | 22/68 [01:00<01:58,  2.57s/ba][A[ARunning tokenizer on dataset #0:  43%|████▎     | 29/68 [01:02<01:16,  1.95s/ba]


Running tokenizer on dataset #3:  43%|████▎     | 29/68 [00:59<01:18,  2.02s/ba][A[A[A





Running tokenizer on dataset #6:  35%|███▌      | 24/68 [00:57<01:42,  2.33s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  31%|███       | 21/68 [00:55<02:02,  2.60s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  28%|██▊       | 19/68 [00:53<02:10,  2.66s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  32%|███▏      | 22/68 [01:00<01:59,  2.60s/ba][A[A[A[A






Running tokenizer on dataset #7:  40%|███▉      | 27/68 [00:57<01:20,  1.96s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  44%|████▍     | 30/68 [01:02<01:13,  1.93s/ba][A








Running tokenizer on dataset #9:  29%|██▉       | 20/68 [00:55<01:59,  2.50s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  44%|████▍     | 30/68 [01:04<01:14,  1.95s/ba]




Running tokenizer on dataset #5:  32%|███▏      | 22/68 [01:00<01:57,  2.56s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  29%|██▉       | 20/68 [00:55<02:05,  2.62s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  34%|███▍      | 23/68 [01:02<01:55,  2.57s/ba][A[A





Running tokenizer on dataset #6:  37%|███▋      | 25/68 [00:59<01:35,  2.21s/ba][A[A[A[A[A[A


Running tokenizer on dataset #3:  44%|████▍     | 30/68 [01:02<01:22,  2.18s/ba][A[A[A






Running tokenizer on dataset #7:  41%|████      | 28/68 [00:58<01:18,  1.95s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  46%|████▌     | 31/68 [01:04<01:11,  1.93s/ba][A







Running tokenizer on dataset #8:  32%|███▏      | 22/68 [00:58<01:59,  2.59s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  31%|███       | 21/68 [00:57<01:49,  2.34s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  29%|██▉       | 20/68 [00:55<02:06,  2.64s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  34%|███▍      | 23/68 [01:02<01:55,  2.57s/ba][A[A[A[ARunning tokenizer on dataset #0:  46%|████▌     | 31/68 [01:06<01:11,  1.94s/ba]





Running tokenizer on dataset #6:  38%|███▊      | 26/68 [01:01<01:30,  2.15s/ba][A[A[A[A[A[A




Running tokenizer on dataset #5:  34%|███▍      | 23/68 [01:02<01:55,  2.56s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  31%|███       | 21/68 [00:57<02:01,  2.59s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  35%|███▌      | 24/68 [01:05<01:53,  2.57s/ba][A[A






Running tokenizer on dataset #7:  43%|████▎     | 29/68 [01:00<01:15,  1.95s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  47%|████▋     | 32/68 [01:06<01:09,  1.92s/ba][A







Running tokenizer on dataset #8:  34%|███▍      | 23/68 [01:00<01:48,  2.41s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  32%|███▏      | 22/68 [00:59<01:42,  2.24s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  46%|████▌     | 31/68 [01:05<01:24,  2.29s/ba][A[A[ARunning tokenizer on dataset #0:  47%|████▋     | 32/68 [01:08<01:10,  1.96s/ba]










Running tokenizer on dataset #11:  31%|███       | 21/68 [00:58<02:03,  2.63s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  35%|███▌      | 24/68 [01:05<01:52,  2.56s/ba][A[A[A[A





Running tokenizer on dataset #6:  40%|███▉      | 27/68 [01:03<01:24,  2.07s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  44%|████▍     | 30/68 [01:02<01:14,  1.95s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  35%|███▌      | 24/68 [01:05<01:51,  2.54s/ba][A[A[A[A[A
Running tokenizer on dataset #1:  49%|████▊     | 33/68 [01:08<01:08,  1.96s/ba][A









Running tokenizer on dataset #10:  32%|███▏      | 22/68 [01:00<01:57,  2.56s/ba][A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  34%|███▍      | 23/68 [01:01<01:36,  2.14s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  35%|███▌      | 24/68 [01:02<01:41,  2.30s/ba][A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  37%|███▋      | 25/68 [01:07<01:50,  2.56s/ba][A[ARunning tokenizer on dataset #0:  49%|████▊     | 33/68 [01:10<01:08,  1.96s/ba]


Running tokenizer on dataset #3:  47%|████▋     | 32/68 [01:07<01:25,  2.36s/ba][A[A[A





Running tokenizer on dataset #6:  41%|████      | 28/68 [01:05<01:20,  2.02s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  32%|███▏      | 22/68 [01:00<02:01,  2.64s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  37%|███▋      | 25/68 [01:07<01:50,  2.57s/ba][A[A[A[A






Running tokenizer on dataset #7:  46%|████▌     | 31/68 [01:04<01:12,  1.96s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  35%|███▌      | 24/68 [01:03<01:31,  2.07s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  37%|███▋      | 25/68 [01:04<01:34,  2.19s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  50%|█████     | 34/68 [01:11<01:11,  2.09s/ba][A




Running tokenizer on dataset #5:  37%|███▋      | 25/68 [01:07<01:48,  2.53s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  50%|█████     | 34/68 [01:12<01:05,  1.94s/ba]









Running tokenizer on dataset #10:  34%|███▍      | 23/68 [01:02<01:56,  2.60s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  38%|███▊      | 26/68 [01:10<01:47,  2.55s/ba][A[A





Running tokenizer on dataset #6:  43%|████▎     | 29/68 [01:07<01:17,  1.99s/ba][A[A[A[A[A[A


Running tokenizer on dataset #3:  49%|████▊     | 33/68 [01:10<01:24,  2.41s/ba][A[A[A






Running tokenizer on dataset #7:  47%|████▋     | 32/68 [01:06<01:09,  1.94s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  37%|███▋      | 25/68 [01:04<01:26,  2.01s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  38%|███▊      | 26/68 [01:06<01:29,  2.13s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  34%|███▍      | 23/68 [01:03<01:57,  2.61s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  38%|███▊      | 26/68 [01:10<01:49,  2.60s/ba][A[A[A[ARunning tokenizer on dataset #0:  51%|█████▏    | 35/68 [01:14<01:03,  1.93s/ba]





Running tokenizer on dataset #6:  44%|████▍     | 30/68 [01:09<01:14,  1.96s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  51%|█████▏    | 35/68 [01:13<01:13,  2.22s/ba][A




Running tokenizer on dataset #5:  38%|███▊      | 26/68 [01:10<01:46,  2.53s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  35%|███▌      | 24/68 [01:05<01:53,  2.58s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  40%|███▉      | 27/68 [01:13<01:44,  2.56s/ba][A[A






Running tokenizer on dataset #7:  49%|████▊     | 33/68 [01:08<01:07,  1.94s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  38%|███▊      | 26/68 [01:06<01:23,  1.99s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  50%|█████     | 34/68 [01:12<01:23,  2.46s/ba][A[A[A







Running tokenizer on dataset #8:  40%|███▉      | 27/68 [01:08<01:25,  2.10s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  53%|█████▎    | 36/68 [01:15<01:01,  1.91s/ba]





Running tokenizer on dataset #6:  46%|████▌     | 31/68 [01:10<01:11,  1.93s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  40%|███▉      | 27/68 [01:12<01:45,  2.58s/ba][A[A[A[A
Running tokenizer on dataset #1:  53%|█████▎    | 36/68 [01:16<01:13,  2.29s/ba][A




Running tokenizer on dataset #5:  40%|███▉      | 27/68 [01:12<01:45,  2.56s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  50%|█████     | 34/68 [01:10<01:05,  1.93s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  40%|███▉      | 27/68 [01:08<01:21,  1.98s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  37%|███▋      | 25/68 [01:08<01:51,  2.59s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  41%|████      | 28/68 [01:15<01:42,  2.55s/ba][A[A







Running tokenizer on dataset #8:  41%|████      | 28/68 [01:10<01:22,  2.06s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  35%|███▌      | 24/68 [01:07<02:11,  2.99s/ba][A[A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  54%|█████▍    | 37/68 [01:17<00:59,  1.93s/ba]


Running tokenizer on dataset #3:  51%|█████▏    | 35/68 [01:15<01:21,  2.47s/ba][A[A[A





Running tokenizer on dataset #6:  47%|████▋     | 32/68 [01:12<01:09,  1.94s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  41%|████      | 28/68 [01:15<01:42,  2.57s/ba][A[A[A[A






Running tokenizer on dataset #7:  51%|█████▏    | 35/68 [01:12<01:04,  1.95s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  41%|████      | 28/68 [01:10<01:19,  1.98s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  54%|█████▍    | 37/68 [01:18<01:12,  2.35s/ba][A







Running tokenizer on dataset #8:  43%|████▎     | 29/68 [01:12<01:19,  2.04s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  56%|█████▌    | 38/68 [01:19<00:57,  1.91s/ba]




Running tokenizer on dataset #5:  41%|████      | 28/68 [01:15<01:42,  2.56s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  38%|███▊      | 26/68 [01:10<01:47,  2.57s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  49%|████▊     | 33/68 [01:14<01:07,  1.92s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  37%|███▋      | 25/68 [01:09<02:03,  2.86s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  43%|████▎     | 29/68 [01:18<01:40,  2.59s/ba][A[A


Running tokenizer on dataset #3:  53%|█████▎    | 36/68 [01:17<01:19,  2.49s/ba][A[A[A






Running tokenizer on dataset #7:  53%|█████▎    | 36/68 [01:14<01:02,  1.95s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  43%|████▎     | 29/68 [01:12<01:17,  1.98s/ba][A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  43%|████▎     | 29/68 [01:18<01:39,  2.56s/ba][A[A[A[A







Running tokenizer on dataset #8:  44%|████▍     | 30/68 [01:14<01:16,  2.02s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  57%|█████▋    | 39/68 [01:21<00:56,  1.93s/ba]
Running tokenizer on dataset #1:  56%|█████▌    | 38/68 [01:21<01:12,  2.40s/ba][A





Running tokenizer on dataset #6:  50%|█████     | 34/68 [01:16<01:05,  1.93s/ba][A[A[A[A[A[A




Running tokenizer on dataset #5:  43%|████▎     | 29/68 [01:17<01:38,  2.53s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  40%|███▉      | 27/68 [01:13<01:44,  2.55s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  38%|███▊      | 26/68 [01:12<01:56,  2.77s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  44%|████▍     | 30/68 [01:20<01:38,  2.59s/ba][A[A






Running tokenizer on dataset #7:  54%|█████▍    | 37/68 [01:16<01:00,  1.94s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  54%|█████▍    | 37/68 [01:20<01:17,  2.50s/ba][A[A[A








Running tokenizer on dataset #9:  44%|████▍     | 30/68 [01:14<01:14,  1.97s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  46%|████▌     | 31/68 [01:16<01:14,  2.00s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  59%|█████▉    | 40/68 [01:23<00:54,  1.93s/ba]





Running tokenizer on dataset #6:  51%|█████▏    | 35/68 [01:18<01:03,  1.92s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  44%|████▍     | 30/68 [01:20<01:36,  2.55s/ba][A[A[A[A
Running tokenizer on dataset #1:  57%|█████▋    | 39/68 [01:23<01:10,  2.42s/ba][A




Running tokenizer on dataset #5:  44%|████▍     | 30/68 [01:20<01:35,  2.52s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  56%|█████▌    | 38/68 [01:18<00:58,  1.95s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  46%|████▌     | 31/68 [01:16<01:11,  1.94s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  41%|████      | 28/68 [01:15<01:42,  2.57s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  40%|███▉      | 27/68 [01:14<01:50,  2.69s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  46%|████▌     | 31/68 [01:23<01:35,  2.57s/ba][A[A







Running tokenizer on dataset #8:  47%|████▋     | 32/68 [01:18<01:10,  1.97s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  56%|█████▌    | 38/68 [01:22<01:14,  2.49s/ba][A[A[ARunning tokenizer on dataset #0:  60%|██████    | 41/68 [01:25<00:52,  1.94s/ba]





Running tokenizer on dataset #6:  53%|█████▎    | 36/68 [01:20<01:00,  1.90s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  46%|████▌     | 31/68 [01:22<01:32,  2.51s/ba][A[A[A[A






Running tokenizer on dataset #7:  57%|█████▋    | 39/68 [01:20<00:55,  1.93s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  59%|█████▉    | 40/68 [01:26<01:08,  2.44s/ba][A








Running tokenizer on dataset #9:  47%|████▋     | 32/68 [01:18<01:09,  1.93s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  46%|████▌     | 31/68 [01:22<01:33,  2.53s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  49%|████▊     | 33/68 [01:20<01:08,  1.95s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  62%|██████▏   | 42/68 [01:27<00:50,  1.94s/ba]









Running tokenizer on dataset #10:  43%|████▎     | 29/68 [01:18<01:40,  2.56s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  54%|█████▍    | 37/68 [01:22<00:58,  1.89s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  41%|████      | 28/68 [01:17<01:46,  2.65s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  47%|████▋     | 32/68 [01:25<01:31,  2.55s/ba][A[A


Running tokenizer on dataset #3:  57%|█████▋    | 39/68 [01:25<01:11,  2.46s/ba][A[A[A








Running tokenizer on dataset #9:  49%|████▊     | 33/68 [01:20<01:06,  1.91s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  59%|█████▉    | 40/68 [01:22<00:54,  1.94s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  47%|████▋     | 32/68 [01:25<01:31,  2.54s/ba][A[A[A[A







Running tokenizer on dataset #8:  50%|█████     | 34/68 [01:22<01:06,  1.97s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  60%|██████    | 41/68 [01:28<01:06,  2.46s/ba][ARunning tokenizer on dataset #0:  63%|██████▎   | 43/68 [01:29<00:48,  1.94s/ba]





Running tokenizer on dataset #6:  56%|█████▌    | 38/68 [01:24<00:56,  1.88s/ba][A[A[A[A[A[A




Running tokenizer on dataset #5:  47%|████▋     | 32/68 [01:25<01:30,  2.53s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  44%|████▍     | 30/68 [01:20<01:37,  2.56s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  43%|████▎     | 29/68 [01:20<01:42,  2.62s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  49%|████▊     | 33/68 [01:28<01:29,  2.55s/ba][A[A


Running tokenizer on dataset #3:  59%|█████▉    | 40/68 [01:27<01:09,  2.49s/ba][A[A[A








Running tokenizer on dataset #9:  50%|█████     | 34/68 [01:22<01:04,  1.91s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  60%|██████    | 41/68 [01:24<00:52,  1.94s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  51%|█████▏    | 35/68 [01:24<01:04,  1.97s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  57%|█████▋    | 39/68 [01:26<00:54,  1.88s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  65%|██████▍   | 44/68 [01:31<00:47,  1.96s/ba]



Running tokenizer on dataset #4:  49%|████▊     | 33/68 [01:28<01:28,  2.54s/ba][A[A[A[A
Running tokenizer on dataset #1:  62%|██████▏   | 42/68 [01:31<01:03,  2.46s/ba][A




Running tokenizer on dataset #5:  49%|████▊     | 33/68 [01:27<01:27,  2.50s/ba][A[A[A[A[A








Running tokenizer on dataset #9:  51%|█████▏    | 35/68 [01:24<01:02,  1.90s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  46%|████▌     | 31/68 [01:23<01:34,  2.55s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  62%|██████▏   | 42/68 [01:26<00:50,  1.94s/ba][A[A[A[A[A[A[A

Running tokenizer on dataset #2:  50%|█████     | 34/68 [01:30<01:25,  2.53s/ba][A[A


Running tokenizer on dataset #3:  60%|██████    | 41/68 [01:30<01:07,  2.49s/ba][A[A[A










Running tokenizer on dataset #11:  44%|████▍     | 30/68 [01:22<01:39,  2.63s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  53%|█████▎    | 36/68 [01:25<01:02,  1.96s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  59%|█████▉    | 40/68 [01:27<00:52,  1.88s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  66%|██████▌   | 45/68 [01:33<00:44,  1.95s/ba]



Running tokenizer on dataset #4:  50%|█████     | 34/68 [01:30<01:26,  2.54s/ba][A[A[A[A
Running tokenizer on dataset #1:  63%|██████▎   | 43/68 [01:33<01:01,  2.46s/ba][A








Running tokenizer on dataset #9:  53%|█████▎    | 36/68 [01:26<01:00,  1.90s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  63%|██████▎   | 43/68 [01:28<00:48,  1.95s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  50%|█████     | 34/68 [01:30<01:25,  2.51s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  54%|█████▍    | 37/68 [01:27<01:00,  1.95s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  60%|██████    | 41/68 [01:29<00:50,  1.88s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  47%|████▋     | 32/68 [01:25<01:31,  2.55s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  51%|█████▏    | 35/68 [01:33<01:22,  2.51s/ba][A[ARunning tokenizer on dataset #0:  68%|██████▊   | 46/68 [01:35<00:42,  1.95s/ba]


Running tokenizer on dataset #3:  62%|██████▏   | 42/68 [01:32<01:04,  2.49s/ba][A[A[A










Running tokenizer on dataset #11:  46%|████▌     | 31/68 [01:25<01:36,  2.62s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  54%|█████▍    | 37/68 [01:27<00:58,  1.89s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  65%|██████▍   | 44/68 [01:30<00:46,  1.94s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  51%|█████▏    | 35/68 [01:33<01:23,  2.53s/ba][A[A[A[A
Running tokenizer on dataset #1:  65%|██████▍   | 44/68 [01:36<00:59,  2.48s/ba][A





Running tokenizer on dataset #6:  62%|██████▏   | 42/68 [01:31<00:48,  1.87s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  56%|█████▌    | 38/68 [01:29<00:58,  1.95s/ba][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  51%|█████▏    | 35/68 [01:32<01:22,  2.50s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  69%|██████▉   | 47/68 [01:37<00:40,  1.93s/ba]









Running tokenizer on dataset #10:  49%|████▊     | 33/68 [01:28<01:28,  2.52s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  53%|█████▎    | 36/68 [01:35<01:20,  2.50s/ba][A[A


Running tokenizer on dataset #3:  63%|██████▎   | 43/68 [01:35<01:02,  2.49s/ba][A[A[A










Running tokenizer on dataset #11:  47%|████▋     | 32/68 [01:27<01:33,  2.59s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  56%|█████▌    | 38/68 [01:29<00:56,  1.88s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  66%|██████▌   | 45/68 [01:32<00:44,  1.94s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  63%|██████▎   | 43/68 [01:33<00:47,  1.88s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  57%|█████▋    | 39/68 [01:31<00:56,  1.94s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  71%|███████   | 48/68 [01:39<00:38,  1.91s/ba]



Running tokenizer on dataset #4:  53%|█████▎    | 36/68 [01:35<01:20,  2.52s/ba][A[A[A[A
Running tokenizer on dataset #1:  66%|██████▌   | 45/68 [01:38<00:56,  2.46s/ba][A




Running tokenizer on dataset #5:  53%|█████▎    | 36/68 [01:35<01:20,  2.51s/ba][A[A[A[A[A








Running tokenizer on dataset #9:  57%|█████▋    | 39/68 [01:31<00:54,  1.89s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  50%|█████     | 34/68 [01:30<01:26,  2.54s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  68%|██████▊   | 46/68 [01:33<00:42,  1.92s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  65%|██████▍   | 44/68 [01:37<00:59,  2.49s/ba][A[A[A

Running tokenizer on dataset #2:  54%|█████▍    | 37/68 [01:38<01:18,  2.55s/ba][A[A










Running tokenizer on dataset #11:  49%|████▊     | 33/68 [01:30<01:30,  2.58s/ba][A[A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  65%|██████▍   | 44/68 [01:35<00:45,  1.88s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  59%|█████▉    | 40/68 [01:33<00:53,  1.93s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  72%|███████▏  | 49/68 [01:41<00:36,  1.91s/ba]



Running tokenizer on dataset #4:  54%|█████▍    | 37/68 [01:38<01:18,  2.52s/ba][A[A[A[A
Running tokenizer on dataset #1:  68%|██████▊   | 46/68 [01:41<00:54,  2.47s/ba][A








Running tokenizer on dataset #9:  59%|█████▉    | 40/68 [01:33<00:52,  1.89s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  54%|█████▍    | 37/68 [01:37<01:18,  2.52s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  69%|██████▉   | 47/68 [01:35<00:40,  1.92s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  66%|██████▌   | 45/68 [01:37<00:43,  1.88s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  51%|█████▏    | 35/68 [01:33<01:23,  2.52s/ba][A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  60%|██████    | 41/68 [01:35<00:51,  1.92s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  74%|███████▎  | 50/68 [01:42<00:34,  1.91s/ba]


Running tokenizer on dataset #3:  66%|██████▌   | 45/68 [01:40<00:57,  2.48s/ba][A[A[A

Running tokenizer on dataset #2:  56%|█████▌    | 38/68 [01:41<01:16,  2.55s/ba][A[A










Running tokenizer on dataset #11:  50%|█████     | 34/68 [01:32<01:27,  2.56s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  60%|██████    | 41/68 [01:35<00:51,  1.90s/ba][A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  56%|█████▌    | 38/68 [01:40<01:15,  2.51s/ba][A[A[A[A






Running tokenizer on dataset #7:  71%|███████   | 48/68 [01:37<00:38,  1.91s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  69%|██████▉   | 47/68 [01:43<00:52,  2.48s/ba][A





Running tokenizer on dataset #6:  68%|██████▊   | 46/68 [01:39<00:41,  1.87s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  62%|██████▏   | 42/68 [01:37<00:49,  1.92s/ba][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  56%|█████▌    | 38/68 [01:40<01:15,  2.52s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  75%|███████▌  | 51/68 [01:44<00:32,  1.91s/ba]









Running tokenizer on dataset #10:  53%|█████▎    | 36/68 [01:35<01:20,  2.53s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  68%|██████▊   | 46/68 [01:42<00:54,  2.49s/ba][A[A[A

Running tokenizer on dataset #2:  57%|█████▋    | 39/68 [01:43<01:13,  2.54s/ba][A[A








Running tokenizer on dataset #9:  62%|██████▏   | 42/68 [01:37<00:49,  1.90s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  51%|█████▏    | 35/68 [01:35<01:26,  2.62s/ba][A[A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  72%|███████▏  | 49/68 [01:39<00:36,  1.91s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  69%|██████▉   | 47/68 [01:41<00:39,  1.88s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  57%|█████▋    | 39/68 [01:43<01:11,  2.48s/ba][A[A[A[A







Running tokenizer on dataset #8:  63%|██████▎   | 43/68 [01:39<00:48,  1.94s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  71%|███████   | 48/68 [01:45<00:49,  2.48s/ba][ARunning tokenizer on dataset #0:  76%|███████▋  | 52/68 [01:46<00:30,  1.93s/ba]




Running tokenizer on dataset #5:  57%|█████▋    | 39/68 [01:42<01:12,  2.50s/ba][A[A[A[A[A








Running tokenizer on dataset #9:  63%|██████▎   | 43/68 [01:39<00:47,  1.90s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  54%|█████▍    | 37/68 [01:38<01:18,  2.53s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  69%|██████▉   | 47/68 [01:45<00:52,  2.50s/ba][A[A[A

Running tokenizer on dataset #2:  59%|█████▉    | 40/68 [01:46<01:10,  2.51s/ba][A[A






Running tokenizer on dataset #7:  74%|███████▎  | 50/68 [01:41<00:34,  1.90s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  71%|███████   | 48/68 [01:42<00:37,  1.88s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  53%|█████▎    | 36/68 [01:38<01:23,  2.61s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  65%|██████▍   | 44/68 [01:41<00:46,  1.93s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  78%|███████▊  | 53/68 [01:48<00:28,  1.92s/ba]
Running tokenizer on dataset #1:  72%|███████▏  | 49/68 [01:48<00:47,  2.48s/ba][A



Running tokenizer on dataset #4:  59%|█████▉    | 40/68 [01:45<01:10,  2.53s/ba][A[A[A[A








Running tokenizer on dataset #9:  65%|██████▍   | 44/68 [01:41<00:45,  1.89s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  59%|█████▉    | 40/68 [01:45<01:09,  2.48s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  75%|███████▌  | 51/68 [01:43<00:32,  1.89s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  72%|███████▏  | 49/68 [01:44<00:35,  1.89s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  56%|█████▌    | 38/68 [01:40<01:15,  2.50s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  71%|███████   | 48/68 [01:47<00:50,  2.50s/ba][A[A[A

Running tokenizer on dataset #2:  60%|██████    | 41/68 [01:48<01:07,  2.51s/ba][A[A







Running tokenizer on dataset #8:  66%|██████▌   | 45/68 [01:43<00:44,  1.93s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  79%|███████▉  | 54/68 [01:50<00:26,  1.92s/ba]










Running tokenizer on dataset #11:  54%|█████▍    | 37/68 [01:40<01:20,  2.60s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  66%|██████▌   | 45/68 [01:43<00:43,  1.89s/ba][A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  60%|██████    | 41/68 [01:48<01:07,  2.50s/ba][A[A[A[A
Running tokenizer on dataset #1:  74%|███████▎  | 50/68 [01:51<00:45,  2.51s/ba][A






Running tokenizer on dataset #7:  76%|███████▋  | 52/68 [01:45<00:30,  1.91s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  74%|███████▎  | 50/68 [01:46<00:33,  1.88s/ba][A[A[A[A[A[A




Running tokenizer on dataset #5:  60%|██████    | 41/68 [01:47<01:07,  2.49s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  68%|██████▊   | 46/68 [01:45<00:42,  1.93s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  81%|████████  | 55/68 [01:52<00:24,  1.92s/ba]









Running tokenizer on dataset #10:  57%|█████▋    | 39/68 [01:43<01:13,  2.52s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  72%|███████▏  | 49/68 [01:50<00:47,  2.51s/ba][A[A[A

Running tokenizer on dataset #2:  62%|██████▏   | 42/68 [01:51<01:05,  2.52s/ba][A[A










Running tokenizer on dataset #11:  56%|█████▌    | 38/68 [01:43<01:17,  2.58s/ba][A[A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  78%|███████▊  | 53/68 [01:47<00:28,  1.92s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  75%|███████▌  | 51/68 [01:48<00:31,  1.85s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  68%|██████▊   | 46/68 [01:45<00:46,  2.10s/ba][A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  62%|██████▏   | 42/68 [01:50<01:04,  2.49s/ba][A[A[A[A
Running tokenizer on dataset #1:  75%|███████▌  | 51/68 [01:53<00:42,  2.47s/ba][A







Running tokenizer on dataset #8:  69%|██████▉   | 47/68 [01:47<00:40,  1.92s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  82%|████████▏ | 56/68 [01:54<00:23,  1.92s/ba]




Running tokenizer on dataset #5:  62%|██████▏   | 42/68 [01:50<01:04,  2.49s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  63%|██████▎   | 43/68 [01:53<01:00,  2.42s/ba][A[A









Running tokenizer on dataset #10:  59%|█████▉    | 40/68 [01:46<01:10,  2.51s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  74%|███████▎  | 50/68 [01:52<00:45,  2.51s/ba][A[A[A






Running tokenizer on dataset #7:  79%|███████▉  | 54/68 [01:49<00:26,  1.91s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  76%|███████▋  | 52/68 [01:50<00:29,  1.85s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  57%|█████▋    | 39/68 [01:45<01:14,  2.59s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  69%|██████▉   | 47/68 [01:47<00:44,  2.12s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  71%|███████   | 48/68 [01:49<00:38,  1.93s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  84%|████████▍ | 57/68 [01:56<00:21,  1.94s/ba]
Running tokenizer on dataset #1:  76%|███████▋  | 52/68 [01:55<00:39,  2.45s/ba][A



Running tokenizer on dataset #4:  63%|██████▎   | 43/68 [01:53<01:02,  2.50s/ba][A[A[A[A




Running tokenizer on dataset #5:  63%|██████▎   | 43/68 [01:52<01:01,  2.47s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  65%|██████▍   | 44/68 [01:55<00:54,  2.27s/ba][A[A






Running tokenizer on dataset #7:  81%|████████  | 55/68 [01:51<00:24,  1.91s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  78%|███████▊  | 53/68 [01:52<00:27,  1.86s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  60%|██████    | 41/68 [01:48<01:08,  2.52s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  75%|███████▌  | 51/68 [01:55<00:42,  2.51s/ba][A[A[A








Running tokenizer on dataset #9:  71%|███████   | 48/68 [01:49<00:40,  2.04s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  72%|███████▏  | 49/68 [01:51<00:36,  1.93s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  85%|████████▌ | 58/68 [01:58<00:19,  1.93s/ba]










Running tokenizer on dataset #11:  59%|█████▉    | 40/68 [01:48<01:12,  2.58s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  78%|███████▊  | 53/68 [01:58<00:36,  2.46s/ba][A

Running tokenizer on dataset #2:  66%|██████▌   | 45/68 [01:57<00:50,  2.18s/ba][A[A



Running tokenizer on dataset #4:  65%|██████▍   | 44/68 [01:55<01:00,  2.50s/ba][A[A[A[A





Running tokenizer on dataset #6:  79%|███████▉  | 54/68 [01:54<00:26,  1.86s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  82%|████████▏ | 56/68 [01:52<00:22,  1.91s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  65%|██████▍   | 44/68 [01:55<00:58,  2.45s/ba][A[A[A[A[A








Running tokenizer on dataset #9:  72%|███████▏  | 49/68 [01:51<00:37,  1.98s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  74%|███████▎  | 50/68 [01:52<00:34,  1.92s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  87%|████████▋ | 59/68 [02:00<00:17,  1.92s/ba]









Running tokenizer on dataset #10:  62%|██████▏   | 42/68 [01:51<01:05,  2.53s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  76%|███████▋  | 52/68 [01:57<00:39,  2.49s/ba][A[A[A

Running tokenizer on dataset #2:  68%|██████▊   | 46/68 [01:59<00:46,  2.09s/ba][A[A










Running tokenizer on dataset #11:  60%|██████    | 41/68 [01:51<01:08,  2.55s/ba][A[A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  81%|████████  | 55/68 [01:55<00:24,  1.86s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  84%|████████▍ | 57/68 [01:54<00:21,  1.93s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  79%|███████▉  | 54/68 [02:00<00:34,  2.45s/ba][A



Running tokenizer on dataset #4:  66%|██████▌   | 45/68 [01:58<00:57,  2.48s/ba][A[A[A[A








Running tokenizer on dataset #9:  74%|███████▎  | 50/68 [01:53<00:35,  1.95s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  66%|██████▌   | 45/68 [01:57<00:56,  2.45s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  75%|███████▌  | 51/68 [01:54<00:32,  1.92s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  88%|████████▊ | 60/68 [02:02<00:15,  1.91s/ba]


Running tokenizer on dataset #3:  78%|███████▊  | 53/68 [01:59<00:36,  2.46s/ba][A[A[A









Running tokenizer on dataset #10:  63%|██████▎   | 43/68 [01:53<01:03,  2.53s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  69%|██████▉   | 47/68 [02:01<00:43,  2.05s/ba][A[A





Running tokenizer on dataset #6:  82%|████████▏ | 56/68 [01:57<00:22,  1.89s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  85%|████████▌ | 58/68 [01:56<00:19,  1.93s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  75%|███████▌  | 51/68 [01:55<00:32,  1.92s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  62%|██████▏   | 42/68 [01:53<01:05,  2.53s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  81%|████████  | 55/68 [02:03<00:31,  2.45s/ba][A







Running tokenizer on dataset #8:  76%|███████▋  | 52/68 [01:56<00:30,  1.93s/ba][A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  68%|██████▊   | 46/68 [02:00<00:54,  2.47s/ba][A[A[A[ARunning tokenizer on dataset #0:  90%|████████▉ | 61/68 [02:04<00:13,  1.91s/ba]




Running tokenizer on dataset #5:  68%|██████▊   | 46/68 [02:00<00:54,  2.47s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  71%|███████   | 48/68 [02:02<00:40,  2.01s/ba][A[A





Running tokenizer on dataset #6:  84%|████████▍ | 57/68 [01:59<00:20,  1.89s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  87%|████████▋ | 59/68 [01:58<00:17,  1.91s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  79%|███████▉  | 54/68 [02:02<00:34,  2.45s/ba][A[A[A









Running tokenizer on dataset #10:  65%|██████▍   | 44/68 [01:56<00:59,  2.50s/ba][A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  76%|███████▋  | 52/68 [01:57<00:30,  1.91s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  78%|███████▊  | 53/68 [01:58<00:28,  1.91s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  91%|█████████ | 62/68 [02:05<00:11,  1.92s/ba]










Running tokenizer on dataset #11:  63%|██████▎   | 43/68 [01:55<01:02,  2.52s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  82%|████████▏ | 56/68 [02:05<00:29,  2.45s/ba][A



Running tokenizer on dataset #4:  69%|██████▉   | 47/68 [02:02<00:51,  2.46s/ba][A[A[A[A

Running tokenizer on dataset #2:  72%|███████▏  | 49/68 [02:04<00:37,  1.97s/ba][A[A




Running tokenizer on dataset #5:  69%|██████▉   | 47/68 [02:02<00:51,  2.47s/ba][A[A[A[A[A





Running tokenizer on dataset #6:  85%|████████▌ | 58/68 [02:01<00:18,  1.89s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  88%|████████▊ | 60/68 [02:00<00:15,  1.90s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  78%|███████▊  | 53/68 [01:59<00:28,  1.93s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  81%|████████  | 55/68 [02:04<00:31,  2.46s/ba][A[A[A







Running tokenizer on dataset #8:  79%|███████▉  | 54/68 [02:00<00:26,  1.90s/ba][A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  66%|██████▌   | 45/68 [01:58<00:57,  2.50s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  93%|█████████▎| 63/68 [02:08<00:09,  1.96s/ba]

Running tokenizer on dataset #2:  74%|███████▎  | 50/68 [02:06<00:35,  1.95s/ba][A[A










Running tokenizer on dataset #11:  65%|██████▍   | 44/68 [01:58<01:00,  2.51s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  84%|████████▍ | 57/68 [02:08<00:26,  2.45s/ba][A



Running tokenizer on dataset #4:  71%|███████   | 48/68 [02:05<00:49,  2.45s/ba][A[A[A[A





Running tokenizer on dataset #6:  87%|████████▋ | 59/68 [02:03<00:16,  1.88s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  90%|████████▉ | 61/68 [02:02<00:13,  1.91s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  79%|███████▉  | 54/68 [02:00<00:26,  1.90s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  71%|███████   | 48/68 [02:04<00:49,  2.47s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  81%|████████  | 55/68 [02:02<00:24,  1.90s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  82%|████████▏ | 56/68 [02:07<00:29,  2.46s/ba][A[A[A









Running tokenizer on dataset #10:  68%|██████▊   | 46/68 [02:01<00:54,  2.48s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  94%|█████████▍| 64/68 [02:10<00:08,  2.10s/ba]

Running tokenizer on dataset #2:  75%|███████▌  | 51/68 [02:08<00:32,  1.93s/ba][A[A





Running tokenizer on dataset #6:  88%|████████▊ | 60/68 [02:05<00:15,  1.88s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  91%|█████████ | 62/68 [02:04<00:11,  1.89s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  81%|████████  | 55/68 [02:02<00:24,  1.88s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  66%|██████▌   | 45/68 [02:00<00:57,  2.51s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  72%|███████▏  | 49/68 [02:07<00:46,  2.43s/ba][A[A[A[A
Running tokenizer on dataset #1:  85%|████████▌ | 58/68 [02:10<00:24,  2.45s/ba][A







Running tokenizer on dataset #8:  82%|████████▏ | 56/68 [02:04<00:22,  1.90s/ba][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  72%|███████▏  | 49/68 [02:07<00:46,  2.45s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  76%|███████▋  | 52/68 [02:10<00:30,  1.92s/ba][A[A


Running tokenizer on dataset #3:  84%|████████▍ | 57/68 [02:09<00:27,  2.46s/ba][A[A[A





Running tokenizer on dataset #6:  90%|████████▉ | 61/68 [02:07<00:13,  1.89s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  69%|██████▉   | 47/68 [02:03<00:52,  2.48s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  93%|█████████▎| 63/68 [02:06<00:09,  1.91s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  96%|█████████▌| 65/68 [02:13<00:06,  2.24s/ba]








Running tokenizer on dataset #9:  82%|████████▏ | 56/68 [02:04<00:22,  1.87s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  84%|████████▍ | 57/68 [02:06<00:21,  1.92s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  68%|██████▊   | 46/68 [02:03<00:54,  2.48s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  87%|████████▋ | 59/68 [02:13<00:22,  2.46s/ba][A



Running tokenizer on dataset #4:  74%|███████▎  | 50/68 [02:10<00:43,  2.44s/ba][A[A[A[A




Running tokenizer on dataset #5:  74%|███████▎  | 50/68 [02:09<00:42,  2.36s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  78%|███████▊  | 53/68 [02:12<00:28,  1.90s/ba][A[A





Running tokenizer on dataset #6:  91%|█████████ | 62/68 [02:09<00:11,  1.86s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  94%|█████████▍| 64/68 [02:08<00:07,  1.91s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  85%|████████▌ | 58/68 [02:12<00:24,  2.47s/ba][A[A[A









Running tokenizer on dataset #10:  71%|███████   | 48/68 [02:06<00:49,  2.49s/ba][A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  85%|████████▌ | 58/68 [02:08<00:19,  1.92s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  84%|████████▍ | 57/68 [02:07<00:22,  2.06s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  97%|█████████▋| 66/68 [02:15<00:04,  2.32s/ba]




Running tokenizer on dataset #5:  75%|███████▌  | 51/68 [02:11<00:37,  2.21s/ba][A[A[A[A[A
Running tokenizer on dataset #1:  88%|████████▊ | 60/68 [02:15<00:19,  2.43s/ba][A










Running tokenizer on dataset #11:  69%|██████▉   | 47/68 [02:05<00:52,  2.48s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  79%|███████▉  | 54/68 [02:14<00:26,  1.90s/ba][A[A



Running tokenizer on dataset #4:  75%|███████▌  | 51/68 [02:12<00:41,  2.46s/ba][A[A[A[A





Running tokenizer on dataset #6:  93%|█████████▎| 63/68 [02:11<00:09,  1.86s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  96%|█████████▌| 65/68 [02:10<00:05,  1.91s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  87%|████████▋ | 59/68 [02:10<00:17,  1.91s/ba][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  76%|███████▋  | 52/68 [02:13<00:33,  2.11s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  87%|████████▋ | 59/68 [02:14<00:22,  2.49s/ba][A[A[A









Running tokenizer on dataset #10:  72%|███████▏  | 49/68 [02:08<00:47,  2.49s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  99%|█████████▊| 67/68 [02:17<00:02,  2.35s/ba]








Running tokenizer on dataset #9:  85%|████████▌ | 58/68 [02:09<00:21,  2.18s/ba][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  81%|████████  | 55/68 [02:16<00:24,  1.91s/ba][A[A





Running tokenizer on dataset #6:  94%|█████████▍| 64/68 [02:12<00:07,  1.87s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  97%|█████████▋| 66/68 [02:11<00:03,  1.89s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  90%|████████▉ | 61/68 [02:17<00:17,  2.44s/ba][A










Running tokenizer on dataset #11:  71%|███████   | 48/68 [02:08<00:49,  2.49s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  76%|███████▋  | 52/68 [02:15<00:39,  2.46s/ba][A[A[A[A







Running tokenizer on dataset #8:  88%|████████▊ | 60/68 [02:11<00:15,  1.90s/ba][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  78%|███████▊  | 53/68 [02:15<00:30,  2.04s/ba][A[A[A[A[ARunning tokenizer on dataset #0: 100%|██████████| 68/68 [02:19<00:00,  2.21s/ba]Running tokenizer on dataset #0: 100%|██████████| 68/68 [02:19<00:00,  2.06s/ba]

Running tokenizer on dataset #2:  82%|████████▏ | 56/68 [02:18<00:22,  1.91s/ba][A[A





Running tokenizer on dataset #6:  96%|█████████▌| 65/68 [02:14<00:05,  1.86s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  74%|███████▎  | 50/68 [02:10<00:44,  2.48s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  88%|████████▊ | 60/68 [02:17<00:20,  2.50s/ba][A[A[A






Running tokenizer on dataset #7:  99%|█████████▊| 67/68 [02:13<00:01,  1.89s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  87%|████████▋ | 59/68 [02:12<00:20,  2.27s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  72%|███████▏  | 49/68 [02:10<00:44,  2.37s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  91%|█████████ | 62/68 [02:20<00:14,  2.43s/ba][A







Running tokenizer on dataset #8:  90%|████████▉ | 61/68 [02:13<00:13,  1.90s/ba][A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  78%|███████▊  | 53/68 [02:17<00:36,  2.46s/ba][A[A[A[A




Running tokenizer on dataset #5:  79%|███████▉  | 54/68 [02:17<00:27,  1.99s/ba][A[A[A[A[A






Running tokenizer on dataset #7: 100%|██████████| 68/68 [02:15<00:00,  1.76s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #7: 100%|██████████| 68/68 [02:15<00:00,  1.99s/ba]

Running tokenizer on dataset #2:  84%|████████▍ | 57/68 [02:19<00:20,  1.91s/ba][A[A





Running tokenizer on dataset #6:  97%|█████████▋| 66/68 [02:16<00:03,  1.94s/ba][A[A[A[A[A[A


Running tokenizer on dataset #3:  90%|████████▉ | 61/68 [02:19<00:17,  2.48s/ba][A[A[A









Running tokenizer on dataset #10:  75%|███████▌  | 51/68 [02:13<00:41,  2.47s/ba][A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  88%|████████▊ | 60/68 [02:14<00:18,  2.33s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  91%|█████████ | 62/68 [02:15<00:11,  1.93s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  74%|███████▎  | 50/68 [02:12<00:43,  2.41s/ba][A[A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  81%|████████  | 55/68 [02:18<00:25,  1.97s/ba][A[A[A[A[A
Running tokenizer on dataset #1:  93%|█████████▎| 63/68 [02:22<00:12,  2.44s/ba][A



Running tokenizer on dataset #4:  79%|███████▉  | 54/68 [02:20<00:34,  2.46s/ba][A[A[A[A

Running tokenizer on dataset #2:  85%|████████▌ | 58/68 [02:21<00:19,  1.91s/ba][A[A





Running tokenizer on dataset #6:  99%|█████████▊| 67/68 [02:19<00:02,  2.09s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  93%|█████████▎| 63/68 [02:17<00:09,  1.93s/ba][A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  76%|███████▋  | 52/68 [02:15<00:39,  2.46s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  91%|█████████ | 62/68 [02:22<00:14,  2.48s/ba][A[A[A




Running tokenizer on dataset #5:  82%|████████▏ | 56/68 [02:20<00:23,  1.93s/ba][A[A[A[A[A








Running tokenizer on dataset #9:  90%|████████▉ | 61/68 [02:17<00:16,  2.38s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  75%|███████▌  | 51/68 [02:15<00:40,  2.40s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  87%|████████▋ | 59/68 [02:23<00:17,  1.91s/ba][A[A
Running tokenizer on dataset #1:  94%|█████████▍| 64/68 [02:25<00:09,  2.43s/ba][A



Running tokenizer on dataset #4:  81%|████████  | 55/68 [02:22<00:31,  2.45s/ba][A[A[A[A





Running tokenizer on dataset #6: 100%|██████████| 68/68 [02:21<00:00,  2.03s/ba][A[A[A[A[A[ARunning tokenizer on dataset #6: 100%|██████████| 68/68 [02:21<00:00,  2.08s/ba]







Running tokenizer on dataset #8:  94%|█████████▍| 64/68 [02:19<00:07,  1.90s/ba][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  84%|████████▍ | 57/68 [02:22<00:20,  1.91s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  93%|█████████▎| 63/68 [02:24<00:12,  2.41s/ba][A[A[A









Running tokenizer on dataset #10:  78%|███████▊  | 53/68 [02:18<00:36,  2.47s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  88%|████████▊ | 60/68 [02:25<00:15,  1.91s/ba][A[A








Running tokenizer on dataset #9:  91%|█████████ | 62/68 [02:19<00:14,  2.41s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  76%|███████▋  | 52/68 [02:17<00:38,  2.42s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  96%|█████████▌| 65/68 [02:27<00:07,  2.42s/ba][A



Running tokenizer on dataset #4:  82%|████████▏ | 56/68 [02:24<00:29,  2.44s/ba][A[A[A[A







Running tokenizer on dataset #8:  96%|█████████▌| 65/68 [02:21<00:05,  1.90s/ba][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  85%|████████▌ | 58/68 [02:24<00:18,  1.88s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  94%|█████████▍| 64/68 [02:26<00:09,  2.25s/ba][A[A[A

Running tokenizer on dataset #2:  90%|████████▉ | 61/68 [02:27<00:13,  1.88s/ba][A[A









Running tokenizer on dataset #10:  79%|███████▉  | 54/68 [02:20<00:34,  2.46s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  78%|███████▊  | 53/68 [02:20<00:36,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  97%|█████████▋| 66/68 [02:29<00:04,  2.42s/ba][A








Running tokenizer on dataset #9:  93%|█████████▎| 63/68 [02:22<00:12,  2.51s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  97%|█████████▋| 66/68 [02:23<00:03,  1.90s/ba][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  87%|████████▋ | 59/68 [02:26<00:16,  1.89s/ba][A[A[A[A[A



Running tokenizer on dataset #4:  84%|████████▍ | 57/68 [02:27<00:27,  2.46s/ba][A[A[A[A


Running tokenizer on dataset #3:  96%|█████████▌| 65/68 [02:28<00:06,  2.13s/ba][A[A[A

Running tokenizer on dataset #2:  91%|█████████ | 62/68 [02:29<00:11,  1.89s/ba][A[A









Running tokenizer on dataset #10:  81%|████████  | 55/68 [02:23<00:32,  2.47s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  88%|████████▊ | 60/68 [02:28<00:14,  1.87s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  99%|█████████▊| 67/68 [02:25<00:01,  1.91s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  97%|█████████▋| 66/68 [02:30<00:04,  2.04s/ba][A[A[A
Running tokenizer on dataset #1:  99%|█████████▊| 67/68 [02:32<00:02,  2.39s/ba][A










Running tokenizer on dataset #11:  79%|███████▉  | 54/68 [02:22<00:34,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  94%|█████████▍| 64/68 [02:24<00:09,  2.50s/ba][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  93%|█████████▎| 63/68 [02:31<00:09,  1.89s/ba][A[A



Running tokenizer on dataset #4:  85%|████████▌ | 58/68 [02:29<00:24,  2.47s/ba][A[A[A[A







Running tokenizer on dataset #8: 100%|██████████| 68/68 [02:26<00:00,  1.77s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #8: 100%|██████████| 68/68 [02:26<00:00,  2.16s/ba]




Running tokenizer on dataset #5:  90%|████████▉ | 61/68 [02:30<00:13,  1.87s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  99%|█████████▊| 67/68 [02:31<00:01,  1.99s/ba][A[A[A
Running tokenizer on dataset #1: 100%|██████████| 68/68 [02:34<00:00,  2.24s/ba][ARunning tokenizer on dataset #1: 100%|██████████| 68/68 [02:34<00:00,  2.27s/ba]









Running tokenizer on dataset #10:  82%|████████▏ | 56/68 [02:25<00:29,  2.45s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  94%|█████████▍| 64/68 [02:33<00:07,  1.87s/ba][A[A










Running tokenizer on dataset #11:  81%|████████  | 55/68 [02:25<00:32,  2.46s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  96%|█████████▌| 65/68 [02:27<00:07,  2.49s/ba][A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  87%|████████▋ | 59/68 [02:32<00:22,  2.47s/ba][A[A[A[A


Running tokenizer on dataset #3: 100%|██████████| 68/68 [02:33<00:00,  1.82s/ba][A[A[ARunning tokenizer on dataset #3: 100%|██████████| 68/68 [02:33<00:00,  2.26s/ba]




Running tokenizer on dataset #5:  91%|█████████ | 62/68 [02:31<00:11,  1.84s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  96%|█████████▌| 65/68 [02:35<00:05,  1.87s/ba][A[A









Running tokenizer on dataset #10:  84%|████████▍ | 57/68 [02:28<00:26,  2.45s/ba][A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  97%|█████████▋| 66/68 [02:29<00:04,  2.45s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  93%|█████████▎| 63/68 [02:33<00:09,  1.83s/ba][A[A[A[A[A










Running tokenizer on dataset #11:  82%|████████▏ | 56/68 [02:27<00:29,  2.49s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  88%|████████▊ | 60/68 [02:34<00:19,  2.48s/ba][A[A[A[A

Running tokenizer on dataset #2:  97%|█████████▋| 66/68 [02:36<00:03,  1.86s/ba][A[A




Running tokenizer on dataset #5:  94%|█████████▍| 64/68 [02:35<00:07,  1.82s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  85%|████████▌ | 58/68 [02:30<00:24,  2.46s/ba][A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  99%|█████████▊| 67/68 [02:31<00:02,  2.44s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  84%|████████▍ | 57/68 [02:30<00:27,  2.47s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  99%|█████████▊| 67/68 [02:38<00:01,  1.86s/ba][A[A



Running tokenizer on dataset #4:  90%|████████▉ | 61/68 [02:37<00:17,  2.47s/ba][A[A[A[A




Running tokenizer on dataset #5:  96%|█████████▌| 65/68 [02:37<00:05,  1.82s/ba][A[A[A[A[A

Running tokenizer on dataset #2: 100%|██████████| 68/68 [02:40<00:00,  1.73s/ba][A[ARunning tokenizer on dataset #2: 100%|██████████| 68/68 [02:40<00:00,  2.36s/ba]








Running tokenizer on dataset #9: 100%|██████████| 68/68 [02:33<00:00,  2.27s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #9: 100%|██████████| 68/68 [02:33<00:00,  2.26s/ba]









Running tokenizer on dataset #10:  87%|████████▋ | 59/68 [02:33<00:22,  2.45s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  85%|████████▌ | 58/68 [02:32<00:24,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  91%|█████████ | 62/68 [02:39<00:14,  2.45s/ba][A[A[A[A




Running tokenizer on dataset #5:  97%|█████████▋| 66/68 [02:39<00:03,  1.83s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  88%|████████▊ | 60/68 [02:35<00:19,  2.42s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  99%|█████████▊| 67/68 [02:40<00:01,  1.83s/ba][A[A[A[A[A










Running tokenizer on dataset #11:  87%|████████▋ | 59/68 [02:35<00:22,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  93%|█████████▎| 63/68 [02:42<00:12,  2.44s/ba][A[A[A[A




Running tokenizer on dataset #5: 100%|██████████| 68/68 [02:42<00:00,  1.70s/ba][A[A[A[A[ARunning tokenizer on dataset #5: 100%|██████████| 68/68 [02:42<00:00,  2.39s/ba]









Running tokenizer on dataset #10:  90%|████████▉ | 61/68 [02:37<00:17,  2.46s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  88%|████████▊ | 60/68 [02:37<00:19,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  94%|█████████▍| 64/68 [02:44<00:09,  2.44s/ba][A[A[A[A









Running tokenizer on dataset #10:  91%|█████████ | 62/68 [02:40<00:14,  2.46s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  90%|████████▉ | 61/68 [02:39<00:17,  2.44s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  96%|█████████▌| 65/68 [02:47<00:07,  2.43s/ba][A[A[A[A









Running tokenizer on dataset #10:  93%|█████████▎| 63/68 [02:42<00:12,  2.48s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  91%|█████████ | 62/68 [02:42<00:14,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  97%|█████████▋| 66/68 [02:49<00:04,  2.43s/ba][A[A[A[A









Running tokenizer on dataset #10:  94%|█████████▍| 64/68 [02:45<00:09,  2.46s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  93%|█████████▎| 63/68 [02:44<00:12,  2.46s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  99%|█████████▊| 67/68 [02:51<00:02,  2.42s/ba][A[A[A[A









Running tokenizer on dataset #10:  96%|█████████▌| 65/68 [02:47<00:07,  2.47s/ba][A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4: 100%|██████████| 68/68 [02:53<00:00,  2.23s/ba][A[A[A[ARunning tokenizer on dataset #4: 100%|██████████| 68/68 [02:53<00:00,  2.55s/ba]










Running tokenizer on dataset #11:  94%|█████████▍| 64/68 [02:47<00:09,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  97%|█████████▋| 66/68 [02:50<00:04,  2.44s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  96%|█████████▌| 65/68 [02:49<00:07,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  99%|█████████▊| 67/68 [02:52<00:02,  2.42s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  97%|█████████▋| 66/68 [02:52<00:04,  2.42s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10: 100%|██████████| 68/68 [02:54<00:00,  2.25s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #10: 100%|██████████| 68/68 [02:54<00:00,  2.56s/ba]










Running tokenizer on dataset #11:  99%|█████████▊| 67/68 [02:54<00:02,  2.42s/ba][A[A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11: 100%|██████████| 68/68 [02:56<00:00,  2.25s/ba][A[A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #11: 100%|██████████| 68/68 [02:56<00:00,  2.59s/ba]










Running tokenizer on dataset #0:   0%|          | 0/68 [00:00<?, ?ba/s]
Running tokenizer on dataset #1:   0%|          | 0/68 [00:00<?, ?ba/s][A

Running tokenizer on dataset #2:   0%|          | 0/68 [00:00<?, ?ba/s][A[ARunning tokenizer on dataset #0:   1%|▏         | 1/68 [00:02<02:57,  2.65s/ba]


Running tokenizer on dataset #3:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A



Running tokenizer on dataset #4:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A
Running tokenizer on dataset #1:   1%|▏         | 1/68 [00:02<03:00,  2.69s/ba][A

Running tokenizer on dataset #2:   1%|▏         | 1/68 [00:02<03:02,  2.73s/ba][A[A




Running tokenizer on dataset #5:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[ARunning tokenizer on dataset #0:   3%|▎         | 2/68 [00:04<02:41,  2.45s/ba]





Running tokenizer on dataset #6:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A
Running tokenizer on dataset #1:   3%|▎         | 2/68 [00:04<02:42,  2.45s/ba][A



Running tokenizer on dataset #4:   1%|▏         | 1/68 [00:02<03:01,  2.70s/ba][A[A[A[A


Running tokenizer on dataset #3:   1%|▏         | 1/68 [00:03<04:08,  3.70s/ba][A[A[A






Running tokenizer on dataset #7:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A

Running tokenizer on dataset #2:   3%|▎         | 2/68 [00:05<02:44,  2.49s/ba][A[ARunning tokenizer on dataset #0:   4%|▍         | 3/68 [00:07<02:34,  2.38s/ba]







Running tokenizer on dataset #8:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:   1%|▏         | 1/68 [00:02<02:59,  2.68s/ba][A[A[A[A[A








Running tokenizer on dataset #9:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:   4%|▍         | 3/68 [00:07<02:31,  2.33s/ba][A





Running tokenizer on dataset #6:   1%|▏         | 1/68 [00:02<03:07,  2.80s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:   3%|▎         | 2/68 [00:05<02:44,  2.49s/ba][A[A[A[A

Running tokenizer on dataset #2:   4%|▍         | 3/68 [00:07<02:33,  2.36s/ba][A[A









Running tokenizer on dataset #10:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:   6%|▌         | 4/68 [00:09<02:30,  2.36s/ba]


Running tokenizer on dataset #3:   3%|▎         | 2/68 [00:06<03:42,  3.38s/ba][A[A[A




Running tokenizer on dataset #5:   3%|▎         | 2/68 [00:05<02:44,  2.50s/ba][A[A[A[A[A







Running tokenizer on dataset #8:   1%|▏         | 1/68 [00:02<02:58,  2.67s/ba][A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:   1%|▏         | 1/68 [00:03<03:59,  3.57s/ba][A[A[A[A[A[A[A










Running tokenizer on dataset #11:   0%|          | 0/68 [00:00<?, ?ba/s][A[A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:   3%|▎         | 2/68 [00:05<02:48,  2.55s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:   4%|▍         | 3/68 [00:07<02:32,  2.35s/ba][A[A[A[A








Running tokenizer on dataset #9:   1%|▏         | 1/68 [00:02<03:03,  2.74s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:   6%|▌         | 4/68 [00:09<02:40,  2.51s/ba][A

Running tokenizer on dataset #2:   6%|▌         | 4/68 [00:09<02:27,  2.30s/ba][A[ARunning tokenizer on dataset #0:   7%|▋         | 5/68 [00:11<02:24,  2.29s/ba]




Running tokenizer on dataset #5:   4%|▍         | 3/68 [00:07<02:33,  2.37s/ba][A[A[A[A[A







Running tokenizer on dataset #8:   3%|▎         | 2/68 [00:04<02:41,  2.45s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:   4%|▍         | 3/68 [00:07<02:36,  2.41s/ba][A[A[A[A[A[A


Running tokenizer on dataset #3:   4%|▍         | 3/68 [00:09<03:30,  3.24s/ba][A[A[A



Running tokenizer on dataset #4:   6%|▌         | 4/68 [00:09<02:25,  2.28s/ba][A[A[A[A









Running tokenizer on dataset #10:   1%|▏         | 1/68 [00:03<04:01,  3.61s/ba][A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:   7%|▋         | 5/68 [00:12<02:29,  2.38s/ba][A








Running tokenizer on dataset #9:   3%|▎         | 2/68 [00:05<02:45,  2.50s/ba][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:   7%|▋         | 5/68 [00:11<02:20,  2.23s/ba][A[A






Running tokenizer on dataset #7:   3%|▎         | 2/68 [00:06<03:38,  3.31s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:   9%|▉         | 6/68 [00:13<02:18,  2.23s/ba]










Running tokenizer on dataset #11:   1%|▏         | 1/68 [00:03<04:01,  3.60s/ba][A[A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:   6%|▌         | 4/68 [00:09<02:28,  2.32s/ba][A[A[A[A[A







Running tokenizer on dataset #8:   4%|▍         | 3/68 [00:07<02:33,  2.37s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:   6%|▌         | 4/68 [00:09<02:29,  2.34s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:   9%|▉         | 6/68 [00:14<02:21,  2.28s/ba][A



Running tokenizer on dataset #4:   7%|▋         | 5/68 [00:11<02:20,  2.23s/ba][A[A[A[A

Running tokenizer on dataset #2:   9%|▉         | 6/68 [00:13<02:15,  2.19s/ba][A[A








Running tokenizer on dataset #9:   4%|▍         | 3/68 [00:07<02:37,  2.43s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:   6%|▌         | 4/68 [00:12<03:20,  3.13s/ba][A[A[ARunning tokenizer on dataset #0:  10%|█         | 7/68 [00:15<02:14,  2.20s/ba]









Running tokenizer on dataset #10:   3%|▎         | 2/68 [00:06<03:33,  3.24s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:   4%|▍         | 3/68 [00:09<03:24,  3.15s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:   7%|▋         | 5/68 [00:11<02:21,  2.24s/ba][A[A[A[A[A







Running tokenizer on dataset #8:   6%|▌         | 4/68 [00:09<02:26,  2.28s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:   3%|▎         | 2/68 [00:06<03:35,  3.27s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  10%|█         | 7/68 [00:16<02:14,  2.21s/ba][A



Running tokenizer on dataset #4:   9%|▉         | 6/68 [00:13<02:16,  2.19s/ba][A[A[A[A





Running tokenizer on dataset #6:   7%|▋         | 5/68 [00:11<02:24,  2.29s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  10%|█         | 7/68 [00:15<02:11,  2.16s/ba][A[A








Running tokenizer on dataset #9:   6%|▌         | 4/68 [00:09<02:30,  2.35s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  12%|█▏        | 8/68 [00:18<02:10,  2.17s/ba]









Running tokenizer on dataset #10:   4%|▍         | 3/68 [00:08<03:02,  2.80s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:   9%|▉         | 6/68 [00:13<02:16,  2.19s/ba][A[A[A[A[A


Running tokenizer on dataset #3:   7%|▋         | 5/68 [00:15<03:12,  3.05s/ba][A[A[A







Running tokenizer on dataset #8:   7%|▋         | 5/68 [00:11<02:19,  2.21s/ba][A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:   6%|▌         | 4/68 [00:12<03:14,  3.04s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  12%|█▏        | 8/68 [00:18<02:10,  2.18s/ba][A



Running tokenizer on dataset #4:  10%|█         | 7/68 [00:15<02:13,  2.19s/ba][A[A[A[A





Running tokenizer on dataset #6:   9%|▉         | 6/68 [00:13<02:18,  2.23s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  12%|█▏        | 8/68 [00:17<02:07,  2.12s/ba][A[A










Running tokenizer on dataset #11:   4%|▍         | 3/68 [00:09<03:23,  3.12s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:   7%|▋         | 5/68 [00:11<02:24,  2.29s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  13%|█▎        | 9/68 [00:20<02:05,  2.13s/ba]




Running tokenizer on dataset #5:  10%|█         | 7/68 [00:15<02:11,  2.16s/ba][A[A[A[A[A









Running tokenizer on dataset #10:   6%|▌         | 4/68 [00:11<02:52,  2.69s/ba][A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:   9%|▉         | 6/68 [00:13<02:13,  2.16s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  13%|█▎        | 9/68 [00:20<02:06,  2.15s/ba][A






Running tokenizer on dataset #7:   7%|▋         | 5/68 [00:14<02:52,  2.73s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  12%|█▏        | 8/68 [00:17<02:08,  2.14s/ba][A[A[A[A

Running tokenizer on dataset #2:  13%|█▎        | 9/68 [00:19<02:03,  2.10s/ba][A[A


Running tokenizer on dataset #3:   9%|▉         | 6/68 [00:18<03:05,  2.99s/ba][A[A[A





Running tokenizer on dataset #6:  10%|█         | 7/68 [00:16<02:16,  2.23s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:   9%|▉         | 6/68 [00:13<02:18,  2.23s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  15%|█▍        | 10/68 [00:22<02:01,  2.10s/ba]




Running tokenizer on dataset #5:  12%|█▏        | 8/68 [00:17<02:08,  2.14s/ba][A[A[A[A[A










Running tokenizer on dataset #11:   6%|▌         | 4/68 [00:12<03:13,  3.03s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  10%|█         | 7/68 [00:15<02:11,  2.16s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  15%|█▍        | 10/68 [00:22<02:02,  2.12s/ba][A






Running tokenizer on dataset #7:   9%|▉         | 6/68 [00:16<02:36,  2.52s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  13%|█▎        | 9/68 [00:19<02:05,  2.12s/ba][A[A[A[A

Running tokenizer on dataset #2:  15%|█▍        | 10/68 [00:21<01:59,  2.07s/ba][A[A









Running tokenizer on dataset #10:   7%|▋         | 5/68 [00:14<02:53,  2.75s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  12%|█▏        | 8/68 [00:18<02:16,  2.28s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  16%|█▌        | 11/68 [00:24<01:59,  2.10s/ba]








Running tokenizer on dataset #9:  10%|█         | 7/68 [00:16<02:14,  2.20s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  10%|█         | 7/68 [00:21<03:00,  2.96s/ba][A[A[A




Running tokenizer on dataset #5:  13%|█▎        | 9/68 [00:19<02:04,  2.11s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  12%|█▏        | 8/68 [00:17<02:06,  2.11s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  16%|█▌        | 11/68 [00:24<01:58,  2.09s/ba][A

Running tokenizer on dataset #2:  16%|█▌        | 11/68 [00:23<01:56,  2.04s/ba][A[A










Running tokenizer on dataset #11:   7%|▋         | 5/68 [00:15<03:05,  2.94s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  15%|█▍        | 10/68 [00:22<02:01,  2.10s/ba][A[A[A[A






Running tokenizer on dataset #7:  10%|█         | 7/68 [00:18<02:26,  2.40s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  18%|█▊        | 12/68 [00:26<01:56,  2.08s/ba]








Running tokenizer on dataset #9:  12%|█▏        | 8/68 [00:18<02:09,  2.16s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:   9%|▉         | 6/68 [00:16<02:50,  2.75s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  15%|█▍        | 10/68 [00:21<01:59,  2.07s/ba][A[A[A[A[A





Running tokenizer on dataset #6:  13%|█▎        | 9/68 [00:21<02:23,  2.43s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  13%|█▎        | 9/68 [00:19<02:03,  2.09s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  12%|█▏        | 8/68 [00:24<02:54,  2.91s/ba][A[A[A
Running tokenizer on dataset #1:  18%|█▊        | 12/68 [00:26<01:55,  2.07s/ba][A

Running tokenizer on dataset #2:  18%|█▊        | 12/68 [00:25<01:55,  2.06s/ba][A[A



Running tokenizer on dataset #4:  16%|█▌        | 11/68 [00:24<01:59,  2.10s/ba][A[A[A[A






Running tokenizer on dataset #7:  12%|█▏        | 8/68 [00:21<02:19,  2.33s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  19%|█▉        | 13/68 [00:28<01:54,  2.08s/ba]








Running tokenizer on dataset #9:  13%|█▎        | 9/68 [00:20<02:05,  2.13s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:   9%|▉         | 6/68 [00:18<02:59,  2.90s/ba][A[A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  16%|█▌        | 11/68 [00:23<01:57,  2.07s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  15%|█▍        | 10/68 [00:21<01:59,  2.06s/ba][A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  10%|█         | 7/68 [00:19<02:49,  2.77s/ba][A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  19%|█▉        | 13/68 [00:28<01:52,  2.05s/ba][A





Running tokenizer on dataset #6:  15%|█▍        | 10/68 [00:24<02:25,  2.51s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  19%|█▉        | 13/68 [00:27<01:51,  2.03s/ba][A[A



Running tokenizer on dataset #4:  18%|█▊        | 12/68 [00:26<01:56,  2.08s/ba][A[A[A[A






Running tokenizer on dataset #7:  13%|█▎        | 9/68 [00:23<02:13,  2.26s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  13%|█▎        | 9/68 [00:27<02:48,  2.86s/ba][A[A[ARunning tokenizer on dataset #0:  21%|██        | 14/68 [00:30<01:51,  2.07s/ba]








Running tokenizer on dataset #9:  15%|█▍        | 10/68 [00:22<02:01,  2.10s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  18%|█▊        | 12/68 [00:25<01:54,  2.04s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  16%|█▌        | 11/68 [00:23<01:55,  2.02s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  10%|█         | 7/68 [00:20<02:55,  2.88s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  21%|██        | 14/68 [00:30<01:50,  2.04s/ba][A

Running tokenizer on dataset #2:  21%|██        | 14/68 [00:29<01:47,  2.00s/ba][A[A



Running tokenizer on dataset #4:  19%|█▉        | 13/68 [00:28<01:53,  2.07s/ba][A[A[A[A









Running tokenizer on dataset #10:  12%|█▏        | 8/68 [00:22<02:45,  2.76s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  15%|█▍        | 10/68 [00:25<02:07,  2.20s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  16%|█▌        | 11/68 [00:26<02:26,  2.57s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  22%|██▏       | 15/68 [00:32<01:48,  2.05s/ba]








Running tokenizer on dataset #9:  16%|█▌        | 11/68 [00:24<01:58,  2.09s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  19%|█▉        | 13/68 [00:27<01:50,  2.01s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  15%|█▍        | 10/68 [00:29<02:43,  2.82s/ba][A[A[A







Running tokenizer on dataset #8:  18%|█▊        | 12/68 [00:25<01:52,  2.01s/ba][A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  22%|██▏       | 15/68 [00:31<01:45,  1.98s/ba][A[A
Running tokenizer on dataset #1:  22%|██▏       | 15/68 [00:32<01:47,  2.04s/ba][A



Running tokenizer on dataset #4:  21%|██        | 14/68 [00:30<01:50,  2.04s/ba][A[A[A[A










Running tokenizer on dataset #11:  12%|█▏        | 8/68 [00:23<02:50,  2.84s/ba][A[A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  16%|█▌        | 11/68 [00:27<02:01,  2.14s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  24%|██▎       | 16/68 [00:34<01:45,  2.04s/ba]








Running tokenizer on dataset #9:  18%|█▊        | 12/68 [00:26<01:56,  2.07s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  21%|██        | 14/68 [00:29<01:48,  2.01s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  13%|█▎        | 9/68 [00:25<02:42,  2.76s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  18%|█▊        | 12/68 [00:29<02:24,  2.59s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  19%|█▉        | 13/68 [00:27<01:49,  2.00s/ba][A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  24%|██▎       | 16/68 [00:33<01:42,  1.97s/ba][A[A
Running tokenizer on dataset #1:  24%|██▎       | 16/68 [00:34<01:44,  2.01s/ba][A


Running tokenizer on dataset #3:  16%|█▌        | 11/68 [00:32<02:38,  2.79s/ba][A[A[A



Running tokenizer on dataset #4:  22%|██▏       | 15/68 [00:32<01:47,  2.03s/ba][A[A[A[A






Running tokenizer on dataset #7:  18%|█▊        | 12/68 [00:29<01:57,  2.10s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  25%|██▌       | 17/68 [00:36<01:43,  2.02s/ba]








Running tokenizer on dataset #9:  19%|█▉        | 13/68 [00:28<01:52,  2.05s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  22%|██▏       | 15/68 [00:31<01:44,  1.98s/ba][A[A[A[A[A










Running tokenizer on dataset #11:  13%|█▎        | 9/68 [00:26<02:45,  2.80s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  21%|██        | 14/68 [00:29<01:47,  1.99s/ba][A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  15%|█▍        | 10/68 [00:27<02:37,  2.71s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  25%|██▌       | 17/68 [00:35<01:40,  1.97s/ba][A[A
Running tokenizer on dataset #1:  25%|██▌       | 17/68 [00:36<01:42,  2.01s/ba][A





Running tokenizer on dataset #6:  19%|█▉        | 13/68 [00:32<02:24,  2.62s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  24%|██▎       | 16/68 [00:34<01:45,  2.02s/ba][A[A[A[A






Running tokenizer on dataset #7:  19%|█▉        | 13/68 [00:31<01:54,  2.08s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  18%|█▊        | 12/68 [00:35<02:35,  2.77s/ba][A[A[ARunning tokenizer on dataset #0:  26%|██▋       | 18/68 [00:38<01:40,  2.01s/ba]








Running tokenizer on dataset #9:  21%|██        | 14/68 [00:30<01:50,  2.05s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  24%|██▎       | 16/68 [00:33<01:45,  2.03s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  22%|██▏       | 15/68 [00:31<01:45,  1.98s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  15%|█▍        | 10/68 [00:29<02:39,  2.76s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  26%|██▋       | 18/68 [00:38<01:39,  1.99s/ba][A



Running tokenizer on dataset #4:  25%|██▌       | 17/68 [00:36<01:42,  2.01s/ba][A[A[A[A

Running tokenizer on dataset #2:  26%|██▋       | 18/68 [00:38<01:44,  2.10s/ba][A[A









Running tokenizer on dataset #10:  16%|█▌        | 11/68 [00:30<02:33,  2.70s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  21%|██        | 14/68 [00:33<01:51,  2.07s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  21%|██        | 14/68 [00:34<02:22,  2.63s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  28%|██▊       | 19/68 [00:40<01:38,  2.00s/ba]








Running tokenizer on dataset #9:  22%|██▏       | 15/68 [00:32<01:48,  2.04s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  25%|██▌       | 17/68 [00:35<01:42,  2.01s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  24%|██▎       | 16/68 [00:33<01:42,  1.97s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  19%|█▉        | 13/68 [00:38<02:31,  2.75s/ba][A[A[A
Running tokenizer on dataset #1:  28%|██▊       | 19/68 [00:40<01:37,  1.99s/ba][A



Running tokenizer on dataset #4:  26%|██▋       | 18/68 [00:38<01:40,  2.01s/ba][A[A[A[A










Running tokenizer on dataset #11:  16%|█▌        | 11/68 [00:31<02:35,  2.73s/ba][A[A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  22%|██▏       | 15/68 [00:35<01:48,  2.05s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  29%|██▉       | 20/68 [00:42<01:35,  2.00s/ba]

Running tokenizer on dataset #2:  28%|██▊       | 19/68 [00:40<01:49,  2.24s/ba][A[A








Running tokenizer on dataset #9:  24%|██▎       | 16/68 [00:34<01:46,  2.04s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  18%|█▊        | 12/68 [00:33<02:30,  2.69s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  26%|██▋       | 18/68 [00:37<01:39,  1.99s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  25%|██▌       | 17/68 [00:35<01:40,  1.97s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  22%|██▏       | 15/68 [00:37<02:20,  2.66s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  29%|██▉       | 20/68 [00:42<01:34,  1.98s/ba][A



Running tokenizer on dataset #4:  28%|██▊       | 19/68 [00:40<01:37,  2.00s/ba][A[A[A[A


Running tokenizer on dataset #3:  21%|██        | 14/68 [00:40<02:28,  2.76s/ba][A[A[A






Running tokenizer on dataset #7:  24%|██▎       | 16/68 [00:37<01:46,  2.04s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  31%|███       | 21/68 [00:44<01:33,  1.99s/ba]








Running tokenizer on dataset #9:  25%|██▌       | 17/68 [00:36<01:43,  2.03s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  28%|██▊       | 19/68 [00:39<01:37,  1.98s/ba][A[A[A[A[A










Running tokenizer on dataset #11:  18%|█▊        | 12/68 [00:34<02:31,  2.70s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  26%|██▋       | 18/68 [00:37<01:38,  1.96s/ba][A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  29%|██▉       | 20/68 [00:43<01:52,  2.34s/ba][A[A









Running tokenizer on dataset #10:  19%|█▉        | 13/68 [00:35<02:27,  2.69s/ba][A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  31%|███       | 21/68 [00:44<01:32,  1.98s/ba][A





Running tokenizer on dataset #6:  24%|██▎       | 16/68 [00:40<02:17,  2.65s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  29%|██▉       | 20/68 [00:42<01:36,  2.00s/ba][A[A[A[A






Running tokenizer on dataset #7:  25%|██▌       | 17/68 [00:39<01:44,  2.04s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  32%|███▏      | 22/68 [00:46<01:30,  1.98s/ba]


Running tokenizer on dataset #3:  22%|██▏       | 15/68 [00:43<02:26,  2.76s/ba][A[A[A




Running tokenizer on dataset #5:  29%|██▉       | 20/68 [00:41<01:35,  1.99s/ba][A[A[A[A[A








Running tokenizer on dataset #9:  26%|██▋       | 18/68 [00:38<01:41,  2.04s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  28%|██▊       | 19/68 [00:39<01:36,  1.97s/ba][A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  32%|███▏      | 22/68 [00:46<01:30,  1.97s/ba][A










Running tokenizer on dataset #11:  19%|█▉        | 13/68 [00:37<02:28,  2.69s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  31%|███       | 21/68 [00:45<01:53,  2.41s/ba][A[A



Running tokenizer on dataset #4:  31%|███       | 21/68 [00:44<01:36,  2.06s/ba][A[A[A[A









Running tokenizer on dataset #10:  21%|██        | 14/68 [00:38<02:25,  2.70s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  25%|██▌       | 17/68 [00:42<02:14,  2.64s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  34%|███▍      | 23/68 [00:48<01:28,  1.97s/ba]






Running tokenizer on dataset #7:  26%|██▋       | 18/68 [00:41<01:42,  2.04s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  31%|███       | 21/68 [00:43<01:32,  1.97s/ba][A[A[A[A[A








Running tokenizer on dataset #9:  28%|██▊       | 19/68 [00:40<01:39,  2.03s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  29%|██▉       | 20/68 [00:41<01:34,  1.97s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  24%|██▎       | 16/68 [00:46<02:22,  2.75s/ba][A[A[A
Running tokenizer on dataset #1:  34%|███▍      | 23/68 [00:48<01:28,  1.96s/ba][A










Running tokenizer on dataset #11:  21%|██        | 14/68 [00:39<02:24,  2.68s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  32%|███▏      | 22/68 [00:48<01:52,  2.44s/ba][A[ARunning tokenizer on dataset #0:  35%|███▌      | 24/68 [00:50<01:26,  1.97s/ba]






Running tokenizer on dataset #7:  28%|██▊       | 19/68 [00:43<01:39,  2.03s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  32%|███▏      | 22/68 [00:45<01:29,  1.94s/ba][A[A[A[A[A



Running tokenizer on dataset #4:  32%|███▏      | 22/68 [00:46<01:42,  2.22s/ba][A[A[A[A








Running tokenizer on dataset #9:  29%|██▉       | 20/68 [00:42<01:36,  2.02s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  31%|███       | 21/68 [00:43<01:31,  1.96s/ba][A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  22%|██▏       | 15/68 [00:41<02:21,  2.68s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  26%|██▋       | 18/68 [00:45<02:10,  2.61s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  35%|███▌      | 24/68 [00:50<01:25,  1.94s/ba][A


Running tokenizer on dataset #3:  25%|██▌       | 17/68 [00:48<02:19,  2.73s/ba][A[A[ARunning tokenizer on dataset #0:  37%|███▋      | 25/68 [00:52<01:24,  1.97s/ba]






Running tokenizer on dataset #7:  29%|██▉       | 20/68 [00:45<01:36,  2.00s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  34%|███▍      | 23/68 [00:47<01:26,  1.93s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  32%|███▏      | 22/68 [00:45<01:29,  1.95s/ba][A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  34%|███▍      | 23/68 [00:50<01:51,  2.48s/ba][A[A








Running tokenizer on dataset #9:  31%|███       | 21/68 [00:44<01:34,  2.01s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  22%|██▏       | 15/68 [00:42<02:21,  2.68s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  37%|███▋      | 25/68 [00:52<01:23,  1.94s/ba][A



Running tokenizer on dataset #4:  34%|███▍      | 23/68 [00:49<01:44,  2.32s/ba][A[A[A[A









Running tokenizer on dataset #10:  24%|██▎       | 16/68 [00:43<02:18,  2.66s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  28%|██▊       | 19/68 [00:47<02:08,  2.62s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  38%|███▊      | 26/68 [00:54<01:22,  1.96s/ba]






Running tokenizer on dataset #7:  31%|███       | 21/68 [00:47<01:34,  2.00s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  35%|███▌      | 24/68 [00:49<01:24,  1.91s/ba][A[A[A[A[A







Running tokenizer on dataset #8:  34%|███▍      | 23/68 [00:47<01:27,  1.94s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  26%|██▋       | 18/68 [00:51<02:15,  2.71s/ba][A[A[A








Running tokenizer on dataset #9:  32%|███▏      | 22/68 [00:46<01:32,  2.02s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  38%|███▊      | 26/68 [00:54<01:21,  1.93s/ba][A

Running tokenizer on dataset #2:  35%|███▌      | 24/68 [00:53<01:50,  2.51s/ba][A[A










Running tokenizer on dataset #11:  24%|██▎       | 16/68 [00:44<02:17,  2.65s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  35%|███▌      | 24/68 [00:52<01:45,  2.39s/ba][A[A[A[A









Running tokenizer on dataset #10:  25%|██▌       | 17/68 [00:46<02:14,  2.63s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  40%|███▉      | 27/68 [00:56<01:19,  1.95s/ba]





Running tokenizer on dataset #6:  29%|██▉       | 20/68 [00:50<02:05,  2.62s/ba][A[A[A[A[A[A




Running tokenizer on dataset #5:  37%|███▋      | 25/68 [00:51<01:21,  1.91s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  32%|███▏      | 22/68 [00:49<01:32,  2.00s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  35%|███▌      | 24/68 [00:49<01:25,  1.95s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  34%|███▍      | 23/68 [00:48<01:29,  1.99s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  40%|███▉      | 27/68 [00:55<01:18,  1.92s/ba][A


Running tokenizer on dataset #3:  28%|██▊       | 19/68 [00:54<02:11,  2.69s/ba][A[A[A

Running tokenizer on dataset #2:  37%|███▋      | 25/68 [00:55<01:47,  2.51s/ba][A[A










Running tokenizer on dataset #11:  25%|██▌       | 17/68 [00:47<02:14,  2.63s/ba][A[A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  41%|████      | 28/68 [00:58<01:18,  1.96s/ba]




Running tokenizer on dataset #5:  38%|███▊      | 26/68 [00:53<01:20,  1.91s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  34%|███▍      | 23/68 [00:51<01:28,  1.97s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  37%|███▋      | 25/68 [00:54<01:45,  2.45s/ba][A[A[A[A







Running tokenizer on dataset #8:  37%|███▋      | 25/68 [00:51<01:22,  1.93s/ba][A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  26%|██▋       | 18/68 [00:49<02:11,  2.62s/ba][A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  35%|███▌      | 24/68 [00:50<01:27,  1.98s/ba][A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  31%|███       | 21/68 [00:53<02:02,  2.61s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  41%|████      | 28/68 [00:57<01:16,  1.92s/ba][A


Running tokenizer on dataset #3:  29%|██▉       | 20/68 [00:56<02:09,  2.69s/ba][A[A[ARunning tokenizer on dataset #0:  43%|████▎     | 29/68 [01:00<01:16,  1.97s/ba]




Running tokenizer on dataset #5:  40%|███▉      | 27/68 [00:55<01:18,  1.92s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  35%|███▌      | 24/68 [00:53<01:26,  1.97s/ba][A[A[A[A[A[A[A

Running tokenizer on dataset #2:  38%|███▊      | 26/68 [00:58<01:45,  2.51s/ba][A[A







Running tokenizer on dataset #8:  38%|███▊      | 26/68 [00:52<01:21,  1.93s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  37%|███▋      | 25/68 [00:52<01:23,  1.95s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  26%|██▋       | 18/68 [00:50<02:11,  2.63s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  43%|████▎     | 29/68 [00:59<01:15,  1.93s/ba][A



Running tokenizer on dataset #4:  38%|███▊      | 26/68 [00:57<01:45,  2.51s/ba][A[A[A[A









Running tokenizer on dataset #10:  28%|██▊       | 19/68 [00:51<02:08,  2.62s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  32%|███▏      | 22/68 [00:55<02:00,  2.61s/ba][A[A[A[A[A[A




Running tokenizer on dataset #5:  41%|████      | 28/68 [00:57<01:16,  1.92s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  44%|████▍     | 30/68 [01:01<01:14,  1.97s/ba]






Running tokenizer on dataset #7:  37%|███▋      | 25/68 [00:55<01:23,  1.94s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  40%|███▉      | 27/68 [00:54<01:19,  1.93s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  38%|███▊      | 26/68 [00:54<01:22,  1.96s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  31%|███       | 21/68 [00:59<02:06,  2.70s/ba][A[A[A
Running tokenizer on dataset #1:  44%|████▍     | 30/68 [01:01<01:13,  1.92s/ba][A

Running tokenizer on dataset #2:  40%|███▉      | 27/68 [01:01<01:43,  2.52s/ba][A[A










Running tokenizer on dataset #11:  28%|██▊       | 19/68 [00:52<02:08,  2.62s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  40%|███▉      | 27/68 [00:59<01:43,  2.52s/ba][A[A[A[A




Running tokenizer on dataset #5:  43%|████▎     | 29/68 [00:58<01:13,  1.90s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  29%|██▉       | 20/68 [00:54<02:06,  2.63s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  34%|███▍      | 23/68 [00:58<01:57,  2.61s/ba][A[A[A[A[A[ARunning tokenizer on dataset #0:  46%|████▌     | 31/68 [01:03<01:12,  1.95s/ba]






Running tokenizer on dataset #7:  38%|███▊      | 26/68 [00:57<01:21,  1.94s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  41%|████      | 28/68 [00:56<01:17,  1.93s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  40%|███▉      | 27/68 [00:56<01:20,  1.96s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  46%|████▌     | 31/68 [01:03<01:11,  1.93s/ba][A

Running tokenizer on dataset #2:  41%|████      | 28/68 [01:03<01:40,  2.52s/ba][A[A


Running tokenizer on dataset #3:  32%|███▏      | 22/68 [01:02<02:03,  2.69s/ba][A[A[A




Running tokenizer on dataset #5:  44%|████▍     | 30/68 [01:00<01:12,  1.89s/ba][A[A[A[A[A










Running tokenizer on dataset #11:  29%|██▉       | 20/68 [00:55<02:05,  2.61s/ba][A[A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  47%|████▋     | 32/68 [01:05<01:10,  1.97s/ba]






Running tokenizer on dataset #7:  40%|███▉      | 27/68 [00:59<01:19,  1.95s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  41%|████      | 28/68 [01:02<01:40,  2.52s/ba][A[A[A[A







Running tokenizer on dataset #8:  43%|████▎     | 29/68 [00:58<01:14,  1.92s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  35%|███▌      | 24/68 [01:00<01:50,  2.51s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  31%|███       | 21/68 [00:56<02:02,  2.60s/ba][A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  41%|████      | 28/68 [00:58<01:18,  1.97s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  47%|████▋     | 32/68 [01:05<01:09,  1.92s/ba][A




Running tokenizer on dataset #5:  46%|████▌     | 31/68 [01:02<01:10,  1.90s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  43%|████▎     | 29/68 [01:05<01:37,  2.49s/ba][A[ARunning tokenizer on dataset #0:  49%|████▊     | 33/68 [01:07<01:08,  1.95s/ba]






Running tokenizer on dataset #7:  41%|████      | 28/68 [01:01<01:17,  1.95s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  34%|███▍      | 23/68 [01:04<02:00,  2.68s/ba][A[A[A







Running tokenizer on dataset #8:  44%|████▍     | 30/68 [01:00<01:12,  1.92s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  37%|███▋      | 25/68 [01:02<01:40,  2.33s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  31%|███       | 21/68 [00:57<02:02,  2.60s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  49%|████▊     | 33/68 [01:07<01:07,  1.92s/ba][A



Running tokenizer on dataset #4:  43%|████▎     | 29/68 [01:04<01:38,  2.52s/ba][A[A[A[A








Running tokenizer on dataset #9:  43%|████▎     | 29/68 [01:00<01:18,  2.01s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  32%|███▏      | 22/68 [00:59<01:58,  2.57s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  47%|████▋     | 32/68 [01:04<01:08,  1.90s/ba][A[A[A[A[A

Running tokenizer on dataset #2:  44%|████▍     | 30/68 [01:07<01:28,  2.32s/ba][A[ARunning tokenizer on dataset #0:  50%|█████     | 34/68 [01:09<01:05,  1.93s/ba]






Running tokenizer on dataset #7:  43%|████▎     | 29/68 [01:03<01:17,  1.98s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  46%|████▌     | 31/68 [01:02<01:10,  1.92s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  38%|███▊      | 26/68 [01:04<01:33,  2.24s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  50%|█████     | 34/68 [01:09<01:04,  1.91s/ba][A


Running tokenizer on dataset #3:  35%|███▌      | 24/68 [01:07<01:54,  2.61s/ba][A[A[A



Running tokenizer on dataset #4:  44%|████▍     | 30/68 [01:07<01:32,  2.44s/ba][A[A[A[A










Running tokenizer on dataset #11:  32%|███▏      | 22/68 [01:00<01:59,  2.61s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  44%|████▍     | 30/68 [01:02<01:22,  2.17s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  49%|████▊     | 33/68 [01:06<01:06,  1.89s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  34%|███▍      | 23/68 [01:02<01:56,  2.60s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  51%|█████▏    | 35/68 [01:11<01:03,  1.92s/ba]







Running tokenizer on dataset #8:  47%|████▋     | 32/68 [01:04<01:08,  1.90s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  40%|███▉      | 27/68 [01:06<01:27,  2.14s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  46%|████▌     | 31/68 [01:10<01:26,  2.33s/ba][A[A
Running tokenizer on dataset #1:  51%|█████▏    | 35/68 [01:11<01:03,  1.91s/ba][A


Running tokenizer on dataset #3:  37%|███▋      | 25/68 [01:09<01:44,  2.42s/ba][A[A[A






Running tokenizer on dataset #7:  44%|████▍     | 30/68 [01:05<01:22,  2.16s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  46%|████▌     | 31/68 [01:09<01:26,  2.33s/ba][A[A[A[A




Running tokenizer on dataset #5:  50%|█████     | 34/68 [01:08<01:04,  1.89s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  53%|█████▎    | 36/68 [01:13<01:00,  1.91s/ba]










Running tokenizer on dataset #11:  34%|███▍      | 23/68 [01:03<01:56,  2.59s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  46%|████▌     | 31/68 [01:05<01:23,  2.27s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  49%|████▊     | 33/68 [01:06<01:06,  1.89s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  41%|████      | 28/68 [01:08<01:23,  2.08s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  35%|███▌      | 24/68 [01:04<01:53,  2.58s/ba][A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  53%|█████▎    | 36/68 [01:13<01:00,  1.91s/ba][A


Running tokenizer on dataset #3:  38%|███▊      | 26/68 [01:11<01:37,  2.31s/ba][A[A[A

Running tokenizer on dataset #2:  47%|████▋     | 32/68 [01:12<01:25,  2.38s/ba][A[A






Running tokenizer on dataset #7:  46%|████▌     | 31/68 [01:08<01:22,  2.22s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  51%|█████▏    | 35/68 [01:10<01:02,  1.89s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  54%|█████▍    | 37/68 [01:15<00:59,  1.90s/ba]



Running tokenizer on dataset #4:  47%|████▋     | 32/68 [01:11<01:26,  2.40s/ba][A[A[A[A







Running tokenizer on dataset #8:  50%|█████     | 34/68 [01:08<01:05,  1.92s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  43%|████▎     | 29/68 [01:10<01:19,  2.04s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  54%|█████▍    | 37/68 [01:15<00:59,  1.91s/ba][A








Running tokenizer on dataset #9:  47%|████▋     | 32/68 [01:07<01:24,  2.35s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  40%|███▉      | 27/68 [01:13<01:31,  2.22s/ba][A[A[A









Running tokenizer on dataset #10:  37%|███▋      | 25/68 [01:07<01:51,  2.58s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  47%|████▋     | 32/68 [01:09<01:16,  2.13s/ba][A[A[A[A[A[A[A

Running tokenizer on dataset #2:  49%|████▊     | 33/68 [01:15<01:24,  2.42s/ba][A[A




Running tokenizer on dataset #5:  53%|█████▎    | 36/68 [01:12<01:01,  1.91s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  56%|█████▌    | 38/68 [01:17<00:56,  1.89s/ba]










Running tokenizer on dataset #11:  35%|███▌      | 24/68 [01:06<02:10,  2.96s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  51%|█████▏    | 35/68 [01:10<01:03,  1.92s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  44%|████▍     | 30/68 [01:12<01:16,  2.00s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  49%|████▊     | 33/68 [01:14<01:25,  2.45s/ba][A[A[A[A
Running tokenizer on dataset #1:  56%|█████▌    | 38/68 [01:16<00:57,  1.91s/ba][A


Running tokenizer on dataset #3:  41%|████      | 28/68 [01:15<01:25,  2.15s/ba][A[A[A








Running tokenizer on dataset #9:  49%|████▊     | 33/68 [01:10<01:23,  2.40s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  49%|████▊     | 33/68 [01:11<01:12,  2.06s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  54%|█████▍    | 37/68 [01:14<00:59,  1.91s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  38%|███▊      | 26/68 [01:09<01:47,  2.56s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  57%|█████▋    | 39/68 [01:19<00:55,  1.91s/ba]







Running tokenizer on dataset #8:  53%|█████▎    | 36/68 [01:12<01:01,  1.91s/ba][A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  50%|█████     | 34/68 [01:17<01:22,  2.44s/ba][A[A





Running tokenizer on dataset #6:  46%|████▌     | 31/68 [01:14<01:12,  1.97s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  37%|███▋      | 25/68 [01:09<02:01,  2.83s/ba][A[A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  57%|█████▋    | 39/68 [01:18<00:55,  1.90s/ba][A


Running tokenizer on dataset #3:  43%|████▎     | 29/68 [01:17<01:21,  2.09s/ba][A[A[A



Running tokenizer on dataset #4:  50%|█████     | 34/68 [01:16<01:23,  2.47s/ba][A[A[A[A






Running tokenizer on dataset #7:  50%|█████     | 34/68 [01:13<01:09,  2.03s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  56%|█████▌    | 38/68 [01:16<00:57,  1.92s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  59%|█████▉    | 40/68 [01:21<00:53,  1.91s/ba]








Running tokenizer on dataset #9:  50%|█████     | 34/68 [01:12<01:23,  2.44s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  54%|█████▍    | 37/68 [01:13<00:59,  1.91s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  47%|████▋     | 32/68 [01:16<01:11,  1.97s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  40%|███▉      | 27/68 [01:12<01:44,  2.55s/ba][A[A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  59%|█████▉    | 40/68 [01:20<00:53,  1.91s/ba][A

Running tokenizer on dataset #2:  51%|█████▏    | 35/68 [01:20<01:20,  2.45s/ba][A[A










Running tokenizer on dataset #11:  38%|███▊      | 26/68 [01:11<01:53,  2.70s/ba][A[A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  44%|████▍     | 30/68 [01:19<01:18,  2.07s/ba][A[A[A






Running tokenizer on dataset #7:  51%|█████▏    | 35/68 [01:15<01:06,  2.03s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  57%|█████▋    | 39/68 [01:17<00:55,  1.90s/ba][A[A[A[A[A



Running tokenizer on dataset #4:  51%|█████▏    | 35/68 [01:19<01:21,  2.48s/ba][A[A[A[ARunning tokenizer on dataset #0:  60%|██████    | 41/68 [01:23<00:51,  1.92s/ba]







Running tokenizer on dataset #8:  56%|█████▌    | 38/68 [01:15<00:57,  1.91s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  49%|████▊     | 33/68 [01:17<01:08,  1.95s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  51%|█████▏    | 35/68 [01:15<01:21,  2.46s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  60%|██████    | 41/68 [01:22<00:51,  1.92s/ba][A










Running tokenizer on dataset #11:  40%|███▉      | 27/68 [01:13<01:40,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  41%|████      | 28/68 [01:14<01:42,  2.57s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  46%|████▌     | 31/68 [01:21<01:15,  2.04s/ba][A[A[A

Running tokenizer on dataset #2:  53%|█████▎    | 36/68 [01:22<01:18,  2.45s/ba][A[A






Running tokenizer on dataset #7:  53%|█████▎    | 36/68 [01:17<01:03,  1.99s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  59%|█████▉    | 40/68 [01:19<00:52,  1.89s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  62%|██████▏   | 42/68 [01:24<00:49,  1.92s/ba]







Running tokenizer on dataset #8:  57%|█████▋    | 39/68 [01:17<00:55,  1.90s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  50%|█████     | 34/68 [01:19<01:06,  1.95s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  53%|█████▎    | 36/68 [01:21<01:19,  2.49s/ba][A[A[A[A
Running tokenizer on dataset #1:  62%|██████▏   | 42/68 [01:24<00:49,  1.91s/ba][A










Running tokenizer on dataset #11:  41%|████      | 28/68 [01:15<01:31,  2.30s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  53%|█████▎    | 36/68 [01:18<01:19,  2.48s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  47%|████▋     | 32/68 [01:23<01:12,  2.02s/ba][A[A[A




Running tokenizer on dataset #5:  60%|██████    | 41/68 [01:21<00:51,  1.90s/ba][A[A[A[A[A









Running tokenizer on dataset #10:  43%|████▎     | 29/68 [01:17<01:39,  2.56s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  63%|██████▎   | 43/68 [01:26<00:47,  1.91s/ba]






Running tokenizer on dataset #7:  54%|█████▍    | 37/68 [01:20<01:06,  2.14s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8:  59%|█████▉    | 40/68 [01:19<00:52,  1.89s/ba][A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  54%|█████▍    | 37/68 [01:25<01:17,  2.50s/ba][A[A





Running tokenizer on dataset #6:  51%|█████▏    | 35/68 [01:21<01:03,  1.93s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  63%|██████▎   | 43/68 [01:26<00:47,  1.90s/ba][A










Running tokenizer on dataset #11:  43%|████▎     | 29/68 [01:17<01:25,  2.18s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  54%|█████▍    | 37/68 [01:24<01:17,  2.49s/ba][A[A[A[A


Running tokenizer on dataset #3:  49%|████▊     | 33/68 [01:25<01:10,  2.00s/ba][A[A[A




Running tokenizer on dataset #5:  62%|██████▏   | 42/68 [01:23<00:49,  1.89s/ba][A[A[A[A[A








Running tokenizer on dataset #9:  54%|█████▍    | 37/68 [01:20<01:16,  2.48s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  65%|██████▍   | 44/68 [01:28<00:46,  1.92s/ba]







Running tokenizer on dataset #8:  60%|██████    | 41/68 [01:21<00:51,  1.89s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  53%|█████▎    | 36/68 [01:23<01:01,  1.91s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  44%|████▍     | 30/68 [01:19<01:36,  2.55s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  56%|█████▌    | 38/68 [01:27<01:13,  2.44s/ba][A[A
Running tokenizer on dataset #1:  65%|██████▍   | 44/68 [01:28<00:45,  1.91s/ba][A






Running tokenizer on dataset #7:  56%|█████▌    | 38/68 [01:22<01:08,  2.27s/ba][A[A[A[A[A[A[A










Running tokenizer on dataset #11:  44%|████▍     | 30/68 [01:19<01:20,  2.11s/ba][A[A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  63%|██████▎   | 43/68 [01:25<00:47,  1.88s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  50%|█████     | 34/68 [01:27<01:08,  2.01s/ba][A[A[A



Running tokenizer on dataset #4:  56%|█████▌    | 38/68 [01:26<01:14,  2.49s/ba][A[A[A[ARunning tokenizer on dataset #0:  66%|██████▌   | 45/68 [01:30<00:43,  1.91s/ba]







Running tokenizer on dataset #8:  62%|██████▏   | 42/68 [01:23<00:49,  1.89s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  54%|█████▍    | 37/68 [01:25<00:59,  1.91s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  56%|█████▌    | 38/68 [01:22<01:14,  2.48s/ba][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  57%|█████▋    | 39/68 [01:29<01:06,  2.28s/ba][A[A
Running tokenizer on dataset #1:  66%|██████▌   | 45/68 [01:30<00:43,  1.90s/ba][A






Running tokenizer on dataset #7:  57%|█████▋    | 39/68 [01:24<01:02,  2.15s/ba][A[A[A[A[A[A[A










Running tokenizer on dataset #11:  46%|████▌     | 31/68 [01:21<01:16,  2.07s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  46%|████▌     | 31/68 [01:22<01:34,  2.55s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  65%|██████▍   | 44/68 [01:27<00:44,  1.87s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  51%|█████▏    | 35/68 [01:29<01:05,  1.98s/ba][A[A[ARunning tokenizer on dataset #0:  68%|██████▊   | 46/68 [01:32<00:41,  1.89s/ba]







Running tokenizer on dataset #8:  63%|██████▎   | 43/68 [01:25<00:47,  1.92s/ba][A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  57%|█████▋    | 39/68 [01:29<01:11,  2.47s/ba][A[A[A[A





Running tokenizer on dataset #6:  56%|█████▌    | 38/68 [01:27<00:56,  1.90s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  59%|█████▉    | 40/68 [01:31<01:00,  2.16s/ba][A[A
Running tokenizer on dataset #1:  68%|██████▊   | 46/68 [01:32<00:42,  1.91s/ba][A






Running tokenizer on dataset #7:  59%|█████▉    | 40/68 [01:26<00:58,  2.08s/ba][A[A[A[A[A[A[A








Running tokenizer on dataset #9:  57%|█████▋    | 39/68 [01:25<01:12,  2.50s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  47%|████▋     | 32/68 [01:23<01:12,  2.02s/ba][A[A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  66%|██████▌   | 45/68 [01:29<00:43,  1.87s/ba][A[A[A[A[A


Running tokenizer on dataset #3:  53%|█████▎    | 36/68 [01:31<01:03,  1.98s/ba][A[A[ARunning tokenizer on dataset #0:  69%|██████▉   | 47/68 [01:34<00:39,  1.88s/ba]









Running tokenizer on dataset #10:  47%|████▋     | 32/68 [01:24<01:31,  2.54s/ba][A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  65%|██████▍   | 44/68 [01:27<00:45,  1.91s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  57%|█████▋    | 39/68 [01:29<00:55,  1.90s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  60%|██████    | 41/68 [01:33<00:56,  2.08s/ba][A[A
Running tokenizer on dataset #1:  69%|██████▉   | 47/68 [01:34<00:40,  1.92s/ba][A






Running tokenizer on dataset #7:  60%|██████    | 41/68 [01:28<00:54,  2.03s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  59%|█████▉    | 40/68 [01:31<01:09,  2.50s/ba][A[A[A[A










Running tokenizer on dataset #11:  49%|████▊     | 33/68 [01:25<01:09,  2.00s/ba][A[A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  68%|██████▊   | 46/68 [01:31<00:41,  1.89s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  71%|███████   | 48/68 [01:36<00:37,  1.86s/ba]








Running tokenizer on dataset #9:  59%|█████▉    | 40/68 [01:28<01:10,  2.51s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  54%|█████▍    | 37/68 [01:33<01:01,  1.97s/ba][A[A[A







Running tokenizer on dataset #8:  66%|██████▌   | 45/68 [01:29<00:43,  1.91s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  59%|█████▉    | 40/68 [01:31<00:52,  1.89s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  49%|████▊     | 33/68 [01:27<01:28,  2.52s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  62%|██████▏   | 42/68 [01:35<00:53,  2.04s/ba][A[A
Running tokenizer on dataset #1:  71%|███████   | 48/68 [01:36<00:38,  1.92s/ba][A






Running tokenizer on dataset #7:  62%|██████▏   | 42/68 [01:30<00:51,  2.00s/ba][A[A[A[A[A[A[A










Running tokenizer on dataset #11:  50%|█████     | 34/68 [01:27<01:06,  1.97s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  60%|██████    | 41/68 [01:34<01:06,  2.48s/ba][A[A[A[A




Running tokenizer on dataset #5:  69%|██████▉   | 47/68 [01:33<00:39,  1.90s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  72%|███████▏  | 49/68 [01:38<00:35,  1.86s/ba]


Running tokenizer on dataset #3:  56%|█████▌    | 38/68 [01:35<00:58,  1.96s/ba][A[A[A







Running tokenizer on dataset #8:  68%|██████▊   | 46/68 [01:31<00:42,  1.92s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  60%|██████    | 41/68 [01:33<00:51,  1.89s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  60%|██████    | 41/68 [01:30<01:08,  2.52s/ba][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  63%|██████▎   | 43/68 [01:37<00:50,  2.00s/ba][A[A
Running tokenizer on dataset #1:  72%|███████▏  | 49/68 [01:38<00:36,  1.92s/ba][A






Running tokenizer on dataset #7:  63%|██████▎   | 43/68 [01:32<00:49,  1.97s/ba][A[A[A[A[A[A[A









Running tokenizer on dataset #10:  50%|█████     | 34/68 [01:29<01:26,  2.54s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  51%|█████▏    | 35/68 [01:29<01:06,  2.01s/ba][A[A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  71%|███████   | 48/68 [01:34<00:37,  1.90s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  74%|███████▎  | 50/68 [01:39<00:33,  1.87s/ba]


Running tokenizer on dataset #3:  57%|█████▋    | 39/68 [01:37<00:56,  1.93s/ba][A[A[A



Running tokenizer on dataset #4:  62%|██████▏   | 42/68 [01:36<01:04,  2.47s/ba][A[A[A[A







Running tokenizer on dataset #8:  69%|██████▉   | 47/68 [01:33<00:40,  1.91s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  62%|██████▏   | 42/68 [01:35<00:49,  1.89s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  65%|██████▍   | 44/68 [01:39<00:47,  1.97s/ba][A[A






Running tokenizer on dataset #7:  65%|██████▍   | 44/68 [01:34<00:46,  1.95s/ba][A[A[A[A[A[A[A
Running tokenizer on dataset #1:  74%|███████▎  | 50/68 [01:40<00:34,  1.93s/ba][A








Running tokenizer on dataset #9:  62%|██████▏   | 42/68 [01:33<01:05,  2.53s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  72%|███████▏  | 49/68 [01:36<00:35,  1.88s/ba][A[A[A[A[A










Running tokenizer on dataset #11:  53%|█████▎    | 36/68 [01:31<01:03,  1.99s/ba][A[A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  75%|███████▌  | 51/68 [01:41<00:31,  1.86s/ba]









Running tokenizer on dataset #10:  51%|█████▏    | 35/68 [01:32<01:23,  2.52s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  59%|█████▉    | 40/68 [01:38<00:54,  1.95s/ba][A[A[A







Running tokenizer on dataset #8:  71%|███████   | 48/68 [01:34<00:38,  1.91s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  63%|██████▎   | 43/68 [01:36<00:47,  1.90s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  66%|██████▌   | 45/68 [01:40<00:45,  1.96s/ba][A[A



Running tokenizer on dataset #4:  63%|██████▎   | 43/68 [01:39<01:01,  2.48s/ba][A[A[A[A
Running tokenizer on dataset #1:  75%|███████▌  | 51/68 [01:41<00:32,  1.91s/ba][A






Running tokenizer on dataset #7:  66%|██████▌   | 45/68 [01:36<00:44,  1.95s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  74%|███████▎  | 50/68 [01:38<00:33,  1.89s/ba][A[A[A[A[A










Running tokenizer on dataset #11:  54%|█████▍    | 37/68 [01:33<01:01,  1.98s/ba][A[A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  76%|███████▋  | 52/68 [01:43<00:29,  1.86s/ba]








Running tokenizer on dataset #9:  63%|██████▎   | 43/68 [01:35<01:03,  2.52s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  60%|██████    | 41/68 [01:40<00:52,  1.94s/ba][A[A[A







Running tokenizer on dataset #8:  72%|███████▏  | 49/68 [01:36<00:36,  1.92s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  65%|██████▍   | 44/68 [01:38<00:45,  1.88s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  53%|█████▎    | 36/68 [01:35<01:21,  2.53s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  68%|██████▊   | 46/68 [01:42<00:42,  1.93s/ba][A[A
Running tokenizer on dataset #1:  76%|███████▋  | 52/68 [01:43<00:30,  1.89s/ba][A






Running tokenizer on dataset #7:  68%|██████▊   | 46/68 [01:38<00:42,  1.91s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  65%|██████▍   | 44/68 [01:41<00:59,  2.49s/ba][A[A[A[A




Running tokenizer on dataset #5:  75%|███████▌  | 51/68 [01:40<00:32,  1.88s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  78%|███████▊  | 53/68 [01:45<00:27,  1.86s/ba]










Running tokenizer on dataset #11:  56%|█████▌    | 38/68 [01:35<00:59,  1.97s/ba][A[A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  62%|██████▏   | 42/68 [01:42<00:50,  1.94s/ba][A[A[A







Running tokenizer on dataset #8:  74%|███████▎  | 50/68 [01:38<00:34,  1.90s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  66%|██████▌   | 45/68 [01:40<00:43,  1.88s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  65%|██████▍   | 44/68 [01:38<01:00,  2.51s/ba][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  69%|██████▉   | 47/68 [01:44<00:40,  1.94s/ba][A[A
Running tokenizer on dataset #1:  78%|███████▊  | 53/68 [01:45<00:28,  1.90s/ba][A









Running tokenizer on dataset #10:  54%|█████▍    | 37/68 [01:37<01:18,  2.53s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  69%|██████▉   | 47/68 [01:40<00:42,  2.02s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  76%|███████▋  | 52/68 [01:42<00:30,  1.88s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  79%|███████▉  | 54/68 [01:47<00:26,  1.86s/ba]










Running tokenizer on dataset #11:  57%|█████▋    | 39/68 [01:37<00:57,  1.97s/ba][A[A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  66%|██████▌   | 45/68 [01:44<00:56,  2.47s/ba][A[A[A[A


Running tokenizer on dataset #3:  63%|██████▎   | 43/68 [01:44<00:48,  1.93s/ba][A[A[A







Running tokenizer on dataset #8:  75%|███████▌  | 51/68 [01:40<00:32,  1.91s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  68%|██████▊   | 46/68 [01:42<00:41,  1.88s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  71%|███████   | 48/68 [01:46<00:38,  1.93s/ba][A[A
Running tokenizer on dataset #1:  79%|███████▉  | 54/68 [01:47<00:26,  1.90s/ba][A








Running tokenizer on dataset #9:  66%|██████▌   | 45/68 [01:40<00:57,  2.51s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  78%|███████▊  | 53/68 [01:44<00:28,  1.89s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  81%|████████  | 55/68 [01:49<00:24,  1.87s/ba]









Running tokenizer on dataset #10:  56%|█████▌    | 38/68 [01:39<01:15,  2.50s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  59%|█████▉    | 40/68 [01:39<00:54,  1.95s/ba][A[A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  71%|███████   | 48/68 [01:42<00:43,  2.16s/ba][A[A[A[A[A[A[A


Running tokenizer on dataset #3:  65%|██████▍   | 44/68 [01:46<00:46,  1.94s/ba][A[A[A







Running tokenizer on dataset #8:  76%|███████▋  | 52/68 [01:42<00:30,  1.91s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  69%|██████▉   | 47/68 [01:44<00:39,  1.89s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  68%|██████▊   | 46/68 [01:46<00:54,  2.48s/ba][A[A[A[A

Running tokenizer on dataset #2:  72%|███████▏  | 49/68 [01:48<00:36,  1.91s/ba][A[A
Running tokenizer on dataset #1:  81%|████████  | 55/68 [01:49<00:25,  1.98s/ba][A




Running tokenizer on dataset #5:  79%|███████▉  | 54/68 [01:46<00:26,  1.89s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  82%|████████▏ | 56/68 [01:51<00:22,  1.87s/ba]








Running tokenizer on dataset #9:  68%|██████▊   | 46/68 [01:43<00:55,  2.51s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  60%|██████    | 41/68 [01:40<00:52,  1.94s/ba][A[A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  66%|██████▌   | 45/68 [01:48<00:44,  1.94s/ba][A[A[A







Running tokenizer on dataset #8:  78%|███████▊  | 53/68 [01:44<00:28,  1.90s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  71%|███████   | 48/68 [01:46<00:37,  1.89s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  57%|█████▋    | 39/68 [01:42<01:13,  2.52s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  72%|███████▏  | 49/68 [01:45<00:42,  2.26s/ba][A[A[A[A[A[A[A

Running tokenizer on dataset #2:  74%|███████▎  | 50/68 [01:50<00:34,  1.91s/ba][A[A



Running tokenizer on dataset #4:  69%|██████▉   | 47/68 [01:49<00:52,  2.48s/ba][A[A[A[A




Running tokenizer on dataset #5:  81%|████████  | 55/68 [01:48<00:24,  1.91s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  84%|████████▍ | 57/68 [01:53<00:20,  1.90s/ba]
Running tokenizer on dataset #1:  82%|████████▏ | 56/68 [01:52<00:25,  2.12s/ba][A


Running tokenizer on dataset #3:  68%|██████▊   | 46/68 [01:50<00:42,  1.94s/ba][A[A[A







Running tokenizer on dataset #8:  79%|███████▉  | 54/68 [01:46<00:26,  1.89s/ba][A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  72%|███████▏  | 49/68 [01:48<00:35,  1.88s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  62%|██████▏   | 42/68 [01:43<00:54,  2.08s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  69%|██████▉   | 47/68 [01:45<00:52,  2.51s/ba][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  75%|███████▌  | 51/68 [01:52<00:32,  1.89s/ba][A[A









Running tokenizer on dataset #10:  59%|█████▉    | 40/68 [01:45<01:10,  2.50s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  74%|███████▎  | 50/68 [01:47<00:41,  2.31s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  82%|████████▏ | 56/68 [01:50<00:22,  1.90s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  85%|████████▌ | 58/68 [01:54<00:18,  1.88s/ba]



Running tokenizer on dataset #4:  71%|███████   | 48/68 [01:51<00:49,  2.47s/ba][A[A[A[A







Running tokenizer on dataset #8:  81%|████████  | 55/68 [01:48<00:24,  1.88s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  69%|██████▉   | 47/68 [01:52<00:40,  1.95s/ba][A[A[A





Running tokenizer on dataset #6:  74%|███████▎  | 50/68 [01:50<00:33,  1.88s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  84%|████████▍ | 57/68 [01:54<00:24,  2.22s/ba][A

Running tokenizer on dataset #2:  76%|███████▋  | 52/68 [01:54<00:30,  1.89s/ba][A[A










Running tokenizer on dataset #11:  63%|██████▎   | 43/68 [01:45<00:54,  2.20s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  71%|███████   | 48/68 [01:48<00:49,  2.49s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  84%|████████▍ | 57/68 [01:51<00:20,  1.89s/ba][A[A[A[A[ARunning tokenizer on dataset #0:  87%|████████▋ | 59/68 [01:56<00:16,  1.87s/ba]






Running tokenizer on dataset #7:  75%|███████▌  | 51/68 [01:50<00:39,  2.35s/ba][A[A[A[A[A[A[A









Running tokenizer on dataset #10:  60%|██████    | 41/68 [01:47<01:07,  2.51s/ba][A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  75%|███████▌  | 51/68 [01:51<00:31,  1.85s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  82%|████████▏ | 56/68 [01:50<00:22,  1.89s/ba][A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  72%|███████▏  | 49/68 [01:53<00:46,  2.45s/ba][A[A[A[A


Running tokenizer on dataset #3:  71%|███████   | 48/68 [01:54<00:39,  1.98s/ba][A[A[A

Running tokenizer on dataset #2:  78%|███████▊  | 53/68 [01:56<00:28,  1.87s/ba][A[A
Running tokenizer on dataset #1:  85%|████████▌ | 58/68 [01:57<00:23,  2.31s/ba][ARunning tokenizer on dataset #0:  88%|████████▊ | 60/68 [01:58<00:14,  1.86s/ba]




Running tokenizer on dataset #5:  85%|████████▌ | 58/68 [01:53<00:18,  1.89s/ba][A[A[A[A[A










Running tokenizer on dataset #11:  65%|██████▍   | 44/68 [01:48<00:54,  2.29s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  72%|███████▏  | 49/68 [01:50<00:47,  2.48s/ba][A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  76%|███████▋  | 52/68 [01:53<00:29,  1.86s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  84%|████████▍ | 57/68 [01:52<00:20,  1.91s/ba][A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  76%|███████▋  | 52/68 [01:52<00:38,  2.41s/ba][A[A[A[A[A[A[A









Running tokenizer on dataset #10:  62%|██████▏   | 42/68 [01:50<01:05,  2.51s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  72%|███████▏  | 49/68 [01:56<00:37,  1.97s/ba][A[A[A

Running tokenizer on dataset #2:  79%|███████▉  | 54/68 [01:57<00:26,  1.88s/ba][A[A



Running tokenizer on dataset #4:  74%|███████▎  | 50/68 [01:56<00:44,  2.45s/ba][A[A[A[ARunning tokenizer on dataset #0:  90%|████████▉ | 61/68 [02:00<00:13,  1.87s/ba]
Running tokenizer on dataset #1:  87%|████████▋ | 59/68 [01:59<00:21,  2.36s/ba][A




Running tokenizer on dataset #5:  87%|████████▋ | 59/68 [01:55<00:17,  1.92s/ba][A[A[A[A[A





Running tokenizer on dataset #6:  78%|███████▊  | 53/68 [01:55<00:27,  1.86s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  66%|██████▌   | 45/68 [01:50<00:53,  2.34s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  74%|███████▎  | 50/68 [01:53<00:44,  2.48s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  85%|████████▌ | 58/68 [01:53<00:19,  1.92s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  74%|███████▎  | 50/68 [01:58<00:35,  1.97s/ba][A[A[A

Running tokenizer on dataset #2:  81%|████████  | 55/68 [01:59<00:24,  1.90s/ba][A[A









Running tokenizer on dataset #10:  63%|██████▎   | 43/68 [01:52<01:02,  2.51s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  78%|███████▊  | 53/68 [01:55<00:36,  2.45s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #0:  91%|█████████ | 62/68 [02:02<00:11,  1.87s/ba]




Running tokenizer on dataset #5:  88%|████████▊ | 60/68 [01:57<00:15,  1.91s/ba][A[A[A[A[A



Running tokenizer on dataset #4:  75%|███████▌  | 51/68 [01:58<00:42,  2.47s/ba][A[A[A[A





Running tokenizer on dataset #6:  79%|███████▉  | 54/68 [01:57<00:26,  1.87s/ba][A[A[A[A[A[A
Running tokenizer on dataset #1:  88%|████████▊ | 60/68 [02:02<00:18,  2.37s/ba][A







Running tokenizer on dataset #8:  87%|████████▋ | 59/68 [01:55<00:17,  1.90s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  75%|███████▌  | 51/68 [02:00<00:33,  1.96s/ba][A[A[A

Running tokenizer on dataset #2:  82%|████████▏ | 56/68 [02:01<00:22,  1.90s/ba][A[A










Running tokenizer on dataset #11:  68%|██████▊   | 46/68 [01:53<00:52,  2.37s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  75%|███████▌  | 51/68 [01:55<00:42,  2.48s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  93%|█████████▎| 63/68 [02:04<00:09,  1.86s/ba]









Running tokenizer on dataset #10:  65%|██████▍   | 44/68 [01:54<00:59,  2.48s/ba][A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  90%|████████▉ | 61/68 [01:59<00:13,  1.91s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  79%|███████▉  | 54/68 [01:57<00:34,  2.45s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  81%|████████  | 55/68 [01:59<00:24,  1.87s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  76%|███████▋  | 52/68 [02:01<00:39,  2.47s/ba][A[A[A[A







Running tokenizer on dataset #8:  88%|████████▊ | 60/68 [01:57<00:15,  1.89s/ba][A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  84%|████████▍ | 57/68 [02:03<00:20,  1.90s/ba][A[A


Running tokenizer on dataset #3:  76%|███████▋  | 52/68 [02:02<00:31,  1.95s/ba][A[A[A
Running tokenizer on dataset #1:  90%|████████▉ | 61/68 [02:04<00:16,  2.40s/ba][ARunning tokenizer on dataset #0:  94%|█████████▍| 64/68 [02:06<00:07,  1.85s/ba]










Running tokenizer on dataset #11:  69%|██████▉   | 47/68 [01:55<00:50,  2.39s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  76%|███████▋  | 52/68 [01:57<00:39,  2.48s/ba][A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  91%|█████████ | 62/68 [02:01<00:11,  1.95s/ba][A[A[A[A[A





Running tokenizer on dataset #6:  82%|████████▏ | 56/68 [02:01<00:22,  1.89s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  81%|████████  | 55/68 [02:00<00:31,  2.46s/ba][A[A[A[A[A[A[A









Running tokenizer on dataset #10:  66%|██████▌   | 45/68 [01:57<00:57,  2.49s/ba][A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  90%|████████▉ | 61/68 [01:59<00:13,  1.89s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  78%|███████▊  | 53/68 [02:04<00:28,  1.93s/ba][A[A[A

Running tokenizer on dataset #2:  85%|████████▌ | 58/68 [02:05<00:19,  1.91s/ba][A[A



Running tokenizer on dataset #4:  78%|███████▊  | 53/68 [02:03<00:37,  2.47s/ba][A[A[A[ARunning tokenizer on dataset #0:  96%|█████████▌| 65/68 [02:07<00:05,  1.86s/ba]
Running tokenizer on dataset #1:  91%|█████████ | 62/68 [02:07<00:14,  2.47s/ba][A




Running tokenizer on dataset #5:  93%|█████████▎| 63/68 [02:03<00:09,  1.93s/ba][A[A[A[A[A










Running tokenizer on dataset #11:  71%|███████   | 48/68 [01:58<00:48,  2.43s/ba][A[A[A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  84%|████████▍ | 57/68 [02:03<00:20,  1.89s/ba][A[A[A[A[A[A








Running tokenizer on dataset #9:  78%|███████▊  | 53/68 [02:00<00:37,  2.51s/ba][A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  91%|█████████ | 62/68 [02:01<00:11,  1.94s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  79%|███████▉  | 54/68 [02:06<00:26,  1.91s/ba][A[A[A

Running tokenizer on dataset #2:  87%|████████▋ | 59/68 [02:07<00:17,  1.91s/ba][A[A






Running tokenizer on dataset #7:  82%|████████▏ | 56/68 [02:02<00:29,  2.46s/ba][A[A[A[A[A[A[A









Running tokenizer on dataset #10:  68%|██████▊   | 46/68 [01:59<00:54,  2.49s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  97%|█████████▋| 66/68 [02:09<00:03,  1.86s/ba]



Running tokenizer on dataset #4:  79%|███████▉  | 54/68 [02:06<00:34,  2.47s/ba][A[A[A[A




Running tokenizer on dataset #5:  94%|█████████▍| 64/68 [02:05<00:07,  1.91s/ba][A[A[A[A[A
Running tokenizer on dataset #1:  93%|█████████▎| 63/68 [02:09<00:11,  2.38s/ba][A





Running tokenizer on dataset #6:  85%|████████▌ | 58/68 [02:05<00:18,  1.89s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  93%|█████████▎| 63/68 [02:03<00:09,  1.92s/ba][A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  72%|███████▏  | 49/68 [02:00<00:46,  2.43s/ba][A[A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  81%|████████  | 55/68 [02:08<00:24,  1.92s/ba][A[A[A

Running tokenizer on dataset #2:  88%|████████▊ | 60/68 [02:09<00:15,  1.90s/ba][A[A








Running tokenizer on dataset #9:  79%|███████▉  | 54/68 [02:02<00:34,  2.48s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #0:  99%|█████████▊| 67/68 [02:11<00:01,  1.84s/ba]









Running tokenizer on dataset #10:  69%|██████▉   | 47/68 [02:02<00:52,  2.48s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  84%|████████▍ | 57/68 [02:05<00:27,  2.49s/ba][A[A[A[A[A[A[A




Running tokenizer on dataset #5:  96%|█████████▌| 65/68 [02:07<00:05,  1.90s/ba][A[A[A[A[A
Running tokenizer on dataset #1:  94%|█████████▍| 64/68 [02:11<00:08,  2.22s/ba][A



Running tokenizer on dataset #4:  81%|████████  | 55/68 [02:08<00:32,  2.46s/ba][A[A[A[A





Running tokenizer on dataset #6:  87%|████████▋ | 59/68 [02:06<00:16,  1.88s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  94%|█████████▍| 64/68 [02:05<00:07,  1.90s/ba][A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  90%|████████▉ | 61/68 [02:11<00:13,  1.87s/ba][A[ARunning tokenizer on dataset #0: 100%|██████████| 68/68 [02:13<00:00,  1.72s/ba]Running tokenizer on dataset #0: 100%|██████████| 68/68 [02:13<00:00,  1.96s/ba]


Running tokenizer on dataset #3:  82%|████████▏ | 56/68 [02:10<00:23,  1.92s/ba][A[A[A










Running tokenizer on dataset #11:  74%|███████▎  | 50/68 [02:03<00:44,  2.46s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  81%|████████  | 55/68 [02:05<00:32,  2.47s/ba][A[A[A[A[A[A[A[A[A
Running tokenizer on dataset #1:  96%|█████████▌| 65/68 [02:12<00:06,  2.10s/ba][A




Running tokenizer on dataset #5:  97%|█████████▋| 66/68 [02:09<00:03,  1.90s/ba][A[A[A[A[A






Running tokenizer on dataset #7:  85%|████████▌ | 58/68 [02:07<00:24,  2.41s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  88%|████████▊ | 60/68 [02:08<00:15,  1.88s/ba][A[A[A[A[A[A









Running tokenizer on dataset #10:  71%|███████   | 48/68 [02:04<00:49,  2.49s/ba][A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  96%|█████████▌| 65/68 [02:07<00:05,  1.89s/ba][A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  82%|████████▏ | 56/68 [02:11<00:29,  2.46s/ba][A[A[A[A


Running tokenizer on dataset #3:  84%|████████▍ | 57/68 [02:11<00:21,  1.91s/ba][A[A[A

Running tokenizer on dataset #2:  91%|█████████ | 62/68 [02:13<00:11,  1.94s/ba][A[A
Running tokenizer on dataset #1:  97%|█████████▋| 66/68 [02:14<00:04,  2.02s/ba][A










Running tokenizer on dataset #11:  75%|███████▌  | 51/68 [02:05<00:41,  2.43s/ba][A[A[A[A[A[A[A[A[A[A[A




Running tokenizer on dataset #5:  99%|█████████▊| 67/68 [02:11<00:01,  1.91s/ba][A[A[A[A[A








Running tokenizer on dataset #9:  82%|████████▏ | 56/68 [02:07<00:29,  2.46s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  87%|████████▋ | 59/68 [02:09<00:20,  2.25s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  90%|████████▉ | 61/68 [02:10<00:13,  1.87s/ba][A[A[A[A[A[A







Running tokenizer on dataset #8:  97%|█████████▋| 66/68 [02:09<00:03,  1.88s/ba][A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  85%|████████▌ | 58/68 [02:13<00:19,  1.92s/ba][A[A[A









Running tokenizer on dataset #10:  72%|███████▏  | 49/68 [02:07<00:47,  2.49s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  93%|█████████▎| 63/68 [02:15<00:09,  1.92s/ba][A[A



Running tokenizer on dataset #4:  84%|████████▍ | 57/68 [02:13<00:27,  2.46s/ba][A[A[A[A




Running tokenizer on dataset #5: 100%|██████████| 68/68 [02:12<00:00,  1.78s/ba][A[A[A[A[ARunning tokenizer on dataset #5: 100%|██████████| 68/68 [02:12<00:00,  1.95s/ba]
Running tokenizer on dataset #1:  99%|█████████▊| 67/68 [02:16<00:01,  1.95s/ba][A






Running tokenizer on dataset #7:  88%|████████▊ | 60/68 [02:11<00:17,  2.13s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  91%|█████████ | 62/68 [02:12<00:11,  1.90s/ba][A[A[A[A[A[A










Running tokenizer on dataset #11:  76%|███████▋  | 52/68 [02:07<00:39,  2.44s/ba][A[A[A[A[A[A[A[A[A[A[A







Running tokenizer on dataset #8:  99%|█████████▊| 67/68 [02:11<00:01,  1.90s/ba][A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  84%|████████▍ | 57/68 [02:10<00:27,  2.47s/ba][A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  94%|█████████▍| 64/68 [02:17<00:07,  1.91s/ba][A[A


Running tokenizer on dataset #3:  87%|████████▋ | 59/68 [02:15<00:17,  1.93s/ba][A[A[A
Running tokenizer on dataset #1: 100%|██████████| 68/68 [02:18<00:00,  1.80s/ba][ARunning tokenizer on dataset #1: 100%|██████████| 68/68 [02:18<00:00,  2.03s/ba]









Running tokenizer on dataset #10:  74%|███████▎  | 50/68 [02:09<00:44,  2.48s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  90%|████████▉ | 61/68 [02:13<00:14,  2.06s/ba][A[A[A[A[A[A[A







Running tokenizer on dataset #8: 100%|██████████| 68/68 [02:12<00:00,  1.75s/ba][A[A[A[A[A[A[A[ARunning tokenizer on dataset #8: 100%|██████████| 68/68 [02:12<00:00,  1.95s/ba]



Running tokenizer on dataset #4:  85%|████████▌ | 58/68 [02:16<00:24,  2.48s/ba][A[A[A[A





Running tokenizer on dataset #6:  93%|█████████▎| 63/68 [02:14<00:09,  1.89s/ba][A[A[A[A[A[A

Running tokenizer on dataset #2:  96%|█████████▌| 65/68 [02:18<00:05,  1.89s/ba][A[A


Running tokenizer on dataset #3:  88%|████████▊ | 60/68 [02:17<00:15,  1.92s/ba][A[A[A










Running tokenizer on dataset #11:  78%|███████▊  | 53/68 [02:10<00:36,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  85%|████████▌ | 58/68 [02:12<00:24,  2.47s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  75%|███████▌  | 51/68 [02:12<00:41,  2.47s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  91%|█████████ | 62/68 [02:15<00:12,  2.05s/ba][A[A[A[A[A[A[A





Running tokenizer on dataset #6:  94%|█████████▍| 64/68 [02:16<00:07,  1.88s/ba][A[A[A[A[A[A



Running tokenizer on dataset #4:  87%|████████▋ | 59/68 [02:18<00:22,  2.48s/ba][A[A[A[A

Running tokenizer on dataset #2:  97%|█████████▋| 66/68 [02:20<00:03,  1.88s/ba][A[A


Running tokenizer on dataset #3:  90%|████████▉ | 61/68 [02:19<00:13,  1.90s/ba][A[A[A










Running tokenizer on dataset #11:  79%|███████▉  | 54/68 [02:12<00:34,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  87%|████████▋ | 59/68 [02:15<00:22,  2.47s/ba][A[A[A[A[A[A[A[A[A





Running tokenizer on dataset #6:  96%|█████████▌| 65/68 [02:18<00:05,  1.87s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  93%|█████████▎| 63/68 [02:17<00:10,  2.02s/ba][A[A[A[A[A[A[A









Running tokenizer on dataset #10:  76%|███████▋  | 52/68 [02:14<00:39,  2.46s/ba][A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2:  99%|█████████▊| 67/68 [02:22<00:01,  1.86s/ba][A[A


Running tokenizer on dataset #3:  91%|█████████ | 62/68 [02:21<00:11,  1.96s/ba][A[A[A



Running tokenizer on dataset #4:  88%|████████▊ | 60/68 [02:21<00:19,  2.48s/ba][A[A[A[A





Running tokenizer on dataset #6:  97%|█████████▋| 66/68 [02:20<00:03,  1.86s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  94%|█████████▍| 64/68 [02:18<00:07,  1.98s/ba][A[A[A[A[A[A[A










Running tokenizer on dataset #11:  81%|████████  | 55/68 [02:15<00:31,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A

Running tokenizer on dataset #2: 100%|██████████| 68/68 [02:24<00:00,  1.73s/ba][A[ARunning tokenizer on dataset #2: 100%|██████████| 68/68 [02:24<00:00,  2.12s/ba]








Running tokenizer on dataset #9:  88%|████████▊ | 60/68 [02:17<00:19,  2.46s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  93%|█████████▎| 63/68 [02:23<00:09,  1.96s/ba][A[A[A









Running tokenizer on dataset #10:  78%|███████▊  | 53/68 [02:17<00:36,  2.46s/ba][A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  90%|████████▉ | 61/68 [02:23<00:17,  2.48s/ba][A[A[A[A





Running tokenizer on dataset #6:  99%|█████████▊| 67/68 [02:21<00:01,  1.85s/ba][A[A[A[A[A[A






Running tokenizer on dataset #7:  96%|█████████▌| 65/68 [02:20<00:05,  1.96s/ba][A[A[A[A[A[A[A










Running tokenizer on dataset #11:  82%|████████▏ | 56/68 [02:17<00:29,  2.46s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  90%|████████▉ | 61/68 [02:20<00:17,  2.46s/ba][A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  94%|█████████▍| 64/68 [02:25<00:07,  1.94s/ba][A[A[A





Running tokenizer on dataset #6: 100%|██████████| 68/68 [02:23<00:00,  1.72s/ba][A[A[A[A[A[ARunning tokenizer on dataset #6: 100%|██████████| 68/68 [02:23<00:00,  2.11s/ba]









Running tokenizer on dataset #10:  79%|███████▉  | 54/68 [02:19<00:34,  2.45s/ba][A[A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  97%|█████████▋| 66/68 [02:22<00:03,  1.91s/ba][A[A[A[A[A[A[A



Running tokenizer on dataset #4:  91%|█████████ | 62/68 [02:26<00:15,  2.52s/ba][A[A[A[A


Running tokenizer on dataset #3:  96%|█████████▌| 65/68 [02:27<00:05,  1.92s/ba][A[A[A










Running tokenizer on dataset #11:  84%|████████▍ | 57/68 [02:20<00:26,  2.44s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  91%|█████████ | 62/68 [02:22<00:15,  2.53s/ba][A[A[A[A[A[A[A[A[A






Running tokenizer on dataset #7:  99%|█████████▊| 67/68 [02:24<00:01,  1.88s/ba][A[A[A[A[A[A[A









Running tokenizer on dataset #10:  81%|████████  | 55/68 [02:22<00:31,  2.46s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  97%|█████████▋| 66/68 [02:29<00:03,  1.90s/ba][A[A[A



Running tokenizer on dataset #4:  93%|█████████▎| 63/68 [02:28<00:12,  2.50s/ba][A[A[A[A






Running tokenizer on dataset #7: 100%|██████████| 68/68 [02:25<00:00,  1.74s/ba][A[A[A[A[A[A[ARunning tokenizer on dataset #7: 100%|██████████| 68/68 [02:25<00:00,  2.15s/ba]










Running tokenizer on dataset #11:  85%|████████▌ | 58/68 [02:22<00:24,  2.42s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  93%|█████████▎| 63/68 [02:25<00:12,  2.41s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  82%|████████▏ | 56/68 [02:24<00:29,  2.44s/ba][A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3:  99%|█████████▊| 67/68 [02:31<00:01,  1.89s/ba][A[A[A



Running tokenizer on dataset #4:  94%|█████████▍| 64/68 [02:31<00:09,  2.49s/ba][A[A[A[A








Running tokenizer on dataset #9:  94%|█████████▍| 64/68 [02:26<00:08,  2.24s/ba][A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  87%|████████▋ | 59/68 [02:25<00:21,  2.42s/ba][A[A[A[A[A[A[A[A[A[A[A


Running tokenizer on dataset #3: 100%|██████████| 68/68 [02:32<00:00,  1.76s/ba][A[A[ARunning tokenizer on dataset #3: 100%|██████████| 68/68 [02:32<00:00,  2.24s/ba]









Running tokenizer on dataset #10:  84%|████████▍ | 57/68 [02:26<00:26,  2.44s/ba][A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  96%|█████████▌| 65/68 [02:28<00:06,  2.12s/ba][A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  96%|█████████▌| 65/68 [02:33<00:07,  2.48s/ba][A[A[A[A










Running tokenizer on dataset #11:  88%|████████▊ | 60/68 [02:27<00:19,  2.44s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  97%|█████████▋| 66/68 [02:30<00:04,  2.01s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  85%|████████▌ | 58/68 [02:29<00:24,  2.45s/ba][A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4:  97%|█████████▋| 66/68 [02:36<00:04,  2.47s/ba][A[A[A[A










Running tokenizer on dataset #11:  90%|████████▉ | 61/68 [02:29<00:17,  2.44s/ba][A[A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9:  99%|█████████▊| 67/68 [02:32<00:01,  1.95s/ba][A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  87%|████████▋ | 59/68 [02:31<00:21,  2.43s/ba][A[A[A[A[A[A[A[A[A[A








Running tokenizer on dataset #9: 100%|██████████| 68/68 [02:33<00:00,  1.78s/ba][A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #9: 100%|██████████| 68/68 [02:33<00:00,  2.26s/ba]



Running tokenizer on dataset #4:  99%|█████████▊| 67/68 [02:38<00:02,  2.45s/ba][A[A[A[A










Running tokenizer on dataset #11:  91%|█████████ | 62/68 [02:32<00:14,  2.49s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  88%|████████▊ | 60/68 [02:34<00:19,  2.40s/ba][A[A[A[A[A[A[A[A[A[A



Running tokenizer on dataset #4: 100%|██████████| 68/68 [02:40<00:00,  2.26s/ba][A[A[A[ARunning tokenizer on dataset #4: 100%|██████████| 68/68 [02:40<00:00,  2.36s/ba]










Running tokenizer on dataset #11:  93%|█████████▎| 63/68 [02:34<00:12,  2.48s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  90%|████████▉ | 61/68 [02:36<00:16,  2.41s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  94%|█████████▍| 64/68 [02:37<00:09,  2.46s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  91%|█████████ | 62/68 [02:39<00:14,  2.48s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  96%|█████████▌| 65/68 [02:39<00:07,  2.45s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  93%|█████████▎| 63/68 [02:41<00:12,  2.45s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  97%|█████████▋| 66/68 [02:42<00:04,  2.41s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  94%|█████████▍| 64/68 [02:44<00:09,  2.43s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11:  99%|█████████▊| 67/68 [02:44<00:02,  2.40s/ba][A[A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  96%|█████████▌| 65/68 [02:46<00:07,  2.41s/ba][A[A[A[A[A[A[A[A[A[A










Running tokenizer on dataset #11: 100%|██████████| 68/68 [02:46<00:00,  2.24s/ba][A[A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #11: 100%|██████████| 68/68 [02:46<00:00,  2.45s/ba]









Running tokenizer on dataset #10:  97%|█████████▋| 66/68 [02:48<00:04,  2.40s/ba][A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10:  99%|█████████▊| 67/68 [02:51<00:02,  2.39s/ba][A[A[A[A[A[A[A[A[A[A









Running tokenizer on dataset #10: 100%|██████████| 68/68 [02:52<00:00,  2.22s/ba][A[A[A[A[A[A[A[A[A[ARunning tokenizer on dataset #10: 100%|██████████| 68/68 [02:52<00:00,  2.54s/ba]










Error executing job with overrides: ['model.name=facebook/opt-2.7b', 'training.eval_every=250', 'training.train_batch_size=32', 'training.weight_decay=0.05', 'training.eval_batch_size=16', 'training.learning_rate=0.000003', 'training.val_split_percent=20', 'training.num_epochs=4', 'training.lr_warmup_steps=5', 'training.gradient_accumulation_steps=2', 'dataset.name=ViktorThink/mountain_combined_813306', 'dataset.num_batches=2']
Traceback (most recent call last):
  File "finetune_using_clm_wandb.py", line 316, in main
    save_tokenized_datasets(tokenized_datasets)
  File "finetune_using_clm_wandb.py", line 238, in save_tokenized_datasets
    eval_dataset = tokenized_datasets["test"]
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/datasets/dataset_dict.py", line 57, in __getitem__
    return super().__getitem__(k)
KeyError: 'test'

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
Traceback (most recent call last):
  File "finetune_using_clm_wandb.py", line 20, in <module>
    from accelerate import Accelerator, DistributedType
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/accelerate/__init__.py", line 7, in <module>
    from .accelerator import Accelerator
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/accelerate/accelerator.py", line 27, in <module>
    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/accelerate/checkpointing.py", line 24, in <module>
    from .utils import (
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/accelerate/utils/__init__.py", line 120, in <module>
    from .other import (
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/accelerate/utils/other.py", line 29, in <module>
    from deepspeed import DeepSpeedEngine
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/__init__.py", line 13, in <module>
    from . import ops
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/ops/__init__.py", line 7, in <module>
    from . import transformer
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/ops/transformer/__init__.py", line 2, in <module>
    from .inference.config import DeepSpeedInferenceConfig
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/ops/transformer/inference/__init__.py", line 5, in <module>
    from .diffusers_transformer_block import DeepSpeedDiffusersTransformerBlock
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/ops/transformer/inference/diffusers_transformer_block.py", line 8, in <module>
    from ....module_inject import GroupQuantizer
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/module_inject/__init__.py", line 1, in <module>
    from .replace_module import replace_transformer_layer, revert_transformer_layer, ReplaceWithTensorSlicing, GroupQuantizer
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/module_inject/replace_module.py", line 1210, in <module>
    from ..pipe import PipelineModule
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/pipe/__init__.py", line 1, in <module>
    from ..runtime.pipe import PipelineModule, LayerSpec, TiedLayerSpec
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/runtime/pipe/__init__.py", line 1, in <module>
    from .module import PipelineModule, LayerSpec, TiedLayerSpec
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/runtime/pipe/module.py", line 14, in <module>
    from ..activation_checkpointing import checkpointing
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/runtime/activation_checkpointing/checkpointing.py", line 25, in <module>
    from deepspeed.runtime.config import DeepSpeedConfig
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/runtime/config.py", line 34, in <module>
    from ..elasticity import (
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/elasticity/__init__.py", line 5, in <module>
    from .elastic_agent import DSElasticAgent
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/deepspeed/elasticity/elastic_agent.py", line 1, in <module>
    from torch.distributed.elastic.agent.server.local_elastic_agent import LocalElasticAgent
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/__init__.py", line 40, in <module>
    from .local_elastic_agent import TORCHELASTIC_ENABLE_FILE_TIMER, TORCHELASTIC_TIMER_FILE
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 35, in <module>
    log = get_logger()
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/torch/distributed/elastic/utils/logging.py", line 32, in get_logger
    return _setup_logger(name or _derive_module_name(depth=2))
  File "/home/paperspace/Documents/Repos/clm_model_tuning/venv/lib/python3.8/site-packages/torch/distributed/elastic/utils/logging.py", line 49, in _derive_module_name
    stack = inspect.stack()
  File "/usr/lib/python3.8/inspect.py", line 1526, in stack
    return getouterframes(sys._getframe(1), context)
  File "/usr/lib/python3.8/inspect.py", line 1503, in getouterframes
    frameinfo = (frame,) + getframeinfo(frame, context)
  File "/usr/lib/python3.8/inspect.py", line 1477, in getframeinfo
    lines, lnum = findsource(frame)
  File "/usr/lib/python3.8/inspect.py", line 792, in findsource
    module = getmodule(object, file)
  File "/usr/lib/python3.8/inspect.py", line 744, in getmodule
    for modname, module in sys.modules.copy().items():
KeyboardInterrupt
